{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "some ideas: https://stackoverflow.com/questions/41488279/neural-network-always-predicts-the-same-class\n",
    "\n",
    "more ideas: https://www.tensorflow.org/tutorials/structured_data/imbalanced_data\n",
    "<pre>\n",
    "\n",
    "---------------------- TODO --------------------------------------\n",
    "\n",
    "1) \n",
    " \n",
    "\n",
    "---------------------- DONE --------------------------------------\n",
    "change the split and downsampling order\n",
    "\n",
    "now: start with an imbalanced dataset\n",
    "     downsample all the data to a small balanced subset\n",
    "     split and shape using stratify the small subset into 70/30 for Train/Test\n",
    "     means the Test dataset is also balanced\n",
    "     \n",
    "want: start with imbalanced dataset\n",
    "      split using sklearn:train_test_split into 70/30 for Train/Test\n",
    "      downsample and shape the Train dataset to become balanced\n",
    "      shape the Test dataset\n",
    "      means the Test dataset is imbalanced as it should be\n",
    "      \n",
    "      normalize the 3 datasets before building the model using StandardScaler\n",
    "      create a validation dataset to give a true unbiased result of my final models performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "colab = os.environ.get('COLAB_GPU', '10')\n",
    "if (int(colab) == 0):\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')  \n",
    "else:\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory  C:\\DataScience\\Repo\\Imbalanced_data\\input\\creditcardzip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Check if Google Colab path exists\n",
    "if os.path.exists(\"/content/drive/My Drive/MyDSNotebooks/Imbalanced_data/input/creditcardzip\") :\n",
    "    # Change the current working Directory    \n",
    "    os.chdir(\"/content/drive/My Drive/MyDSNotebooks/Imbalanced_data/input/creditcardzip\")\n",
    "# else check if Kaggle/local path exists\n",
    "elif os.path.exists(\"../input/creditcardzip\") :\n",
    "    # Change the current working Directory    \n",
    "    os.chdir(\"../input/creditcardzip\")\n",
    "else:\n",
    "    print(\"Can't change the Current Working Directory\") \n",
    "print(\"Current Working Directory \" , os.getcwd())\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers import Embedding\n",
    "#from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras import regularizers\n",
    "from keras import optimizers\n",
    "from keras import initializers\n",
    "from keras.optimizers import SGD\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.optimizers import Adagrad\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "# -------------------------------------------------------\n",
    "#initialize variables\n",
    "mpl.rcParams['figure.figsize'] = (12, 10)\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "cm_results = []\n",
    "class_names=[0,1] # name  of classes 1=fraudulent transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_learning_rate(epoch, lrate):\n",
    "\treturn lrate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_metrics(model_name, train_features, test_features, train_label, test_label, pred):\n",
    "    try:\n",
    "        print(model_name.score(test_features, test_label)) \n",
    "        print(\"Accuracy score (training): {0:.3f}\".format(model_name.score(train_features, train_label))) \n",
    "        print(\"Accuracy score (validation): {0:.3f}\".format(model_name.score(test_features, test_label))) \n",
    "    except Exception as e:\n",
    "        print(\"error\")  \n",
    "    try:\n",
    "        print(pd.Series(model_name.feature_importances_, index=train_features.columns[:]).nlargest(10).plot(kind='barh')) \n",
    "    except Exception as e:\n",
    "        print(\"error\")  \n",
    "        \n",
    "    print(\"Confusion Matrix:\")\n",
    "    tn, fp, fn, tp = confusion_matrix(test_label, pred).ravel()\n",
    "    total = tn+ fp+ fn+ tp \n",
    "    print(\"false positive pct:\",(fp/total)*100) \n",
    "    print(\"tn\", \" fp\", \" fn\", \" tp\") \n",
    "    print(tn, fp, fn, tp) \n",
    "    print(confusion_matrix(test_label, pred)) \n",
    "    print(\"Classification Report\") \n",
    "    print(classification_report(test_label, pred))\n",
    "    print(\"Specificity =\", tn/(tn+fp))\n",
    "    print(\"Sensitivity =\", tp/(tp+fn))\n",
    "    return tn, fp, fn, tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CalcPct(df,title):\n",
    "    unique_elements, counts_elements = np.unique(df, return_counts=True)\n",
    "    calc_pct = round(counts_elements[1]/(counts_elements[0]+counts_elements[1]) * 100,6)\n",
    "    print(title)\n",
    "    print(np.asarray((unique_elements, counts_elements)))\n",
    "    return calc_pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(history, label, n):\n",
    "    # Use a log scale to show the wide range of values.\n",
    "    plt.semilogy(history.epoch,  history.history['loss'],\n",
    "               color=colors[n], label='Train '+label)\n",
    "    plt.semilogy(history.epoch,  history.history['val_loss'],\n",
    "          color=colors[n], label='Val '+label,\n",
    "          linestyle=\"--\")\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284802</th>\n",
       "      <td>172786.0</td>\n",
       "      <td>-11.881118</td>\n",
       "      <td>10.071785</td>\n",
       "      <td>-9.834783</td>\n",
       "      <td>-2.066656</td>\n",
       "      <td>-5.364473</td>\n",
       "      <td>-2.606837</td>\n",
       "      <td>-4.918215</td>\n",
       "      <td>7.305334</td>\n",
       "      <td>1.914428</td>\n",
       "      <td>...</td>\n",
       "      <td>0.213454</td>\n",
       "      <td>0.111864</td>\n",
       "      <td>1.014480</td>\n",
       "      <td>-0.509348</td>\n",
       "      <td>1.436807</td>\n",
       "      <td>0.250034</td>\n",
       "      <td>0.943651</td>\n",
       "      <td>0.823731</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284803</th>\n",
       "      <td>172787.0</td>\n",
       "      <td>-0.732789</td>\n",
       "      <td>-0.055080</td>\n",
       "      <td>2.035030</td>\n",
       "      <td>-0.738589</td>\n",
       "      <td>0.868229</td>\n",
       "      <td>1.058415</td>\n",
       "      <td>0.024330</td>\n",
       "      <td>0.294869</td>\n",
       "      <td>0.584800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.214205</td>\n",
       "      <td>0.924384</td>\n",
       "      <td>0.012463</td>\n",
       "      <td>-1.016226</td>\n",
       "      <td>-0.606624</td>\n",
       "      <td>-0.395255</td>\n",
       "      <td>0.068472</td>\n",
       "      <td>-0.053527</td>\n",
       "      <td>24.79</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284804</th>\n",
       "      <td>172788.0</td>\n",
       "      <td>1.919565</td>\n",
       "      <td>-0.301254</td>\n",
       "      <td>-3.249640</td>\n",
       "      <td>-0.557828</td>\n",
       "      <td>2.630515</td>\n",
       "      <td>3.031260</td>\n",
       "      <td>-0.296827</td>\n",
       "      <td>0.708417</td>\n",
       "      <td>0.432454</td>\n",
       "      <td>...</td>\n",
       "      <td>0.232045</td>\n",
       "      <td>0.578229</td>\n",
       "      <td>-0.037501</td>\n",
       "      <td>0.640134</td>\n",
       "      <td>0.265745</td>\n",
       "      <td>-0.087371</td>\n",
       "      <td>0.004455</td>\n",
       "      <td>-0.026561</td>\n",
       "      <td>67.88</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284805</th>\n",
       "      <td>172788.0</td>\n",
       "      <td>-0.240440</td>\n",
       "      <td>0.530483</td>\n",
       "      <td>0.702510</td>\n",
       "      <td>0.689799</td>\n",
       "      <td>-0.377961</td>\n",
       "      <td>0.623708</td>\n",
       "      <td>-0.686180</td>\n",
       "      <td>0.679145</td>\n",
       "      <td>0.392087</td>\n",
       "      <td>...</td>\n",
       "      <td>0.265245</td>\n",
       "      <td>0.800049</td>\n",
       "      <td>-0.163298</td>\n",
       "      <td>0.123205</td>\n",
       "      <td>-0.569159</td>\n",
       "      <td>0.546668</td>\n",
       "      <td>0.108821</td>\n",
       "      <td>0.104533</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284806</th>\n",
       "      <td>172792.0</td>\n",
       "      <td>-0.533413</td>\n",
       "      <td>-0.189733</td>\n",
       "      <td>0.703337</td>\n",
       "      <td>-0.506271</td>\n",
       "      <td>-0.012546</td>\n",
       "      <td>-0.649617</td>\n",
       "      <td>1.577006</td>\n",
       "      <td>-0.414650</td>\n",
       "      <td>0.486180</td>\n",
       "      <td>...</td>\n",
       "      <td>0.261057</td>\n",
       "      <td>0.643078</td>\n",
       "      <td>0.376777</td>\n",
       "      <td>0.008797</td>\n",
       "      <td>-0.473649</td>\n",
       "      <td>-0.818267</td>\n",
       "      <td>-0.002415</td>\n",
       "      <td>0.013649</td>\n",
       "      <td>217.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>284807 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Time         V1         V2        V3        V4        V5  \\\n",
       "0            0.0  -1.359807  -0.072781  2.536347  1.378155 -0.338321   \n",
       "1            0.0   1.191857   0.266151  0.166480  0.448154  0.060018   \n",
       "2            1.0  -1.358354  -1.340163  1.773209  0.379780 -0.503198   \n",
       "3            1.0  -0.966272  -0.185226  1.792993 -0.863291 -0.010309   \n",
       "4            2.0  -1.158233   0.877737  1.548718  0.403034 -0.407193   \n",
       "...          ...        ...        ...       ...       ...       ...   \n",
       "284802  172786.0 -11.881118  10.071785 -9.834783 -2.066656 -5.364473   \n",
       "284803  172787.0  -0.732789  -0.055080  2.035030 -0.738589  0.868229   \n",
       "284804  172788.0   1.919565  -0.301254 -3.249640 -0.557828  2.630515   \n",
       "284805  172788.0  -0.240440   0.530483  0.702510  0.689799 -0.377961   \n",
       "284806  172792.0  -0.533413  -0.189733  0.703337 -0.506271 -0.012546   \n",
       "\n",
       "              V6        V7        V8        V9  ...       V21       V22  \\\n",
       "0       0.462388  0.239599  0.098698  0.363787  ... -0.018307  0.277838   \n",
       "1      -0.082361 -0.078803  0.085102 -0.255425  ... -0.225775 -0.638672   \n",
       "2       1.800499  0.791461  0.247676 -1.514654  ...  0.247998  0.771679   \n",
       "3       1.247203  0.237609  0.377436 -1.387024  ... -0.108300  0.005274   \n",
       "4       0.095921  0.592941 -0.270533  0.817739  ... -0.009431  0.798278   \n",
       "...          ...       ...       ...       ...  ...       ...       ...   \n",
       "284802 -2.606837 -4.918215  7.305334  1.914428  ...  0.213454  0.111864   \n",
       "284803  1.058415  0.024330  0.294869  0.584800  ...  0.214205  0.924384   \n",
       "284804  3.031260 -0.296827  0.708417  0.432454  ...  0.232045  0.578229   \n",
       "284805  0.623708 -0.686180  0.679145  0.392087  ...  0.265245  0.800049   \n",
       "284806 -0.649617  1.577006 -0.414650  0.486180  ...  0.261057  0.643078   \n",
       "\n",
       "             V23       V24       V25       V26       V27       V28  Amount  \\\n",
       "0      -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053  149.62   \n",
       "1       0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724    2.69   \n",
       "2       0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752  378.66   \n",
       "3      -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458  123.50   \n",
       "4      -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153   69.99   \n",
       "...          ...       ...       ...       ...       ...       ...     ...   \n",
       "284802  1.014480 -0.509348  1.436807  0.250034  0.943651  0.823731    0.77   \n",
       "284803  0.012463 -1.016226 -0.606624 -0.395255  0.068472 -0.053527   24.79   \n",
       "284804 -0.037501  0.640134  0.265745 -0.087371  0.004455 -0.026561   67.88   \n",
       "284805 -0.163298  0.123205 -0.569159  0.546668  0.108821  0.104533   10.00   \n",
       "284806  0.376777  0.008797 -0.473649 -0.818267 -0.002415  0.013649  217.00   \n",
       "\n",
       "        Class  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           0  \n",
       "4           0  \n",
       "...       ...  \n",
       "284802      0  \n",
       "284803      0  \n",
       "284804      0  \n",
       "284805      0  \n",
       "284806      0  \n",
       "\n",
       "[284807 rows x 31 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('https://storage.googleapis.com/download.tensorflow.org/data/creditcard.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try some data cleansing\n",
    "\n",
    "temp_df = df.copy()\n",
    "temp_df = temp_df.drop(['Time'], axis=1)\n",
    "temp_df['Log_Amount'] = np.log(temp_df.pop('Amount')+0.001)\n",
    "df = temp_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Class</th>\n",
       "      <th>Log_Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>0.090794</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>0</td>\n",
       "      <td>5.008105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>-0.166974</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>0</td>\n",
       "      <td>0.989913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>0.207643</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>0</td>\n",
       "      <td>5.936641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>-0.054952</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>0</td>\n",
       "      <td>4.816249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>0.753074</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>0</td>\n",
       "      <td>4.248367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284802</th>\n",
       "      <td>-11.881118</td>\n",
       "      <td>10.071785</td>\n",
       "      <td>-9.834783</td>\n",
       "      <td>-2.066656</td>\n",
       "      <td>-5.364473</td>\n",
       "      <td>-2.606837</td>\n",
       "      <td>-4.918215</td>\n",
       "      <td>7.305334</td>\n",
       "      <td>1.914428</td>\n",
       "      <td>4.356170</td>\n",
       "      <td>...</td>\n",
       "      <td>0.213454</td>\n",
       "      <td>0.111864</td>\n",
       "      <td>1.014480</td>\n",
       "      <td>-0.509348</td>\n",
       "      <td>1.436807</td>\n",
       "      <td>0.250034</td>\n",
       "      <td>0.943651</td>\n",
       "      <td>0.823731</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.260067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284803</th>\n",
       "      <td>-0.732789</td>\n",
       "      <td>-0.055080</td>\n",
       "      <td>2.035030</td>\n",
       "      <td>-0.738589</td>\n",
       "      <td>0.868229</td>\n",
       "      <td>1.058415</td>\n",
       "      <td>0.024330</td>\n",
       "      <td>0.294869</td>\n",
       "      <td>0.584800</td>\n",
       "      <td>-0.975926</td>\n",
       "      <td>...</td>\n",
       "      <td>0.214205</td>\n",
       "      <td>0.924384</td>\n",
       "      <td>0.012463</td>\n",
       "      <td>-1.016226</td>\n",
       "      <td>-0.606624</td>\n",
       "      <td>-0.395255</td>\n",
       "      <td>0.068472</td>\n",
       "      <td>-0.053527</td>\n",
       "      <td>0</td>\n",
       "      <td>3.210481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284804</th>\n",
       "      <td>1.919565</td>\n",
       "      <td>-0.301254</td>\n",
       "      <td>-3.249640</td>\n",
       "      <td>-0.557828</td>\n",
       "      <td>2.630515</td>\n",
       "      <td>3.031260</td>\n",
       "      <td>-0.296827</td>\n",
       "      <td>0.708417</td>\n",
       "      <td>0.432454</td>\n",
       "      <td>-0.484782</td>\n",
       "      <td>...</td>\n",
       "      <td>0.232045</td>\n",
       "      <td>0.578229</td>\n",
       "      <td>-0.037501</td>\n",
       "      <td>0.640134</td>\n",
       "      <td>0.265745</td>\n",
       "      <td>-0.087371</td>\n",
       "      <td>0.004455</td>\n",
       "      <td>-0.026561</td>\n",
       "      <td>0</td>\n",
       "      <td>4.217756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284805</th>\n",
       "      <td>-0.240440</td>\n",
       "      <td>0.530483</td>\n",
       "      <td>0.702510</td>\n",
       "      <td>0.689799</td>\n",
       "      <td>-0.377961</td>\n",
       "      <td>0.623708</td>\n",
       "      <td>-0.686180</td>\n",
       "      <td>0.679145</td>\n",
       "      <td>0.392087</td>\n",
       "      <td>-0.399126</td>\n",
       "      <td>...</td>\n",
       "      <td>0.265245</td>\n",
       "      <td>0.800049</td>\n",
       "      <td>-0.163298</td>\n",
       "      <td>0.123205</td>\n",
       "      <td>-0.569159</td>\n",
       "      <td>0.546668</td>\n",
       "      <td>0.108821</td>\n",
       "      <td>0.104533</td>\n",
       "      <td>0</td>\n",
       "      <td>2.302685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284806</th>\n",
       "      <td>-0.533413</td>\n",
       "      <td>-0.189733</td>\n",
       "      <td>0.703337</td>\n",
       "      <td>-0.506271</td>\n",
       "      <td>-0.012546</td>\n",
       "      <td>-0.649617</td>\n",
       "      <td>1.577006</td>\n",
       "      <td>-0.414650</td>\n",
       "      <td>0.486180</td>\n",
       "      <td>-0.915427</td>\n",
       "      <td>...</td>\n",
       "      <td>0.261057</td>\n",
       "      <td>0.643078</td>\n",
       "      <td>0.376777</td>\n",
       "      <td>0.008797</td>\n",
       "      <td>-0.473649</td>\n",
       "      <td>-0.818267</td>\n",
       "      <td>-0.002415</td>\n",
       "      <td>0.013649</td>\n",
       "      <td>0</td>\n",
       "      <td>5.379902</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>284807 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               V1         V2        V3        V4        V5        V6  \\\n",
       "0       -1.359807  -0.072781  2.536347  1.378155 -0.338321  0.462388   \n",
       "1        1.191857   0.266151  0.166480  0.448154  0.060018 -0.082361   \n",
       "2       -1.358354  -1.340163  1.773209  0.379780 -0.503198  1.800499   \n",
       "3       -0.966272  -0.185226  1.792993 -0.863291 -0.010309  1.247203   \n",
       "4       -1.158233   0.877737  1.548718  0.403034 -0.407193  0.095921   \n",
       "...           ...        ...       ...       ...       ...       ...   \n",
       "284802 -11.881118  10.071785 -9.834783 -2.066656 -5.364473 -2.606837   \n",
       "284803  -0.732789  -0.055080  2.035030 -0.738589  0.868229  1.058415   \n",
       "284804   1.919565  -0.301254 -3.249640 -0.557828  2.630515  3.031260   \n",
       "284805  -0.240440   0.530483  0.702510  0.689799 -0.377961  0.623708   \n",
       "284806  -0.533413  -0.189733  0.703337 -0.506271 -0.012546 -0.649617   \n",
       "\n",
       "              V7        V8        V9       V10  ...       V21       V22  \\\n",
       "0       0.239599  0.098698  0.363787  0.090794  ... -0.018307  0.277838   \n",
       "1      -0.078803  0.085102 -0.255425 -0.166974  ... -0.225775 -0.638672   \n",
       "2       0.791461  0.247676 -1.514654  0.207643  ...  0.247998  0.771679   \n",
       "3       0.237609  0.377436 -1.387024 -0.054952  ... -0.108300  0.005274   \n",
       "4       0.592941 -0.270533  0.817739  0.753074  ... -0.009431  0.798278   \n",
       "...          ...       ...       ...       ...  ...       ...       ...   \n",
       "284802 -4.918215  7.305334  1.914428  4.356170  ...  0.213454  0.111864   \n",
       "284803  0.024330  0.294869  0.584800 -0.975926  ...  0.214205  0.924384   \n",
       "284804 -0.296827  0.708417  0.432454 -0.484782  ...  0.232045  0.578229   \n",
       "284805 -0.686180  0.679145  0.392087 -0.399126  ...  0.265245  0.800049   \n",
       "284806  1.577006 -0.414650  0.486180 -0.915427  ...  0.261057  0.643078   \n",
       "\n",
       "             V23       V24       V25       V26       V27       V28  Class  \\\n",
       "0      -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053      0   \n",
       "1       0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724      0   \n",
       "2       0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752      0   \n",
       "3      -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458      0   \n",
       "4      -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153      0   \n",
       "...          ...       ...       ...       ...       ...       ...    ...   \n",
       "284802  1.014480 -0.509348  1.436807  0.250034  0.943651  0.823731      0   \n",
       "284803  0.012463 -1.016226 -0.606624 -0.395255  0.068472 -0.053527      0   \n",
       "284804 -0.037501  0.640134  0.265745 -0.087371  0.004455 -0.026561      0   \n",
       "284805 -0.163298  0.123205 -0.569159  0.546668  0.108821  0.104533      0   \n",
       "284806  0.376777  0.008797 -0.473649 -0.818267 -0.002415  0.013649      0   \n",
       "\n",
       "        Log_Amount  \n",
       "0         5.008105  \n",
       "1         0.989913  \n",
       "2         5.936641  \n",
       "3         4.816249  \n",
       "4         4.248367  \n",
       "...            ...  \n",
       "284802   -0.260067  \n",
       "284803    3.210481  \n",
       "284804    4.217756  \n",
       "284805    2.302685  \n",
       "284806    5.379902  \n",
       "\n",
       "[284807 rows x 30 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original\n",
      "[[     0      1]\n",
      " [284315    492]]\n",
      "Train\n",
      "[[     0      1]\n",
      " [199020    344]]\n",
      "Train\n",
      "[[    0     1]\n",
      " [42652    69]]\n",
      "Train\n",
      "[[    0     1]\n",
      " [42643    79]]\n"
     ]
    }
   ],
   "source": [
    "X = df.loc[:, df.columns != 'Class']\n",
    "y = df.loc[:, df.columns == 'Class']\n",
    "OrigPct = CalcPct(y,\"Original\")\n",
    "\n",
    "#print(y['Class'].value_counts())\n",
    "# split the dataset\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = None, shuffle=True)\n",
    "test_size = 0.3\n",
    "val_size = 0.5\n",
    "\n",
    "strat = True\n",
    "if (strat == True):\n",
    "    stratify=y['Class']\n",
    "else:\n",
    "    stratify=\"None\"\n",
    "# stratify will ensure that Train, Test and Validation get the same pct of minority classes (.17%)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = None, shuffle=True, stratify=stratify)\n",
    "X_train, X_test1, y_train, y_test1 = train_test_split(X,y, test_size = test_size, random_state = None, shuffle=True, stratify=stratify)\n",
    "# then split Test1 into Test and Validate\n",
    "# Validate will be used as a final benchmark, once all the parameter tuning is completed\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test1,y_test1, test_size = val_size, random_state = None, shuffle=True)\n",
    "\n",
    "TrainPct = CalcPct(y_train,\"Train\")\n",
    "TestPct = CalcPct(y_test,\"Train\")\n",
    "ValPct = CalcPct(y_val,\"Train\")\n",
    "zeros, ones = np.bincount(y_train['Class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final data validation\n",
    "\n",
    "# Form np arrays of labels and features.\n",
    "\n",
    "train_labels = np.array(y_train).flatten()\n",
    "bool_train_labels = train_labels != 0 # has an extra ,1 in the bool_train_labels.shape\n",
    "val_labels = np.array(y_val)\n",
    "test_labels = np.array(y_test)\n",
    "train_features = np.array(X_train)\n",
    "val_features = np.array(X_val)\n",
    "test_features = np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pos_df = pd.DataFrame(train_features[ bool_train_labels], columns = X.columns)\n",
    "neg_df = pd.DataFrame(train_features[~bool_train_labels], columns = X.columns)\n",
    "\n",
    "sns.jointplot(pos_df['V5'], pos_df['V6'],\n",
    "              kind='hex', xlim = (-5,5), ylim = (-5,5))\n",
    "plt.suptitle(\"Positive distribution\")\n",
    "\n",
    "sns.jointplot(neg_df['V5'], neg_df['V6'],\n",
    "              kind='hex', xlim = (-5,5), ylim = (-5,5))\n",
    "_ = plt.suptitle(\"Negative distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainDownSampled\n",
      "[[  0   1]\n",
      " [344 344]]\n",
      "Test\n",
      "[[    0     1]\n",
      " [42652    69]]\n",
      "Val\n",
      "[[    0     1]\n",
      " [42643    79]]\n"
     ]
    }
   ],
   "source": [
    "# prepare the Train, Test and Validation data for the model. Need input to include arrays with certain shape\n",
    "\n",
    "# downsample the Train data\n",
    "\n",
    "# find the number of minority (value=1) samples in our dataset so we can down-sample our majority to it\n",
    "yes = len(y_train[y_train['Class'] ==1])\n",
    "\n",
    "# retrieve the indices of the minority and majority samples \n",
    "yes_ind = y_train[y_train['Class'] == 1].index\n",
    "no_ind = y_train[y_train['Class'] == 0].index\n",
    "\n",
    "# random sample the majority indices based on the amount of \n",
    "# minority samples\n",
    "new_no_ind = np.random.choice(no_ind, yes, replace = False)\n",
    "\n",
    "# merge the two indices together\n",
    "undersample_ind = np.concatenate([new_no_ind, yes_ind])\n",
    "\n",
    "# get undersampled dataframe from the merged indices of the dataset\n",
    "X_train = X_train.loc[undersample_ind]\n",
    "y_temp = y_train.loc[undersample_ind]\n",
    "y_train_original = np.array(y_temp['Class'], dtype='int')\n",
    "TrainDownPct = CalcPct(y_train_original,\"TrainDownSampled\")\n",
    "y_test_original = np.array(y_test['Class'], dtype='int')\n",
    "TestPct = CalcPct(y_test_original,\"Test\")\n",
    "y_val_original = np.array(y_val['Class'], dtype='int')\n",
    "ValPct = CalcPct(y_val_original,\"Val\")\n",
    "\n",
    "# normalize the features\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "X_val = sc.transform(X_val)\n",
    "\n",
    "# handle any extreme fliers, set to 5 or -5\n",
    "X_train = np.clip(X_train, -5, 5)\n",
    "X_test = np.clip(X_test, -5, 5)\n",
    "X_val = np.clip(X_val, -5, 5)\n",
    "\n",
    "#shape the Train data \n",
    "rolling_window_size = 10  ### this selects how many historical transactions should be analyzed to judge the transaction at hand -- RNN width\n",
    "\n",
    "X_train_interim = np.zeros([(X_train.shape[0]-rolling_window_size)*rolling_window_size,X_train.shape[1]]) # change 30 to a variable\n",
    "y_train = []\n",
    "for i in range((X_train.shape[0]-rolling_window_size)):\n",
    "    beg = 0+i\n",
    "    end = beg+rolling_window_size\n",
    "    s = np.array(X_train[beg:end], dtype='float')\n",
    "    X_train_interim[(rolling_window_size*i):(rolling_window_size*(i+1)),:] = s\n",
    "    y_train.append(y_train_original[end])\n",
    "\n",
    "y_train = np.array(y_train, dtype='int')\n",
    "X_train_interim = X_train_interim[:,1::]\n",
    "\n",
    "X_train_tensor = X_train_interim.reshape(int(np.shape(X_train_interim)[0]/rolling_window_size), rolling_window_size, np.shape(X_train_interim)[1])\n",
    "X_train = X_train_tensor\n",
    "\n",
    "#shape the Test data for LSTM\n",
    "\n",
    "X_test_interim = np.zeros([(X_test.shape[0]-rolling_window_size)*rolling_window_size,X_test.shape[1]])\n",
    "y_test = []\n",
    "for i in range((X_test.shape[0]-rolling_window_size)):\n",
    "    beg = 0+i\n",
    "    end = beg+rolling_window_size\n",
    "    s = np.array(X_test[beg:end], dtype='float')\n",
    "    X_test_interim[(rolling_window_size*i):(rolling_window_size*(i+1)),:] = s\n",
    "    y_test.append(y_test_original[end]) \n",
    "\n",
    "y_test = np.array(y_test, dtype='int')\n",
    "X_test_interim = X_test_interim[:,1::]\n",
    "\n",
    "X_test_tensor = X_test_interim.reshape(int(np.shape(X_test_interim)[0]/rolling_window_size), rolling_window_size, np.shape(X_test_interim)[1])\n",
    "X_test = X_test_tensor\n",
    "\n",
    "#shape the Val data for LSTM\n",
    "\n",
    "X_val_interim = np.zeros([(X_val.shape[0]-rolling_window_size)*rolling_window_size,X_val.shape[1]])\n",
    "y_val = []\n",
    "for i in range((X_val.shape[0]-rolling_window_size)):\n",
    "    beg = 0+i\n",
    "    end = beg+rolling_window_size\n",
    "    s = np.array(X_val[beg:end], dtype='float')\n",
    "    X_val_interim[(rolling_window_size*i):(rolling_window_size*(i+1)),:] = s\n",
    "    y_val.append(y_val_original[end]) \n",
    "\n",
    "y_val = np.array(y_val, dtype='int')\n",
    "X_val_interim = X_val_interim[:,1::]\n",
    "\n",
    "X_val_tensor = X_val_interim.reshape(int(np.shape(X_val_interim)[0]/rolling_window_size), rolling_window_size, np.shape(X_val_interim)[1])\n",
    "X_val = X_val_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final data validation\n",
    "\n",
    "# Form np arrays of labels and features.\n",
    "\n",
    "train_labels = np.array(y_train)\n",
    "bool_train_labels = train_labels != 0\n",
    "val_labels = np.array(y_val)\n",
    "test_labels = np.array(y_test)\n",
    "\n",
    "train_features = np.array(X_train)\n",
    "val_features = np.array(X_val)\n",
    "test_features = np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bias_initializer 0.0017284695005527083\n",
      "Initial Loss: 0.7518\n",
      "['loss', 'tp', 'fp', 'tn', 'fn', 'accuracy', 'precision', 'recall', 'auc']\n",
      "[0.7517620921134949, 344.0, 334.0, 0.0, 0.0, 0.50737464427948, 0.50737464427948, 1.0, 0.5]\n",
      "Train on 678 samples, validate on 42712 samples\n",
      "Epoch 1/100\n",
      "678/678 [==============================] - 5s 7ms/step - loss: 0.7518 - tp: 688.0000 - fp: 668.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - accuracy: 0.5074 - precision: 0.5074 - recall: 1.0000 - auc: 0.5000 - val_loss: 1.1039 - val_tp: 732.3636 - val_fp: 24986.5449 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.0416 - val_precision: 0.0416 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 2/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.7483 - tp: 1111.0000 - fp: 43635.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - accuracy: 0.0248 - precision: 0.0248 - recall: 1.0000 - auc: 0.5000 - val_loss: 1.0905 - val_tp: 1155.3636 - val_fp: 67953.5469 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.0173 - val_precision: 0.0173 - val_recall: 1.0000 - val_auc: 0.6480\n",
      "Epoch 3/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.7445 - tp: 1534.0000 - fp: 86602.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - accuracy: 0.0174 - precision: 0.0174 - recall: 1.0000 - auc: 0.6102 - val_loss: 1.0762 - val_tp: 1578.3636 - val_fp: 110920.5469 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.0142 - val_precision: 0.0142 - val_recall: 1.0000 - val_auc: 0.6754\n",
      "Epoch 4/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.7403 - tp: 1957.0000 - fp: 129569.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - accuracy: 0.0149 - precision: 0.0149 - recall: 1.0000 - auc: 0.6155 - val_loss: 1.0604 - val_tp: 2001.3636 - val_fp: 153887.5469 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.0129 - val_precision: 0.0129 - val_recall: 1.0000 - val_auc: 0.6609\n",
      "Epoch 5/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.7350 - tp: 2380.0000 - fp: 172536.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - accuracy: 0.0136 - precision: 0.0136 - recall: 1.0000 - auc: 0.6255 - val_loss: 1.0412 - val_tp: 2424.3635 - val_fp: 196854.5469 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.0122 - val_precision: 0.0122 - val_recall: 1.0000 - val_auc: 0.6593\n",
      "Epoch 6/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.7278 - tp: 2803.0000 - fp: 215503.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - accuracy: 0.0128 - precision: 0.0128 - recall: 1.0000 - auc: 0.6329 - val_loss: 1.0155 - val_tp: 2847.3635 - val_fp: 239821.5469 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.0118 - val_precision: 0.0118 - val_recall: 1.0000 - val_auc: 0.6596\n",
      "Epoch 7/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.7166 - tp: 3226.0000 - fp: 258470.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - accuracy: 0.0123 - precision: 0.0123 - recall: 1.0000 - auc: 0.6446 - val_loss: 0.9782 - val_tp: 3270.3635 - val_fp: 282788.5312 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.0115 - val_precision: 0.0115 - val_recall: 1.0000 - val_auc: 0.6660\n",
      "Epoch 8/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.6996 - tp: 3649.0000 - fp: 301437.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - accuracy: 0.0120 - precision: 0.0120 - recall: 1.0000 - auc: 0.6580 - val_loss: 0.9273 - val_tp: 3693.3635 - val_fp: 325755.5312 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.0112 - val_precision: 0.0112 - val_recall: 1.0000 - val_auc: 0.6754\n",
      "Epoch 9/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.6736 - tp: 4072.0000 - fp: 344404.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - accuracy: 0.0117 - precision: 0.0117 - recall: 1.0000 - auc: 0.6757 - val_loss: 0.8569 - val_tp: 4116.3638 - val_fp: 368722.5312 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.0111 - val_precision: 0.0111 - val_recall: 1.0000 - val_auc: 0.6896\n",
      "Epoch 10/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.6371 - tp: 4495.0000 - fp: 387371.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - accuracy: 0.0115 - precision: 0.0115 - recall: 1.0000 - auc: 0.7007 - val_loss: 0.7429 - val_tp: 4539.3638 - val_fp: 411689.5312 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.0109 - val_precision: 0.0109 - val_recall: 1.0000 - val_auc: 0.7114\n",
      "Epoch 11/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.5728 - tp: 4918.0000 - fp: 430326.0000 - tn: 12.0000 - fn: 0.0000e+00 - accuracy: 0.0113 - precision: 0.0113 - recall: 1.0000 - auc: 0.7297 - val_loss: 0.6525 - val_tp: 4918.0000 - val_fp: 430331.6250 - val_tn: 24324.9082 - val_fn: 44.3636 - val_accuracy: 0.0629 - val_precision: 0.0113 - val_recall: 0.9911 - val_auc: 0.7376\n",
      "Epoch 12/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.5117 - tp: 5256.0000 - fp: 430395.0000 - tn: 42910.0000 - fn: 85.0000 - accuracy: 0.1006 - precision: 0.0121 - recall: 0.9841 - auc: 0.7555 - val_loss: 0.6379 - val_tp: 5256.0000 - val_fp: 430395.0000 - val_tn: 67228.5469 - val_fn: 129.3636 - val_accuracy: 0.1436 - val_precision: 0.0121 - val_recall: 0.9760 - val_auc: 0.7613\n",
      "Epoch 13/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4721 - tp: 5588.0000 - fp: 430396.0000 - tn: 85876.0000 - fn: 176.0000 - accuracy: 0.1752 - precision: 0.0128 - recall: 0.9695 - auc: 0.7764 - val_loss: 0.6330 - val_tp: 5588.0000 - val_fp: 430396.0000 - val_tn: 110194.5469 - val_fn: 220.3636 - val_accuracy: 0.2115 - val_precision: 0.0128 - val_recall: 0.9621 - val_auc: 0.7805\n",
      "Epoch 14/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4563 - tp: 5920.0000 - fp: 430396.0000 - tn: 128843.0000 - fn: 267.0000 - accuracy: 0.2383 - precision: 0.0136 - recall: 0.9568 - auc: 0.7934 - val_loss: 0.6287 - val_tp: 5920.0000 - val_fp: 430396.0000 - val_tn: 153161.5469 - val_fn: 311.3636 - val_accuracy: 0.2694 - val_precision: 0.0136 - val_recall: 0.9500 - val_auc: 0.7964\n",
      "Epoch 15/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4524 - tp: 6253.0000 - fp: 430396.0000 - tn: 171810.0000 - fn: 357.0000 - accuracy: 0.2925 - precision: 0.0143 - recall: 0.9460 - auc: 0.8070 - val_loss: 0.6245 - val_tp: 6253.0000 - val_fp: 430396.0000 - val_tn: 196128.5469 - val_fn: 401.3636 - val_accuracy: 0.3194 - val_precision: 0.0143 - val_recall: 0.9397 - val_auc: 0.8091\n",
      "Epoch 16/100\n",
      "678/678 [==============================] - 2s 2ms/step - loss: 0.4498 - tp: 6586.0000 - fp: 430396.0000 - tn: 214777.0000 - fn: 447.0000 - accuracy: 0.3394 - precision: 0.0151 - recall: 0.9364 - auc: 0.8182 - val_loss: 0.6203 - val_tp: 6586.0000 - val_fp: 430396.0000 - val_tn: 239095.5469 - val_fn: 491.3636 - val_accuracy: 0.3629 - val_precision: 0.0151 - val_recall: 0.9306 - val_auc: 0.8198\n",
      "Epoch 17/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4469 - tp: 6921.0000 - fp: 430396.0000 - tn: 257744.0000 - fn: 535.0000 - accuracy: 0.3805 - precision: 0.0158 - recall: 0.9282 - auc: 0.8277 - val_loss: 0.6163 - val_tp: 6921.0000 - val_fp: 430399.8125 - val_tn: 282058.7188 - val_fn: 579.3636 - val_accuracy: 0.4012 - val_precision: 0.0158 - val_recall: 0.9228 - val_auc: 0.8287\n",
      "Epoch 18/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4444 - tp: 7256.0000 - fp: 430404.0000 - tn: 300703.0000 - fn: 623.0000 - accuracy: 0.4167 - precision: 0.0166 - recall: 0.9209 - auc: 0.8356 - val_loss: 0.6124 - val_tp: 7256.0000 - val_fp: 430412.2812 - val_tn: 325013.2812 - val_fn: 667.3636 - val_accuracy: 0.4351 - val_precision: 0.0166 - val_recall: 0.9158 - val_auc: 0.8363\n",
      "Epoch 19/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4426 - tp: 7591.0000 - fp: 430423.0000 - tn: 343651.0000 - fn: 711.0000 - accuracy: 0.4489 - precision: 0.0173 - recall: 0.9144 - auc: 0.8424 - val_loss: 0.6086 - val_tp: 7591.0000 - val_fp: 430439.7188 - val_tn: 367952.8125 - val_fn: 755.3636 - val_accuracy: 0.4654 - val_precision: 0.0173 - val_recall: 0.9095 - val_auc: 0.8427\n",
      "Epoch 20/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4400 - tp: 7927.0000 - fp: 430459.0000 - tn: 386582.0000 - fn: 798.0000 - accuracy: 0.4777 - precision: 0.0181 - recall: 0.9085 - auc: 0.8482 - val_loss: 0.6050 - val_tp: 7927.0000 - val_fp: 430489.7188 - val_tn: 410869.8125 - val_fn: 842.3636 - val_accuracy: 0.4925 - val_precision: 0.0181 - val_recall: 0.9039 - val_auc: 0.8482\n",
      "Epoch 21/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4382 - tp: 8263.0000 - fp: 430521.0000 - tn: 429487.0000 - fn: 885.0000 - accuracy: 0.5036 - precision: 0.0188 - recall: 0.9033 - auc: 0.8531 - val_loss: 0.6014 - val_tp: 8263.0000 - val_fp: 430559.5312 - val_tn: 453767.0000 - val_fn: 929.3636 - val_accuracy: 0.5170 - val_precision: 0.0188 - val_recall: 0.8989 - val_auc: 0.8530\n",
      "Epoch 22/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4363 - tp: 8599.0000 - fp: 430599.0000 - tn: 472376.0000 - fn: 972.0000 - accuracy: 0.5271 - precision: 0.0196 - recall: 0.8984 - auc: 0.8573 - val_loss: 0.5977 - val_tp: 8599.0000 - val_fp: 430645.2812 - val_tn: 496648.2812 - val_fn: 1016.3636 - val_accuracy: 0.5392 - val_precision: 0.0196 - val_recall: 0.8943 - val_auc: 0.8571\n",
      "Epoch 23/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4344 - tp: 8935.0000 - fp: 430689.0000 - tn: 515253.0000 - fn: 1059.0000 - accuracy: 0.5484 - precision: 0.0203 - recall: 0.8940 - auc: 0.8610 - val_loss: 0.5940 - val_tp: 8936.0000 - val_fp: 430742.6250 - val_tn: 539517.9375 - val_fn: 1102.3636 - val_accuracy: 0.5594 - val_precision: 0.0203 - val_recall: 0.8902 - val_auc: 0.8608\n",
      "Epoch 24/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4317 - tp: 9273.0000 - fp: 430792.0000 - tn: 558117.0000 - fn: 1144.0000 - accuracy: 0.5678 - precision: 0.0211 - recall: 0.8902 - auc: 0.8644 - val_loss: 0.5905 - val_tp: 9274.0000 - val_fp: 430857.4688 - val_tn: 582370.0625 - val_fn: 1187.3636 - val_accuracy: 0.5779 - val_precision: 0.0211 - val_recall: 0.8865 - val_auc: 0.8641\n",
      "Epoch 25/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4298 - tp: 9611.0000 - fp: 430916.0000 - tn: 600960.0000 - fn: 1229.0000 - accuracy: 0.5856 - precision: 0.0218 - recall: 0.8866 - auc: 0.8674 - val_loss: 0.5870 - val_tp: 9612.0000 - val_fp: 430991.0000 - val_tn: 625203.5625 - val_fn: 1272.3636 - val_accuracy: 0.5949 - val_precision: 0.0218 - val_recall: 0.8831 - val_auc: 0.8669\n",
      "Epoch 26/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4278 - tp: 9949.0000 - fp: 431058.0000 - tn: 643785.0000 - fn: 1314.0000 - accuracy: 0.6019 - precision: 0.0226 - recall: 0.8833 - auc: 0.8699 - val_loss: 0.5835 - val_tp: 9950.0000 - val_fp: 431143.9062 - val_tn: 668017.6250 - val_fn: 1357.3636 - val_accuracy: 0.6105 - val_precision: 0.0226 - val_recall: 0.8800 - val_auc: 0.8695\n",
      "Epoch 27/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4250 - tp: 10288.0000 - fp: 431221.0000 - tn: 686589.0000 - fn: 1398.0000 - accuracy: 0.6170 - precision: 0.0233 - recall: 0.8804 - auc: 0.8723 - val_loss: 0.5801 - val_tp: 10289.0000 - val_fp: 431316.8125 - val_tn: 710811.7500 - val_fn: 1441.3636 - val_accuracy: 0.6249 - val_precision: 0.0233 - val_recall: 0.8771 - val_auc: 0.8718\n",
      "Epoch 28/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4220 - tp: 10628.0000 - fp: 431405.0000 - tn: 729372.0000 - fn: 1481.0000 - accuracy: 0.6309 - precision: 0.0240 - recall: 0.8777 - auc: 0.8745 - val_loss: 0.5771 - val_tp: 10629.0000 - val_fp: 431519.1875 - val_tn: 753576.3750 - val_fn: 1524.3636 - val_accuracy: 0.6383 - val_precision: 0.0240 - val_recall: 0.8746 - val_auc: 0.8740\n",
      "Epoch 29/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4194 - tp: 10969.0000 - fp: 431626.0000 - tn: 772118.0000 - fn: 1563.0000 - accuracy: 0.6438 - precision: 0.0248 - recall: 0.8753 - auc: 0.8765 - val_loss: 0.5743 - val_tp: 10970.0000 - val_fp: 431770.0000 - val_tn: 796292.5625 - val_fn: 1606.3636 - val_accuracy: 0.6506 - val_precision: 0.0248 - val_recall: 0.8723 - val_auc: 0.8759\n",
      "Epoch 30/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4165 - tp: 11311.0000 - fp: 431904.0000 - tn: 814807.0000 - fn: 1644.0000 - accuracy: 0.6558 - precision: 0.0255 - recall: 0.8731 - auc: 0.8784 - val_loss: 0.5718 - val_tp: 11312.0000 - val_fp: 432082.2812 - val_tn: 838947.2500 - val_fn: 1687.3636 - val_accuracy: 0.6621 - val_precision: 0.0255 - val_recall: 0.8702 - val_auc: 0.8777\n",
      "Epoch 31/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4137 - tp: 11654.0000 - fp: 432245.0000 - tn: 857433.0000 - fn: 1724.0000 - accuracy: 0.6670 - precision: 0.0263 - recall: 0.8711 - auc: 0.8801 - val_loss: 0.5707 - val_tp: 11655.0000 - val_fp: 432503.2812 - val_tn: 881493.2500 - val_fn: 1767.3636 - val_accuracy: 0.6728 - val_precision: 0.0262 - val_recall: 0.8683 - val_auc: 0.8793\n",
      "Epoch 32/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4117 - tp: 11997.0000 - fp: 432730.0000 - tn: 899915.0000 - fn: 1804.0000 - accuracy: 0.6773 - precision: 0.0270 - recall: 0.8693 - auc: 0.8815 - val_loss: 0.5745 - val_tp: 11998.8184 - val_fp: 433226.5312 - val_tn: 923737.0000 - val_fn: 1846.5454 - val_accuracy: 0.6826 - val_precision: 0.0269 - val_recall: 0.8666 - val_auc: 0.8808\n",
      "Epoch 33/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4114 - tp: 12343.0000 - fp: 433656.0000 - tn: 941956.0000 - fn: 1881.0000 - accuracy: 0.6866 - precision: 0.0277 - recall: 0.8678 - auc: 0.8830 - val_loss: 0.5640 - val_tp: 12344.0000 - val_fp: 433931.0000 - val_tn: 965999.5625 - val_fn: 1924.3636 - val_accuracy: 0.6918 - val_precision: 0.0277 - val_recall: 0.8651 - val_auc: 0.8822\n",
      "Epoch 34/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4075 - tp: 12686.0000 - fp: 434175.0000 - tn: 984404.0000 - fn: 1961.0000 - accuracy: 0.6957 - precision: 0.0284 - recall: 0.8661 - auc: 0.8841 - val_loss: 0.5580 - val_tp: 12687.0000 - val_fp: 434378.0000 - val_tn: 1008519.5625 - val_fn: 2004.3636 - val_accuracy: 0.7006 - val_precision: 0.0284 - val_recall: 0.8636 - val_auc: 0.8834\n",
      "Epoch 35/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4057 - tp: 13029.0000 - fp: 434563.0000 - tn: 1026983.0000 - fn: 2041.0000 - accuracy: 0.7043 - precision: 0.0291 - recall: 0.8646 - auc: 0.8852 - val_loss: 0.5533 - val_tp: 13030.0000 - val_fp: 434734.0938 - val_tn: 1051130.5000 - val_fn: 2084.3635 - val_accuracy: 0.7090 - val_precision: 0.0291 - val_recall: 0.8621 - val_auc: 0.8845\n",
      "Epoch 36/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4038 - tp: 13372.0000 - fp: 434893.0000 - tn: 1069620.0000 - fn: 2121.0000 - accuracy: 0.7125 - precision: 0.0298 - recall: 0.8631 - auc: 0.8863 - val_loss: 0.5494 - val_tp: 13373.0000 - val_fp: 435053.8125 - val_tn: 1093777.7500 - val_fn: 2164.3635 - val_accuracy: 0.7169 - val_precision: 0.0298 - val_recall: 0.8607 - val_auc: 0.8856\n",
      "Epoch 37/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4016 - tp: 13715.0000 - fp: 435204.0000 - tn: 1112276.0000 - fn: 2201.0000 - accuracy: 0.7202 - precision: 0.0306 - recall: 0.8617 - auc: 0.8872 - val_loss: 0.5458 - val_tp: 13716.0000 - val_fp: 435361.2812 - val_tn: 1136437.2500 - val_fn: 2244.3635 - val_accuracy: 0.7244 - val_precision: 0.0305 - val_recall: 0.8594 - val_auc: 0.8865\n",
      "Epoch 38/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3995 - tp: 14058.0000 - fp: 435506.0000 - tn: 1154941.0000 - fn: 2281.0000 - accuracy: 0.7275 - precision: 0.0313 - recall: 0.8604 - auc: 0.8881 - val_loss: 0.5421 - val_tp: 14059.0000 - val_fp: 435658.5312 - val_tn: 1179107.0000 - val_fn: 2324.3635 - val_accuracy: 0.7315 - val_precision: 0.0313 - val_recall: 0.8581 - val_auc: 0.8874\n",
      "Epoch 39/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3976 - tp: 14401.0000 - fp: 435801.0000 - tn: 1197613.0000 - fn: 2361.0000 - accuracy: 0.7345 - precision: 0.0320 - recall: 0.8591 - auc: 0.8890 - val_loss: 0.5385 - val_tp: 14402.0000 - val_fp: 435950.1875 - val_tn: 1221782.3750 - val_fn: 2404.3635 - val_accuracy: 0.7382 - val_precision: 0.0320 - val_recall: 0.8569 - val_auc: 0.8882\n",
      "Epoch 40/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3956 - tp: 14744.0000 - fp: 436090.0000 - tn: 1240291.0000 - fn: 2441.0000 - accuracy: 0.7411 - precision: 0.0327 - recall: 0.8580 - auc: 0.8897 - val_loss: 0.5351 - val_tp: 14745.0000 - val_fp: 436238.8125 - val_tn: 1264460.7500 - val_fn: 2484.3635 - val_accuracy: 0.7446 - val_precision: 0.0327 - val_recall: 0.8558 - val_auc: 0.8890\n",
      "Epoch 41/100\n",
      "678/678 [==============================] - 2s 3ms/step - loss: 0.3936 - tp: 15087.0000 - fp: 436380.0000 - tn: 1282968.0000 - fn: 2521.0000 - accuracy: 0.7473 - precision: 0.0334 - recall: 0.8568 - auc: 0.8904 - val_loss: 0.5318 - val_tp: 15088.0000 - val_fp: 436531.4688 - val_tn: 1307135.1250 - val_fn: 2564.3635 - val_accuracy: 0.7507 - val_precision: 0.0334 - val_recall: 0.8547 - val_auc: 0.8897\n",
      "Epoch 42/100\n",
      "678/678 [==============================] - 2s 2ms/step - loss: 0.3917 - tp: 15430.0000 - fp: 436676.0000 - tn: 1325639.0000 - fn: 2601.0000 - accuracy: 0.7533 - precision: 0.0341 - recall: 0.8557 - auc: 0.8911 - val_loss: 0.5285 - val_tp: 15431.0000 - val_fp: 436828.3750 - val_tn: 1349805.1250 - val_fn: 2644.3635 - val_accuracy: 0.7565 - val_precision: 0.0341 - val_recall: 0.8537 - val_auc: 0.8904\n",
      "Epoch 43/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3897 - tp: 15773.0000 - fp: 436974.0000 - tn: 1368308.0000 - fn: 2681.0000 - accuracy: 0.7589 - precision: 0.0348 - recall: 0.8547 - auc: 0.8917 - val_loss: 0.5252 - val_tp: 15774.0000 - val_fp: 437127.3750 - val_tn: 1392473.1250 - val_fn: 2724.3635 - val_accuracy: 0.7620 - val_precision: 0.0348 - val_recall: 0.8527 - val_auc: 0.8910\n",
      "Epoch 44/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3878 - tp: 16116.0000 - fp: 437274.0000 - tn: 1410975.0000 - fn: 2761.0000 - accuracy: 0.7643 - precision: 0.0355 - recall: 0.8537 - auc: 0.8922 - val_loss: 0.5219 - val_tp: 16117.0000 - val_fp: 437430.1875 - val_tn: 1435137.3750 - val_fn: 2804.3635 - val_accuracy: 0.7672 - val_precision: 0.0355 - val_recall: 0.8518 - val_auc: 0.8915\n",
      "Epoch 45/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3859 - tp: 16459.0000 - fp: 437579.0000 - tn: 1453637.0000 - fn: 2841.0000 - accuracy: 0.7695 - precision: 0.0363 - recall: 0.8528 - auc: 0.8928 - val_loss: 0.5186 - val_tp: 16460.0000 - val_fp: 437736.5312 - val_tn: 1477798.0000 - val_fn: 2884.3635 - val_accuracy: 0.7723 - val_precision: 0.0362 - val_recall: 0.8509 - val_auc: 0.8921\n",
      "Epoch 46/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3839 - tp: 16802.0000 - fp: 437884.0000 - tn: 1496299.0000 - fn: 2921.0000 - accuracy: 0.7744 - precision: 0.0370 - recall: 0.8519 - auc: 0.8932 - val_loss: 0.5154 - val_tp: 16803.0000 - val_fp: 438043.7188 - val_tn: 1520457.8750 - val_fn: 2964.3635 - val_accuracy: 0.7771 - val_precision: 0.0369 - val_recall: 0.8500 - val_auc: 0.8926\n",
      "Epoch 47/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3820 - tp: 17145.0000 - fp: 438192.0000 - tn: 1538958.0000 - fn: 3001.0000 - accuracy: 0.7791 - precision: 0.0377 - recall: 0.8510 - auc: 0.8937 - val_loss: 0.5122 - val_tp: 17146.0000 - val_fp: 438353.4688 - val_tn: 1563115.1250 - val_fn: 3044.3635 - val_accuracy: 0.7817 - val_precision: 0.0376 - val_recall: 0.8492 - val_auc: 0.8930\n",
      "Epoch 48/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3802 - tp: 17488.0000 - fp: 438502.0000 - tn: 1581615.0000 - fn: 3081.0000 - accuracy: 0.7836 - precision: 0.0384 - recall: 0.8502 - auc: 0.8941 - val_loss: 0.5091 - val_tp: 17489.0000 - val_fp: 438665.4688 - val_tn: 1605770.1250 - val_fn: 3124.3635 - val_accuracy: 0.7861 - val_precision: 0.0383 - val_recall: 0.8484 - val_auc: 0.8934\n",
      "Epoch 49/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3783 - tp: 17831.0000 - fp: 438814.0000 - tn: 1624270.0000 - fn: 3161.0000 - accuracy: 0.7879 - precision: 0.0390 - recall: 0.8494 - auc: 0.8945 - val_loss: 0.5059 - val_tp: 17832.0000 - val_fp: 438979.0000 - val_tn: 1648423.6250 - val_fn: 3204.3635 - val_accuracy: 0.7903 - val_precision: 0.0390 - val_recall: 0.8477 - val_auc: 0.8938\n",
      "Epoch 50/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3764 - tp: 18174.0000 - fp: 439128.0000 - tn: 1666923.0000 - fn: 3241.0000 - accuracy: 0.7921 - precision: 0.0397 - recall: 0.8487 - auc: 0.8949 - val_loss: 0.5028 - val_tp: 18175.0000 - val_fp: 439294.0000 - val_tn: 1691075.6250 - val_fn: 3284.3635 - val_accuracy: 0.7943 - val_precision: 0.0397 - val_recall: 0.8470 - val_auc: 0.8942\n",
      "Epoch 51/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3746 - tp: 18517.0000 - fp: 439444.0000 - tn: 1709574.0000 - fn: 3321.0000 - accuracy: 0.7960 - precision: 0.0404 - recall: 0.8479 - auc: 0.8952 - val_loss: 0.4998 - val_tp: 18518.0000 - val_fp: 439611.0938 - val_tn: 1733725.5000 - val_fn: 3364.3635 - val_accuracy: 0.7982 - val_precision: 0.0404 - val_recall: 0.8463 - val_auc: 0.8946\n",
      "Epoch 52/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3727 - tp: 18860.0000 - fp: 439763.0000 - tn: 1752222.0000 - fn: 3401.0000 - accuracy: 0.7999 - precision: 0.0411 - recall: 0.8472 - auc: 0.8955 - val_loss: 0.4968 - val_tp: 18861.0000 - val_fp: 439932.8125 - val_tn: 1776370.5000 - val_fn: 3444.3635 - val_accuracy: 0.8019 - val_precision: 0.0411 - val_recall: 0.8456 - val_auc: 0.8949\n",
      "Epoch 53/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3709 - tp: 19203.0000 - fp: 440085.0000 - tn: 1794867.0000 - fn: 3481.0000 - accuracy: 0.8035 - precision: 0.0418 - recall: 0.8465 - auc: 0.8958 - val_loss: 0.4937 - val_tp: 19204.0000 - val_fp: 440255.2812 - val_tn: 1819015.2500 - val_fn: 3524.3635 - val_accuracy: 0.8055 - val_precision: 0.0418 - val_recall: 0.8449 - val_auc: 0.8952\n",
      "Epoch 54/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3702 - tp: 19545.0000 - fp: 440407.0000 - tn: 1837512.0000 - fn: 3562.0000 - accuracy: 0.8071 - precision: 0.0425 - recall: 0.8458 - auc: 0.8961 - val_loss: 0.4908 - val_tp: 19546.0000 - val_fp: 440577.1875 - val_tn: 1861660.3750 - val_fn: 3605.3635 - val_accuracy: 0.8090 - val_precision: 0.0425 - val_recall: 0.8443 - val_auc: 0.8955\n",
      "Epoch 55/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3673 - tp: 19888.0000 - fp: 440728.0000 - tn: 1880158.0000 - fn: 3642.0000 - accuracy: 0.8105 - precision: 0.0432 - recall: 0.8452 - auc: 0.8964 - val_loss: 0.4878 - val_tp: 19889.0000 - val_fp: 440896.9062 - val_tn: 1904307.6250 - val_fn: 3685.3635 - val_accuracy: 0.8123 - val_precision: 0.0432 - val_recall: 0.8437 - val_auc: 0.8957\n",
      "Epoch 56/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3657 - tp: 20231.0000 - fp: 441047.0000 - tn: 1922806.0000 - fn: 3722.0000 - accuracy: 0.8137 - precision: 0.0439 - recall: 0.8446 - auc: 0.8966 - val_loss: 0.4861 - val_tp: 20232.0000 - val_fp: 441250.5312 - val_tn: 1946921.1250 - val_fn: 3765.3635 - val_accuracy: 0.8155 - val_precision: 0.0438 - val_recall: 0.8431 - val_auc: 0.8960\n",
      "Epoch 57/100\n",
      "678/678 [==============================] - 2s 2ms/step - loss: 0.3638 - tp: 20574.0000 - fp: 441424.0000 - tn: 1965396.0000 - fn: 3802.0000 - accuracy: 0.8169 - precision: 0.0445 - recall: 0.8440 - auc: 0.8968 - val_loss: 0.4849 - val_tp: 20575.0000 - val_fp: 441678.7188 - val_tn: 1989459.8750 - val_fn: 3845.3635 - val_accuracy: 0.8186 - val_precision: 0.0445 - val_recall: 0.8425 - val_auc: 0.8962\n",
      "Epoch 58/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3621 - tp: 20917.0000 - fp: 441891.0000 - tn: 2007896.0000 - fn: 3882.0000 - accuracy: 0.8199 - precision: 0.0452 - recall: 0.8435 - auc: 0.8970 - val_loss: 0.4859 - val_tp: 20918.0000 - val_fp: 442244.3750 - val_tn: 2031861.5000 - val_fn: 3925.3635 - val_accuracy: 0.8215 - val_precision: 0.0452 - val_recall: 0.8420 - val_auc: 0.8964\n",
      "Epoch 59/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3585 - tp: 21262.0000 - fp: 442537.0000 - tn: 2050217.0000 - fn: 3960.0000 - accuracy: 0.8227 - precision: 0.0458 - recall: 0.8430 - auc: 0.8972 - val_loss: 0.4927 - val_tp: 21263.0000 - val_fp: 443203.4688 - val_tn: 2073869.1250 - val_fn: 4003.3635 - val_accuracy: 0.8241 - val_precision: 0.0458 - val_recall: 0.8416 - val_auc: 0.8965\n",
      "Epoch 60/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3588 - tp: 21607.0000 - fp: 443732.0000 - tn: 2091989.0000 - fn: 4038.0000 - accuracy: 0.8252 - precision: 0.0464 - recall: 0.8425 - auc: 0.8973 - val_loss: 0.4903 - val_tp: 21608.0000 - val_fp: 444405.8125 - val_tn: 2115633.7500 - val_fn: 4081.3635 - val_accuracy: 0.8265 - val_precision: 0.0464 - val_recall: 0.8411 - val_auc: 0.8967\n",
      "Epoch 61/100\n",
      "678/678 [==============================] - 2s 2ms/step - loss: 0.3558 - tp: 21952.0000 - fp: 444935.0000 - tn: 2133753.0000 - fn: 4116.0000 - accuracy: 0.8276 - precision: 0.0470 - recall: 0.8421 - auc: 0.8974 - val_loss: 0.4833 - val_tp: 21953.0000 - val_fp: 445454.0000 - val_tn: 2157552.2500 - val_fn: 4159.3638 - val_accuracy: 0.8290 - val_precision: 0.0470 - val_recall: 0.8407 - val_auc: 0.8968\n",
      "Epoch 62/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3534 - tp: 22297.0000 - fp: 445868.0000 - tn: 2175787.0000 - fn: 4194.0000 - accuracy: 0.8300 - precision: 0.0476 - recall: 0.8417 - auc: 0.8976 - val_loss: 0.4765 - val_tp: 22298.0000 - val_fp: 446262.5312 - val_tn: 2199711.0000 - val_fn: 4237.3638 - val_accuracy: 0.8314 - val_precision: 0.0476 - val_recall: 0.8403 - val_auc: 0.8969\n",
      "Epoch 63/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3514 - tp: 22642.0000 - fp: 446589.0000 - tn: 2218033.0000 - fn: 4272.0000 - accuracy: 0.8325 - precision: 0.0483 - recall: 0.8413 - auc: 0.8977 - val_loss: 0.4711 - val_tp: 22643.0000 - val_fp: 446910.5312 - val_tn: 2242030.0000 - val_fn: 4315.3638 - val_accuracy: 0.8339 - val_precision: 0.0482 - val_recall: 0.8399 - val_auc: 0.8971\n",
      "Epoch 64/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3495 - tp: 22987.0000 - fp: 447181.0000 - tn: 2260408.0000 - fn: 4350.0000 - accuracy: 0.8349 - precision: 0.0489 - recall: 0.8409 - auc: 0.8979 - val_loss: 0.4668 - val_tp: 22988.0000 - val_fp: 447456.5312 - val_tn: 2284451.0000 - val_fn: 4393.3638 - val_accuracy: 0.8362 - val_precision: 0.0489 - val_recall: 0.8395 - val_auc: 0.8973\n",
      "Epoch 65/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3492 - tp: 23332.0000 - fp: 447697.0000 - tn: 2302859.0000 - fn: 4428.0000 - accuracy: 0.8373 - precision: 0.0495 - recall: 0.8405 - auc: 0.8980 - val_loss: 0.4624 - val_tp: 23333.0000 - val_fp: 447924.8125 - val_tn: 2326949.7500 - val_fn: 4471.3638 - val_accuracy: 0.8386 - val_precision: 0.0495 - val_recall: 0.8392 - val_auc: 0.8975\n",
      "Epoch 66/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3462 - tp: 23677.0000 - fp: 448128.0000 - tn: 2345395.0000 - fn: 4506.0000 - accuracy: 0.8396 - precision: 0.0502 - recall: 0.8401 - auc: 0.8982 - val_loss: 0.4584 - val_tp: 23678.0000 - val_fp: 448327.8125 - val_tn: 2369513.7500 - val_fn: 4549.3638 - val_accuracy: 0.8409 - val_precision: 0.0502 - val_recall: 0.8388 - val_auc: 0.8977\n",
      "Epoch 67/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3445 - tp: 24022.0000 - fp: 448509.0000 - tn: 2387981.0000 - fn: 4584.0000 - accuracy: 0.8419 - precision: 0.0508 - recall: 0.8398 - auc: 0.8984 - val_loss: 0.4550 - val_tp: 24023.0000 - val_fp: 448683.8125 - val_tn: 2412124.5000 - val_fn: 4627.3638 - val_accuracy: 0.8431 - val_precision: 0.0508 - val_recall: 0.8385 - val_auc: 0.8979\n",
      "Epoch 68/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3428 - tp: 24367.0000 - fp: 448849.0000 - tn: 2430608.0000 - fn: 4662.0000 - accuracy: 0.8441 - precision: 0.0515 - recall: 0.8394 - auc: 0.8986 - val_loss: 0.4518 - val_tp: 24368.0000 - val_fp: 449014.0000 - val_tn: 2454761.7500 - val_fn: 4705.3638 - val_accuracy: 0.8453 - val_precision: 0.0515 - val_recall: 0.8382 - val_auc: 0.8981\n",
      "Epoch 69/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3412 - tp: 24712.0000 - fp: 449172.0000 - tn: 2473252.0000 - fn: 4740.0000 - accuracy: 0.8462 - precision: 0.0521 - recall: 0.8391 - auc: 0.8988 - val_loss: 0.4487 - val_tp: 24713.0000 - val_fp: 449324.3750 - val_tn: 2497418.2500 - val_fn: 4783.3638 - val_accuracy: 0.8474 - val_precision: 0.0521 - val_recall: 0.8378 - val_auc: 0.8983\n",
      "Epoch 70/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3396 - tp: 25057.0000 - fp: 449476.0000 - tn: 2515915.0000 - fn: 4818.0000 - accuracy: 0.8483 - precision: 0.0528 - recall: 0.8387 - auc: 0.8990 - val_loss: 0.4457 - val_tp: 25058.0000 - val_fp: 449619.9062 - val_tn: 2540089.5000 - val_fn: 4861.3638 - val_accuracy: 0.8495 - val_precision: 0.0528 - val_recall: 0.8375 - val_auc: 0.8985\n",
      "Epoch 71/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3379 - tp: 25402.0000 - fp: 449763.0000 - tn: 2558595.0000 - fn: 4896.0000 - accuracy: 0.8504 - precision: 0.0535 - recall: 0.8384 - auc: 0.8992 - val_loss: 0.4428 - val_tp: 25403.0000 - val_fp: 449899.3750 - val_tn: 2582777.0000 - val_fn: 4939.3638 - val_accuracy: 0.8515 - val_precision: 0.0534 - val_recall: 0.8372 - val_auc: 0.8987\n",
      "Epoch 72/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3363 - tp: 25747.0000 - fp: 450035.0000 - tn: 2601290.0000 - fn: 4974.0000 - accuracy: 0.8524 - precision: 0.0541 - recall: 0.8381 - auc: 0.8994 - val_loss: 0.4399 - val_tp: 25748.0000 - val_fp: 450164.6250 - val_tn: 2625479.0000 - val_fn: 5017.3638 - val_accuracy: 0.8535 - val_precision: 0.0541 - val_recall: 0.8369 - val_auc: 0.8989\n",
      "Epoch 73/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3359 - tp: 26091.0000 - fp: 450293.0000 - tn: 2643999.0000 - fn: 5053.0000 - accuracy: 0.8543 - precision: 0.0548 - recall: 0.8378 - auc: 0.8995 - val_loss: 0.4371 - val_tp: 26092.0000 - val_fp: 450416.2812 - val_tn: 2668194.2500 - val_fn: 5096.3638 - val_accuracy: 0.8554 - val_precision: 0.0548 - val_recall: 0.8366 - val_auc: 0.8990\n",
      "Epoch 74/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3332 - tp: 26436.0000 - fp: 450539.0000 - tn: 2686720.0000 - fn: 5131.0000 - accuracy: 0.8562 - precision: 0.0554 - recall: 0.8375 - auc: 0.8997 - val_loss: 0.4345 - val_tp: 26437.0000 - val_fp: 450660.7188 - val_tn: 2710916.7500 - val_fn: 5174.3638 - val_accuracy: 0.8572 - val_precision: 0.0554 - val_recall: 0.8363 - val_auc: 0.8992\n",
      "Epoch 75/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3328 - tp: 26780.0000 - fp: 450781.0000 - tn: 2729445.0000 - fn: 5210.0000 - accuracy: 0.8580 - precision: 0.0561 - recall: 0.8371 - auc: 0.8998 - val_loss: 0.4320 - val_tp: 26781.0000 - val_fp: 450901.0938 - val_tn: 2753643.5000 - val_fn: 5253.3638 - val_accuracy: 0.8591 - val_precision: 0.0561 - val_recall: 0.8360 - val_auc: 0.8993\n",
      "Epoch 76/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3312 - tp: 27124.0000 - fp: 451021.0000 - tn: 2772172.0000 - fn: 5289.0000 - accuracy: 0.8598 - precision: 0.0567 - recall: 0.8368 - auc: 0.9000 - val_loss: 0.4295 - val_tp: 27125.0000 - val_fp: 451140.2812 - val_tn: 2796371.2500 - val_fn: 5332.3638 - val_accuracy: 0.8608 - val_precision: 0.0567 - val_recall: 0.8357 - val_auc: 0.8995\n",
      "Epoch 77/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3297 - tp: 27468.0000 - fp: 451259.0000 - tn: 2814901.0000 - fn: 5368.0000 - accuracy: 0.8616 - precision: 0.0574 - recall: 0.8365 - auc: 0.9001 - val_loss: 0.4270 - val_tp: 27469.0000 - val_fp: 451378.0000 - val_tn: 2839100.7500 - val_fn: 5411.3638 - val_accuracy: 0.8626 - val_precision: 0.0574 - val_recall: 0.8354 - val_auc: 0.8996\n",
      "Epoch 78/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3282 - tp: 27812.0000 - fp: 451496.0000 - tn: 2857631.0000 - fn: 5447.0000 - accuracy: 0.8633 - precision: 0.0580 - recall: 0.8362 - auc: 0.9002 - val_loss: 0.4246 - val_tp: 27813.0000 - val_fp: 451614.1875 - val_tn: 2881831.7500 - val_fn: 5490.3638 - val_accuracy: 0.8642 - val_precision: 0.0580 - val_recall: 0.8351 - val_auc: 0.8997\n",
      "Epoch 79/100\n",
      "678/678 [==============================] - 2s 2ms/step - loss: 0.3267 - tp: 28156.0000 - fp: 451732.0000 - tn: 2900362.0000 - fn: 5526.0000 - accuracy: 0.8649 - precision: 0.0587 - recall: 0.8359 - auc: 0.9003 - val_loss: 0.4221 - val_tp: 28157.0000 - val_fp: 451849.0000 - val_tn: 2924563.7500 - val_fn: 5569.3638 - val_accuracy: 0.8659 - val_precision: 0.0587 - val_recall: 0.8349 - val_auc: 0.8999\n",
      "Epoch 80/100\n",
      "678/678 [==============================] - 2s 2ms/step - loss: 0.3251 - tp: 28500.0000 - fp: 451965.0000 - tn: 2943096.0000 - fn: 5605.0000 - accuracy: 0.8666 - precision: 0.0593 - recall: 0.8357 - auc: 0.9004 - val_loss: 0.4197 - val_tp: 28501.0000 - val_fp: 452080.8125 - val_tn: 2967299.0000 - val_fn: 5648.3638 - val_accuracy: 0.8675 - val_precision: 0.0593 - val_recall: 0.8346 - val_auc: 0.9000\n",
      "Epoch 81/100\n",
      "678/678 [==============================] - 2s 2ms/step - loss: 0.3236 - tp: 28844.0000 - fp: 452196.0000 - tn: 2985832.0000 - fn: 5684.0000 - accuracy: 0.8681 - precision: 0.0600 - recall: 0.8354 - auc: 0.9005 - val_loss: 0.4173 - val_tp: 28845.0000 - val_fp: 452310.8125 - val_tn: 3010035.7500 - val_fn: 5727.3638 - val_accuracy: 0.8690 - val_precision: 0.0599 - val_recall: 0.8343 - val_auc: 0.9001\n",
      "Epoch 82/100\n",
      "678/678 [==============================] - 2s 2ms/step - loss: 0.3222 - tp: 29188.0000 - fp: 452425.0000 - tn: 3028570.0000 - fn: 5763.0000 - accuracy: 0.8697 - precision: 0.0606 - recall: 0.8351 - auc: 0.9007 - val_loss: 0.4150 - val_tp: 29189.0000 - val_fp: 452539.1875 - val_tn: 3052774.2500 - val_fn: 5806.3638 - val_accuracy: 0.8705 - val_precision: 0.0606 - val_recall: 0.8341 - val_auc: 0.9002\n",
      "Epoch 83/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3207 - tp: 29532.0000 - fp: 452653.0000 - tn: 3071309.0000 - fn: 5842.0000 - accuracy: 0.8712 - precision: 0.0612 - recall: 0.8349 - auc: 0.9008 - val_loss: 0.4127 - val_tp: 29533.0000 - val_fp: 452767.0938 - val_tn: 3095513.7500 - val_fn: 5885.3638 - val_accuracy: 0.8720 - val_precision: 0.0612 - val_recall: 0.8338 - val_auc: 0.9003\n",
      "Epoch 84/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3180 - tp: 29877.0000 - fp: 452880.0000 - tn: 3114049.0000 - fn: 5920.0000 - accuracy: 0.8727 - precision: 0.0619 - recall: 0.8346 - auc: 0.9009 - val_loss: 0.4105 - val_tp: 29878.0000 - val_fp: 452995.2812 - val_tn: 3138252.2500 - val_fn: 5963.3638 - val_accuracy: 0.8735 - val_precision: 0.0619 - val_recall: 0.8336 - val_auc: 0.9004\n",
      "Epoch 85/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3178 - tp: 30221.0000 - fp: 453111.0000 - tn: 3156785.0000 - fn: 5999.0000 - accuracy: 0.8741 - precision: 0.0625 - recall: 0.8344 - auc: 0.9010 - val_loss: 0.4084 - val_tp: 30222.0000 - val_fp: 453227.9062 - val_tn: 3180987.0000 - val_fn: 6042.3638 - val_accuracy: 0.8749 - val_precision: 0.0625 - val_recall: 0.8334 - val_auc: 0.9005\n",
      "Epoch 86/100\n",
      "678/678 [==============================] - 2s 2ms/step - loss: 0.3163 - tp: 30565.0000 - fp: 453345.0000 - tn: 3199518.0000 - fn: 6078.0000 - accuracy: 0.8755 - precision: 0.0632 - recall: 0.8341 - auc: 0.9011 - val_loss: 0.4062 - val_tp: 30566.0000 - val_fp: 453461.9062 - val_tn: 3223719.7500 - val_fn: 6121.3638 - val_accuracy: 0.8763 - val_precision: 0.0631 - val_recall: 0.8331 - val_auc: 0.9006\n",
      "Epoch 87/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3136 - tp: 30910.0000 - fp: 453579.0000 - tn: 3242251.0000 - fn: 6156.0000 - accuracy: 0.8768 - precision: 0.0638 - recall: 0.8339 - auc: 0.9012 - val_loss: 0.4041 - val_tp: 30911.0000 - val_fp: 453696.2812 - val_tn: 3266452.2500 - val_fn: 6199.3638 - val_accuracy: 0.8776 - val_precision: 0.0638 - val_recall: 0.8329 - val_auc: 0.9007\n",
      "Epoch 88/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3135 - tp: 31254.0000 - fp: 453815.0000 - tn: 3284982.0000 - fn: 6235.0000 - accuracy: 0.8782 - precision: 0.0644 - recall: 0.8337 - auc: 0.9013 - val_loss: 0.4020 - val_tp: 31255.0000 - val_fp: 453933.7188 - val_tn: 3309181.7500 - val_fn: 6278.3638 - val_accuracy: 0.8789 - val_precision: 0.0644 - val_recall: 0.8327 - val_auc: 0.9008\n",
      "Epoch 89/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3120 - tp: 31598.0000 - fp: 454054.0000 - tn: 3327710.0000 - fn: 6314.0000 - accuracy: 0.8795 - precision: 0.0651 - recall: 0.8335 - auc: 0.9013 - val_loss: 0.3999 - val_tp: 31599.0000 - val_fp: 454173.5312 - val_tn: 3351909.0000 - val_fn: 6357.3638 - val_accuracy: 0.8802 - val_precision: 0.0650 - val_recall: 0.8325 - val_auc: 0.9009\n",
      "Epoch 90/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3093 - tp: 31943.0000 - fp: 454294.0000 - tn: 3370437.0000 - fn: 6392.0000 - accuracy: 0.8807 - precision: 0.0657 - recall: 0.8333 - auc: 0.9014 - val_loss: 0.3978 - val_tp: 31944.0000 - val_fp: 454415.4688 - val_tn: 3394634.5000 - val_fn: 6435.3638 - val_accuracy: 0.8814 - val_precision: 0.0657 - val_recall: 0.8323 - val_auc: 0.9010\n",
      "Epoch 91/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3079 - tp: 32288.0000 - fp: 454537.0000 - tn: 3413161.0000 - fn: 6470.0000 - accuracy: 0.8820 - precision: 0.0663 - recall: 0.8331 - auc: 0.9015 - val_loss: 0.3957 - val_tp: 32289.0000 - val_fp: 454658.4688 - val_tn: 3437358.5000 - val_fn: 6513.3638 - val_accuracy: 0.8827 - val_precision: 0.0663 - val_recall: 0.8321 - val_auc: 0.9011\n",
      "Epoch 92/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3079 - tp: 32632.0000 - fp: 454780.0000 - tn: 3455885.0000 - fn: 6549.0000 - accuracy: 0.8832 - precision: 0.0669 - recall: 0.8329 - auc: 0.9016 - val_loss: 0.3936 - val_tp: 32633.0000 - val_fp: 454902.2812 - val_tn: 3480081.5000 - val_fn: 6592.3638 - val_accuracy: 0.8839 - val_precision: 0.0669 - val_recall: 0.8319 - val_auc: 0.9012\n",
      "Epoch 93/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3065 - tp: 32976.0000 - fp: 455024.0000 - tn: 3498608.0000 - fn: 6628.0000 - accuracy: 0.8844 - precision: 0.0676 - recall: 0.8326 - auc: 0.9017 - val_loss: 0.3917 - val_tp: 32977.0000 - val_fp: 455149.4688 - val_tn: 3522801.0000 - val_fn: 6671.3638 - val_accuracy: 0.8850 - val_precision: 0.0676 - val_recall: 0.8317 - val_auc: 0.9012\n",
      "Epoch 94/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3038 - tp: 33321.0000 - fp: 455273.0000 - tn: 3541326.0000 - fn: 6706.0000 - accuracy: 0.8856 - precision: 0.0682 - recall: 0.8325 - auc: 0.9017 - val_loss: 0.3898 - val_tp: 33322.0000 - val_fp: 455402.0000 - val_tn: 3565515.7500 - val_fn: 6749.3638 - val_accuracy: 0.8862 - val_precision: 0.0682 - val_recall: 0.8316 - val_auc: 0.9013\n",
      "Epoch 95/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3024 - tp: 33666.0000 - fp: 455528.0000 - tn: 3584038.0000 - fn: 6784.0000 - accuracy: 0.8867 - precision: 0.0688 - recall: 0.8323 - auc: 0.9018 - val_loss: 0.3878 - val_tp: 33667.0000 - val_fp: 455657.9062 - val_tn: 3608227.0000 - val_fn: 6827.3638 - val_accuracy: 0.8873 - val_precision: 0.0688 - val_recall: 0.8314 - val_auc: 0.9014\n",
      "Epoch 96/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3024 - tp: 34010.0000 - fp: 455785.0000 - tn: 3626748.0000 - fn: 6863.0000 - accuracy: 0.8878 - precision: 0.0694 - recall: 0.8321 - auc: 0.9019 - val_loss: 0.3859 - val_tp: 34011.0000 - val_fp: 455917.3750 - val_tn: 3650934.2500 - val_fn: 6906.3638 - val_accuracy: 0.8884 - val_precision: 0.0694 - val_recall: 0.8312 - val_auc: 0.9015\n",
      "Epoch 97/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.2997 - tp: 34355.0000 - fp: 456046.0000 - tn: 3669454.0000 - fn: 6941.0000 - accuracy: 0.8889 - precision: 0.0701 - recall: 0.8319 - auc: 0.9020 - val_loss: 0.3840 - val_tp: 34356.0000 - val_fp: 456178.3750 - val_tn: 3693640.0000 - val_fn: 6984.3638 - val_accuracy: 0.8895 - val_precision: 0.0700 - val_recall: 0.8311 - val_auc: 0.9016\n",
      "Epoch 98/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.2984 - tp: 34700.0000 - fp: 456307.0000 - tn: 3712160.0000 - fn: 7019.0000 - accuracy: 0.8900 - precision: 0.0707 - recall: 0.8318 - auc: 0.9020 - val_loss: 0.3820 - val_tp: 34701.0000 - val_fp: 456440.8125 - val_tn: 3736344.2500 - val_fn: 7062.3638 - val_accuracy: 0.8905 - val_precision: 0.0707 - val_recall: 0.8309 - val_auc: 0.9016\n",
      "Epoch 99/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.2970 - tp: 35045.0000 - fp: 456571.0000 - tn: 3754863.0000 - fn: 7097.0000 - accuracy: 0.8910 - precision: 0.0713 - recall: 0.8316 - auc: 0.9021 - val_loss: 0.3802 - val_tp: 35046.0000 - val_fp: 456705.3750 - val_tn: 3779047.2500 - val_fn: 7140.3638 - val_accuracy: 0.8916 - val_precision: 0.0713 - val_recall: 0.8307 - val_auc: 0.9017\n",
      "Epoch 100/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.2957 - tp: 35390.0000 - fp: 456837.0000 - tn: 3797564.0000 - fn: 7175.0000 - accuracy: 0.8920 - precision: 0.0719 - recall: 0.8314 - auc: 0.9022 - val_loss: 0.3783 - val_tp: 35391.0000 - val_fp: 456971.3750 - val_tn: 3821748.0000 - val_fn: 7218.3638 - val_accuracy: 0.8926 - val_precision: 0.0719 - val_recall: 0.8306 - val_auc: 0.9018\n",
      "TRAIN | AUC Score: 1.0\n",
      "TEST | AUC Score: 0.5043501711927209\n",
      "error\n",
      "error\n",
      "Confusion Matrix:\n",
      "false positive pct: 0.5783053545924937\n",
      "tn  fp  fn  tp\n",
      "42395 247 68 1\n",
      "[[42395   247]\n",
      " [   68     1]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00     42642\n",
      "           1       0.00      0.01      0.01        69\n",
      "\n",
      "    accuracy                           0.99     42711\n",
      "   macro avg       0.50      0.50      0.50     42711\n",
      "weighted avg       1.00      0.99      0.99     42711\n",
      "\n",
      "Specificity = 0.9942075887622531\n",
      "Sensitivity = 0.014492753623188406\n",
      "lr 0.01 w1 1.0 w2 1.0 epochs 100 batch_size 4096\n"
     ]
    }
   ],
   "source": [
    "# run model\n",
    "METRICS = [\n",
    "      keras.metrics.TruePositives(name='tp'),\n",
    "      keras.metrics.FalsePositives(name='fp'),\n",
    "      keras.metrics.TrueNegatives(name='tn'),\n",
    "      keras.metrics.FalseNegatives(name='fn'), \n",
    "      keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      keras.metrics.Precision(name='precision'),\n",
    "      keras.metrics.Recall(name='recall'),\n",
    "      keras.metrics.AUC(name='auc'),\n",
    "]\n",
    "### Hyperparameters Tuning\n",
    "# First test optimal epochs holding everything else constant\n",
    "# Dropout: 0.1-0.6\n",
    "# GradientClipping: 0.1-10\n",
    "# BatchSize: 32,64,128,256,512 (power of 2)\n",
    "\n",
    "# setting variables\n",
    "lr = 0.025\n",
    "w1 = 4.0\n",
    "w2 = 1.0\n",
    "epochs = 100 # original was 200, also saw 100\n",
    "batch_size = 4096 # original was 10000, also saw 2048\n",
    "min_ = 0\n",
    "max_ = 2\n",
    "num_iter = 1\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_auc', \n",
    "    verbose=1,\n",
    "    patience=10,\n",
    "    mode='max',\n",
    "    restore_best_weights=True)\n",
    "\n",
    "for w1 in np.linspace(1, 4, num = 1):\n",
    "    for w2 in np.linspace(1, 4, num = 1):\n",
    "        # keep lr constant at 0.01\n",
    "        for lr in np.linspace(.01, .01, num=1):\n",
    "            # repeat same settings for num of interations\n",
    "            for n in np.linspace(0, 3, num=num_iter):\n",
    "                ### Train LSTM using Keras 2 API ###\n",
    "                model = Sequential()\n",
    "                #model.add(LSTM(20, return_sequences=True, input_shape=X_train.shape[1:], kernel_initializer='lecun_uniform', activation='relu', kernel_regularizer=regularizers.l1(0.1), recurrent_regularizer=regularizers.l1(0.01), bias_regularizer=None, activity_regularizer=None, dropout=0.2, recurrent_dropout=0.2))#, return_sequences=True))\n",
    "                #model.add(LSTM(20, return_sequences=True, input_shape=X_train.shape[1:], activation='relu', kernel_regularizer=regularizers.l1(0.1), recurrent_regularizer=regularizers.l1(0.01), bias_regularizer=None, activity_regularizer=None, dropout=0.2, recurrent_dropout=0.2))#, return_sequences=True))\n",
    "                # defaults seem to work best, otherwise get very low score, AUC of 0.5 and all 0 or all 1 predictions\n",
    "                model.add(LSTM(20, return_sequences=True, input_shape=X_train.shape[1:]))\n",
    "                model.add(Dropout(0.2))\n",
    "                model.add(LSTM(12, activation='relu', return_sequences=True))\n",
    "                model.add(Dropout(0.2))\n",
    "                model.add(LSTM(8, activation='relu', return_sequences=False)) # this is the last LSTM, so should return_sequences=False\n",
    "                #model.add(Dense(1, kernel_initializer='lecun_uniform', activation='sigmoid'))\n",
    "                model.add(Dense(units=1, kernel_initializer='uniform', activation='sigmoid'))\n",
    "                # from https://www.tensorflow.org/tutorials/structured_data/imbalanced_data\n",
    "                bias_initializer = ones/zeros # think this is just the ratio of minority/total, and so far the log() makes this worse\n",
    "                #bias_initializer = np.log([bias_initializer])\n",
    "                print('bias_initializer', bias_initializer)\n",
    "                bias_initializer = initializers.Constant(bias_initializer)\n",
    "                model.add(Dense(units=1, activation='sigmoid', bias_initializer=bias_initializer))\n",
    "                #model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "                #optimizer = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False) # ValueError: None values not supported.\n",
    "\n",
    "                optimizer = optimizers.Adam(lr=lr)\n",
    "\n",
    "                #model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "                #model.compile(loss='mean_squared_error', optimizer=adam)\n",
    "                #model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy']) #optimizer='rmsprop', optimizer='sgd', optimizer='adam'\n",
    "\n",
    "                a='''\n",
    "                lr_schedule = optimizers.schedules.ExponentialDecay(\n",
    "                    initial_learning_rate=1e-2,\n",
    "                    decay_steps=10000,\n",
    "                    decay_rate=0.9)\n",
    "                optimizer = optimizers.SGD(learning_rate=lr_schedule)'''\n",
    "\n",
    "                #optimizer = optimizers.SGD(learning_rate=lr)\n",
    "                #optimizer = optimizers.SGD(learning_rate=0.001, momentum=0.9,decay=0.01)\n",
    "\n",
    "                model.compile(loss='binary_crossentropy', optimizer=optimizer, \n",
    "                              metrics=METRICS) #optimizer='rmsprop', optimizer='sgd', optimizer='adam'\n",
    "                \n",
    "                results = model.evaluate(X_train, y_train, batch_size=batch_size, verbose=0)\n",
    "                print(\"Initial Loss: {:0.4f}\".format(results[0]))\n",
    "                print(model.metrics_names)\n",
    "                print(results)\n",
    "                #print(model.summary())\n",
    "                \n",
    "                lrs = LearningRateScheduler(my_learning_rate)\n",
    "                #model.fit(..., callbacks=[lrs])\n",
    "\n",
    "                #or\n",
    "\n",
    "                rlrop = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=100)\n",
    "                # TypeError: unsupported operand type(s) for +: 'ReduceLROnPlateau' and 'list'\n",
    "                #model.fit(..., callbacks=[rlrop])\n",
    "\n",
    "\n",
    "                #model.fit(X_train, y_train, epochs=200, batch_size=10000, class_weight={0 : 1., 1: float(int(1/np.mean(y_train)))}, validation_split=0.3)\n",
    "                #model.fit(X_train, y_train, epochs=4, batch_size=8)#, class_weight=np.where(y_train == 1,4.0,1.0).flatten() )\n",
    "                \n",
    "                #model_output = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, class_weight=np.where(y_train == 1,w1,w2).flatten() )\n",
    "                \n",
    "                model_output = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=1, callbacks=[rlrop],\n",
    "                                         class_weight=np.where(y_train == 1,w1,w2).flatten(), validation_data=(val_features, val_labels) )\n",
    "\n",
    "                train_predict = model.predict_classes(X_train)\n",
    "                test_predict = model.predict_classes(X_test)\n",
    "                \n",
    "                ### test AUC ###\n",
    "\n",
    "                fpr, tpr, thresholds = metrics.roc_curve(y_train, train_predict, pos_label=1)\n",
    "                print('TRAIN | AUC Score: ' + str((metrics.auc(fpr, tpr))))\n",
    "                fpr, tpr, thresholds = metrics.roc_curve(y_test, test_predict, pos_label=1)\n",
    "                print('TEST | AUC Score: ' + str((metrics.auc(fpr, tpr))))\n",
    "                tn, fp, fn, tp = display_metrics(model, X_train, X_test, y_train, y_test, test_predict)\n",
    "                cm_results.append([model.name, tn, fp, fn, tp, lr, w1, w2, epochs, batch_size])\n",
    "                print('lr',lr,'w1',w1,'w2',w2,'epochs',epochs,'batch_size',batch_size)\n",
    "                del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss(model_output, \"Plot Loss\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True 0.172749 0.172549 50.0 0.161513\n"
     ]
    }
   ],
   "source": [
    "final_results = pd.DataFrame(cm_results, columns=('algo','TN','FP','FN','TP', 'lr', 'w1', 'w2', 'epochs', 'batch_size')) \n",
    "#sp = round((tn1 + tn2)/(tn1 + tn2 +fp2), 3)\n",
    "#se = round(tp2/(tp2 + fn1 + fn2), 3)\n",
    "final_results['SP'] = round(final_results['TN']/(final_results['TN'] + final_results['FP']), 3)\n",
    "final_results['SE'] = round(final_results['TP']/(final_results['TP'] + final_results['FN']), 3)\n",
    "final_results['Avg'] = (final_results['SP'] + final_results['SE'])/2\n",
    "\n",
    "print(strat,OrigPct,TrainPct,TrainDownPct,TestPct)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           algo     TN   FP  FN  TP    lr   w1   w2  epochs  batch_size  \\\n",
      "0  sequential_1  42395  247  68   1  0.01  1.0  1.0     100        4096   \n",
      "\n",
      "      SP     SE    Avg  \n",
      "0  0.994  0.014  0.504  \n"
     ]
    }
   ],
   "source": [
    "sort = final_results.sort_values(final_results.columns[5], ascending = False)\n",
    "sort.to_csv('results_lstm.csv', sep=',', mode='a', encoding='utf-8', header=True)\n",
    "print(sort)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bias_initializer 0.0017284695005527083\n",
      "Train on 678 samples, validate on 42712 samples\n",
      "Epoch 1/200\n",
      "678/678 [==============================] - 3s 4ms/step - loss: 0.6983 - accuracy: 0.4926 - val_loss: 0.6145 - val_accuracy: 0.9982\n",
      "Epoch 2/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.6975 - accuracy: 0.4926 - val_loss: 0.6201 - val_accuracy: 0.9982\n",
      "Epoch 3/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.6964 - accuracy: 0.4926 - val_loss: 0.6260 - val_accuracy: 0.9982\n",
      "Epoch 4/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.6951 - accuracy: 0.4926 - val_loss: 0.6312 - val_accuracy: 0.9982\n",
      "Epoch 5/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.6935 - accuracy: 0.4926 - val_loss: 0.6349 - val_accuracy: 0.9982\n",
      "Epoch 6/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.6911 - accuracy: 0.4926 - val_loss: 0.6352 - val_accuracy: 0.9982\n",
      "Epoch 7/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.6874 - accuracy: 0.4926 - val_loss: 0.6274 - val_accuracy: 0.9982\n",
      "Epoch 8/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.6800 - accuracy: 0.4926 - val_loss: 0.6057 - val_accuracy: 0.9982\n",
      "Epoch 9/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.6679 - accuracy: 0.4926 - val_loss: 0.5946 - val_accuracy: 0.9982\n",
      "Epoch 10/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.6595 - accuracy: 0.4926 - val_loss: 0.5961 - val_accuracy: 0.9982\n",
      "Epoch 11/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.6569 - accuracy: 0.4926 - val_loss: 0.5975 - val_accuracy: 0.9982\n",
      "Epoch 12/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.6531 - accuracy: 0.5619 - val_loss: 0.5983 - val_accuracy: 0.9982\n",
      "Epoch 13/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.6461 - accuracy: 0.8555 - val_loss: 0.5988 - val_accuracy: 0.9982\n",
      "Epoch 14/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.6361 - accuracy: 0.9528 - val_loss: 0.5990 - val_accuracy: 0.9982\n",
      "Epoch 15/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.6252 - accuracy: 0.9690 - val_loss: 0.5988 - val_accuracy: 0.9982\n",
      "Epoch 16/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.6175 - accuracy: 0.9720 - val_loss: 0.5983 - val_accuracy: 0.9982\n",
      "Epoch 17/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.6135 - accuracy: 0.9779 - val_loss: 0.5974 - val_accuracy: 0.9982\n",
      "Epoch 18/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.6099 - accuracy: 0.9838 - val_loss: 0.5963 - val_accuracy: 0.9982\n",
      "Epoch 19/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.6071 - accuracy: 0.9867 - val_loss: 0.5950 - val_accuracy: 0.9979\n",
      "Epoch 20/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.6044 - accuracy: 0.9882 - val_loss: 0.5936 - val_accuracy: 0.9967\n",
      "Epoch 21/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.6019 - accuracy: 0.9882 - val_loss: 0.5920 - val_accuracy: 0.9955\n",
      "Epoch 22/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.6040 - accuracy: 0.9661 - val_loss: 0.5900 - val_accuracy: 0.9954\n",
      "Epoch 23/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.5974 - accuracy: 0.9867 - val_loss: 0.5879 - val_accuracy: 0.9952\n",
      "Epoch 24/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.5942 - accuracy: 0.9897 - val_loss: 0.5856 - val_accuracy: 0.9951\n",
      "Epoch 25/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.5916 - accuracy: 0.9897 - val_loss: 0.5833 - val_accuracy: 0.9947\n",
      "Epoch 26/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.5892 - accuracy: 0.9897 - val_loss: 0.5809 - val_accuracy: 0.9944\n",
      "Epoch 27/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.5859 - accuracy: 0.9926 - val_loss: 0.5784 - val_accuracy: 0.9938\n",
      "Epoch 28/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.5831 - accuracy: 0.9941 - val_loss: 0.5759 - val_accuracy: 0.9928\n",
      "Epoch 29/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.5801 - accuracy: 0.9956 - val_loss: 0.5735 - val_accuracy: 0.9917\n",
      "Epoch 30/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.5777 - accuracy: 0.9956 - val_loss: 0.5711 - val_accuracy: 0.9907\n",
      "Epoch 31/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.5762 - accuracy: 0.9912 - val_loss: 0.5683 - val_accuracy: 0.9905\n",
      "Epoch 32/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.5724 - accuracy: 0.9971 - val_loss: 0.5658 - val_accuracy: 0.9898\n",
      "Epoch 33/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.5697 - accuracy: 0.9971 - val_loss: 0.5634 - val_accuracy: 0.9882\n",
      "Epoch 34/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.5669 - accuracy: 0.9985 - val_loss: 0.5612 - val_accuracy: 0.9863\n",
      "Epoch 35/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.5647 - accuracy: 0.9971 - val_loss: 0.5583 - val_accuracy: 0.9863\n",
      "Epoch 36/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.5621 - accuracy: 0.9985 - val_loss: 0.5550 - val_accuracy: 0.9877\n",
      "Epoch 37/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.5594 - accuracy: 0.9985 - val_loss: 0.5518 - val_accuracy: 0.9888\n",
      "Epoch 38/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.5569 - accuracy: 0.9985 - val_loss: 0.5488 - val_accuracy: 0.9893\n",
      "Epoch 39/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.5546 - accuracy: 0.9985 - val_loss: 0.5461 - val_accuracy: 0.9890\n",
      "Epoch 40/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.5520 - accuracy: 0.9985 - val_loss: 0.5435 - val_accuracy: 0.9886\n",
      "Epoch 41/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.5496 - accuracy: 0.9985 - val_loss: 0.5410 - val_accuracy: 0.9884\n",
      "Epoch 42/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.5471 - accuracy: 0.9985 - val_loss: 0.5385 - val_accuracy: 0.9879\n",
      "Epoch 43/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.5443 - accuracy: 1.0000 - val_loss: 0.5361 - val_accuracy: 0.9876\n",
      "Epoch 44/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.5424 - accuracy: 0.9985 - val_loss: 0.5337 - val_accuracy: 0.9871\n",
      "Epoch 45/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.5395 - accuracy: 1.0000 - val_loss: 0.5313 - val_accuracy: 0.9867\n",
      "Epoch 46/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.5373 - accuracy: 1.0000 - val_loss: 0.5294 - val_accuracy: 0.9852\n",
      "Epoch 47/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.5353 - accuracy: 0.9985 - val_loss: 0.5278 - val_accuracy: 0.9833\n",
      "Epoch 48/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.5326 - accuracy: 1.0000 - val_loss: 0.5256 - val_accuracy: 0.9829\n",
      "Epoch 49/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.5304 - accuracy: 1.0000 - val_loss: 0.5227 - val_accuracy: 0.9845\n",
      "Epoch 50/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.5279 - accuracy: 1.0000 - val_loss: 0.5200 - val_accuracy: 0.9855\n",
      "Epoch 51/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.5256 - accuracy: 1.0000 - val_loss: 0.5175 - val_accuracy: 0.9863\n",
      "Epoch 52/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.5234 - accuracy: 1.0000 - val_loss: 0.5150 - val_accuracy: 0.9869\n",
      "Epoch 53/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.5211 - accuracy: 1.0000 - val_loss: 0.5127 - val_accuracy: 0.9873\n",
      "Epoch 54/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.5189 - accuracy: 1.0000 - val_loss: 0.5104 - val_accuracy: 0.9877\n",
      "Epoch 55/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.5167 - accuracy: 1.0000 - val_loss: 0.5082 - val_accuracy: 0.9881\n",
      "Epoch 56/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.5144 - accuracy: 1.0000 - val_loss: 0.5061 - val_accuracy: 0.9884\n",
      "Epoch 57/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.5123 - accuracy: 1.0000 - val_loss: 0.5040 - val_accuracy: 0.9886\n",
      "Epoch 58/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.5101 - accuracy: 1.0000 - val_loss: 0.5020 - val_accuracy: 0.9887\n",
      "Epoch 59/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.5079 - accuracy: 1.0000 - val_loss: 0.5000 - val_accuracy: 0.9889\n",
      "Epoch 60/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.5062 - accuracy: 0.9985 - val_loss: 0.4988 - val_accuracy: 0.9874\n",
      "Epoch 61/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.5036 - accuracy: 1.0000 - val_loss: 0.4977 - val_accuracy: 0.9857\n",
      "Epoch 62/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.5016 - accuracy: 1.0000 - val_loss: 0.4966 - val_accuracy: 0.9843\n",
      "Epoch 63/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4995 - accuracy: 1.0000 - val_loss: 0.4953 - val_accuracy: 0.9834\n",
      "Epoch 64/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4974 - accuracy: 1.0000 - val_loss: 0.4938 - val_accuracy: 0.9829\n",
      "Epoch 65/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4957 - accuracy: 0.9985 - val_loss: 0.4905 - val_accuracy: 0.9858\n",
      "Epoch 66/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.4932 - accuracy: 1.0000 - val_loss: 0.4879 - val_accuracy: 0.9876\n",
      "Epoch 67/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4911 - accuracy: 1.0000 - val_loss: 0.4855 - val_accuracy: 0.9889\n",
      "Epoch 68/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4891 - accuracy: 1.0000 - val_loss: 0.4833 - val_accuracy: 0.9900\n",
      "Epoch 69/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.4870 - accuracy: 1.0000 - val_loss: 0.4813 - val_accuracy: 0.9905\n",
      "Epoch 70/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4850 - accuracy: 1.0000 - val_loss: 0.4794 - val_accuracy: 0.9909\n",
      "Epoch 71/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4837 - accuracy: 0.9985 - val_loss: 0.4775 - val_accuracy: 0.9913\n",
      "Epoch 72/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4816 - accuracy: 0.9985 - val_loss: 0.4760 - val_accuracy: 0.9909\n",
      "Epoch 73/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4798 - accuracy: 0.9985 - val_loss: 0.4744 - val_accuracy: 0.9906\n",
      "Epoch 74/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4778 - accuracy: 0.9985 - val_loss: 0.4729 - val_accuracy: 0.9904\n",
      "Epoch 75/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4751 - accuracy: 1.0000 - val_loss: 0.4714 - val_accuracy: 0.9899\n",
      "Epoch 76/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4734 - accuracy: 1.0000 - val_loss: 0.4709 - val_accuracy: 0.9876\n",
      "Epoch 77/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.4712 - accuracy: 1.0000 - val_loss: 0.4708 - val_accuracy: 0.9847\n",
      "Epoch 78/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.4693 - accuracy: 1.0000 - val_loss: 0.4719 - val_accuracy: 0.9794\n",
      "Epoch 79/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.4674 - accuracy: 1.0000 - val_loss: 0.4745 - val_accuracy: 0.9708\n",
      "Epoch 80/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4657 - accuracy: 1.0000 - val_loss: 0.4758 - val_accuracy: 0.9655\n",
      "Epoch 81/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4655 - accuracy: 0.9985 - val_loss: 0.4701 - val_accuracy: 0.9735\n",
      "Epoch 82/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4627 - accuracy: 0.9985 - val_loss: 0.4652 - val_accuracy: 0.9797\n",
      "Epoch 83/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4607 - accuracy: 0.9985 - val_loss: 0.4618 - val_accuracy: 0.9829\n",
      "Epoch 84/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.4580 - accuracy: 1.0000 - val_loss: 0.4592 - val_accuracy: 0.9845\n",
      "Epoch 85/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4562 - accuracy: 1.0000 - val_loss: 0.4569 - val_accuracy: 0.9856\n",
      "Epoch 86/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.4552 - accuracy: 0.9985 - val_loss: 0.4549 - val_accuracy: 0.9864\n",
      "Epoch 87/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4526 - accuracy: 1.0000 - val_loss: 0.4529 - val_accuracy: 0.9869\n",
      "Epoch 88/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4511 - accuracy: 1.0000 - val_loss: 0.4504 - val_accuracy: 0.9884\n",
      "Epoch 89/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4490 - accuracy: 1.0000 - val_loss: 0.4481 - val_accuracy: 0.9896\n",
      "Epoch 90/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4472 - accuracy: 1.0000 - val_loss: 0.4460 - val_accuracy: 0.9903\n",
      "Epoch 91/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4454 - accuracy: 1.0000 - val_loss: 0.4439 - val_accuracy: 0.9910\n",
      "Epoch 92/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4436 - accuracy: 1.0000 - val_loss: 0.4420 - val_accuracy: 0.9913\n",
      "Epoch 93/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4419 - accuracy: 1.0000 - val_loss: 0.4401 - val_accuracy: 0.9919\n",
      "Epoch 94/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4401 - accuracy: 1.0000 - val_loss: 0.4381 - val_accuracy: 0.9923\n",
      "Epoch 95/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4384 - accuracy: 1.0000 - val_loss: 0.4363 - val_accuracy: 0.9926\n",
      "Epoch 96/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.4375 - accuracy: 0.9985 - val_loss: 0.4346 - val_accuracy: 0.9928\n",
      "Epoch 97/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.4367 - accuracy: 0.9971 - val_loss: 0.4329 - val_accuracy: 0.9929\n",
      "Epoch 98/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.4350 - accuracy: 0.9971 - val_loss: 0.4314 - val_accuracy: 0.9929\n",
      "Epoch 99/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4334 - accuracy: 0.9971 - val_loss: 0.4298 - val_accuracy: 0.9930\n",
      "Epoch 100/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4308 - accuracy: 0.9985 - val_loss: 0.4283 - val_accuracy: 0.9930\n",
      "Epoch 101/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4300 - accuracy: 0.9971 - val_loss: 0.4267 - val_accuracy: 0.9930\n",
      "Epoch 102/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4274 - accuracy: 0.9985 - val_loss: 0.4252 - val_accuracy: 0.9929\n",
      "Epoch 103/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4267 - accuracy: 0.9971 - val_loss: 0.4237 - val_accuracy: 0.9929\n",
      "Epoch 104/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.4241 - accuracy: 0.9985 - val_loss: 0.4223 - val_accuracy: 0.9929\n",
      "Epoch 105/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.4225 - accuracy: 0.9985 - val_loss: 0.4208 - val_accuracy: 0.9929\n",
      "Epoch 106/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4218 - accuracy: 0.9971 - val_loss: 0.4193 - val_accuracy: 0.9929\n",
      "Epoch 107/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4202 - accuracy: 0.9971 - val_loss: 0.4179 - val_accuracy: 0.9929\n",
      "Epoch 108/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4176 - accuracy: 0.9985 - val_loss: 0.4164 - val_accuracy: 0.9929\n",
      "Epoch 109/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4170 - accuracy: 0.9971 - val_loss: 0.4150 - val_accuracy: 0.9929\n",
      "Epoch 110/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4145 - accuracy: 0.9985 - val_loss: 0.4136 - val_accuracy: 0.9928\n",
      "Epoch 111/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4119 - accuracy: 1.0000 - val_loss: 0.4121 - val_accuracy: 0.9928\n",
      "Epoch 112/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.4121 - accuracy: 0.9971 - val_loss: 0.4114 - val_accuracy: 0.9918\n",
      "Epoch 113/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4087 - accuracy: 1.0000 - val_loss: 0.4107 - val_accuracy: 0.9907\n",
      "Epoch 114/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4072 - accuracy: 1.0000 - val_loss: 0.4099 - val_accuracy: 0.9897\n",
      "Epoch 115/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4056 - accuracy: 1.0000 - val_loss: 0.4094 - val_accuracy: 0.9884\n",
      "Epoch 116/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.4041 - accuracy: 1.0000 - val_loss: 0.4087 - val_accuracy: 0.9872\n",
      "Epoch 117/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4025 - accuracy: 1.0000 - val_loss: 0.4080 - val_accuracy: 0.9861\n",
      "Epoch 118/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4010 - accuracy: 1.0000 - val_loss: 0.4075 - val_accuracy: 0.9850\n",
      "Epoch 119/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3995 - accuracy: 1.0000 - val_loss: 0.4069 - val_accuracy: 0.9835\n",
      "Epoch 120/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3990 - accuracy: 0.9985 - val_loss: 0.4065 - val_accuracy: 0.9820\n",
      "Epoch 121/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3964 - accuracy: 1.0000 - val_loss: 0.4062 - val_accuracy: 0.9805\n",
      "Epoch 122/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3951 - accuracy: 1.0000 - val_loss: 0.4058 - val_accuracy: 0.9793\n",
      "Epoch 123/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3944 - accuracy: 0.9985 - val_loss: 0.4038 - val_accuracy: 0.9800\n",
      "Epoch 124/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.3928 - accuracy: 0.9985 - val_loss: 0.4007 - val_accuracy: 0.9821\n",
      "Epoch 125/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.3905 - accuracy: 1.0000 - val_loss: 0.3980 - val_accuracy: 0.9838\n",
      "Epoch 126/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.3890 - accuracy: 1.0000 - val_loss: 0.3958 - val_accuracy: 0.9849\n",
      "Epoch 127/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3875 - accuracy: 1.0000 - val_loss: 0.3938 - val_accuracy: 0.9857\n",
      "Epoch 128/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3861 - accuracy: 1.0000 - val_loss: 0.3918 - val_accuracy: 0.9864\n",
      "Epoch 129/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3846 - accuracy: 1.0000 - val_loss: 0.3900 - val_accuracy: 0.9868\n",
      "Epoch 130/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3832 - accuracy: 1.0000 - val_loss: 0.3883 - val_accuracy: 0.9873\n",
      "Epoch 131/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3817 - accuracy: 1.0000 - val_loss: 0.3865 - val_accuracy: 0.9877\n",
      "Epoch 132/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3803 - accuracy: 1.0000 - val_loss: 0.3849 - val_accuracy: 0.9881\n",
      "Epoch 133/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3789 - accuracy: 1.0000 - val_loss: 0.3832 - val_accuracy: 0.9886\n",
      "Epoch 134/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3775 - accuracy: 1.0000 - val_loss: 0.3817 - val_accuracy: 0.9888\n",
      "Epoch 135/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3761 - accuracy: 1.0000 - val_loss: 0.3801 - val_accuracy: 0.9891\n",
      "Epoch 136/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.3747 - accuracy: 1.0000 - val_loss: 0.3786 - val_accuracy: 0.9893\n",
      "Epoch 137/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.3733 - accuracy: 1.0000 - val_loss: 0.3771 - val_accuracy: 0.9895\n",
      "Epoch 138/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.3719 - accuracy: 1.0000 - val_loss: 0.3757 - val_accuracy: 0.9896\n",
      "Epoch 139/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3705 - accuracy: 1.0000 - val_loss: 0.3743 - val_accuracy: 0.9897\n",
      "Epoch 140/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3692 - accuracy: 1.0000 - val_loss: 0.3730 - val_accuracy: 0.9899\n",
      "Epoch 141/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3678 - accuracy: 1.0000 - val_loss: 0.3716 - val_accuracy: 0.9899\n",
      "Epoch 142/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3664 - accuracy: 1.0000 - val_loss: 0.3702 - val_accuracy: 0.9900\n",
      "Epoch 143/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3651 - accuracy: 1.0000 - val_loss: 0.3689 - val_accuracy: 0.9901\n",
      "Epoch 144/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3637 - accuracy: 1.0000 - val_loss: 0.3676 - val_accuracy: 0.9901\n",
      "Epoch 145/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.3624 - accuracy: 1.0000 - val_loss: 0.3663 - val_accuracy: 0.9902\n",
      "Epoch 146/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.3611 - accuracy: 1.0000 - val_loss: 0.3651 - val_accuracy: 0.9903\n",
      "Epoch 147/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.3597 - accuracy: 1.0000 - val_loss: 0.3638 - val_accuracy: 0.9903\n",
      "Epoch 148/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3584 - accuracy: 1.0000 - val_loss: 0.3626 - val_accuracy: 0.9903\n",
      "Epoch 149/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3571 - accuracy: 1.0000 - val_loss: 0.3614 - val_accuracy: 0.9903\n",
      "Epoch 150/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3568 - accuracy: 0.9985 - val_loss: 0.3606 - val_accuracy: 0.9898\n",
      "Epoch 151/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3545 - accuracy: 1.0000 - val_loss: 0.3599 - val_accuracy: 0.9892\n",
      "Epoch 152/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3532 - accuracy: 1.0000 - val_loss: 0.3592 - val_accuracy: 0.9886\n",
      "Epoch 153/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3519 - accuracy: 1.0000 - val_loss: 0.3586 - val_accuracy: 0.9879\n",
      "Epoch 154/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.3507 - accuracy: 1.0000 - val_loss: 0.3579 - val_accuracy: 0.9874\n",
      "Epoch 155/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.3494 - accuracy: 1.0000 - val_loss: 0.3573 - val_accuracy: 0.9867\n",
      "Epoch 156/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.3481 - accuracy: 1.0000 - val_loss: 0.3566 - val_accuracy: 0.9862\n",
      "Epoch 157/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3469 - accuracy: 1.0000 - val_loss: 0.3559 - val_accuracy: 0.9857\n",
      "Epoch 158/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3456 - accuracy: 1.0000 - val_loss: 0.3552 - val_accuracy: 0.9851\n",
      "Epoch 159/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3444 - accuracy: 1.0000 - val_loss: 0.3545 - val_accuracy: 0.9847\n",
      "Epoch 160/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3431 - accuracy: 1.0000 - val_loss: 0.3538 - val_accuracy: 0.9843\n",
      "Epoch 161/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3419 - accuracy: 1.0000 - val_loss: 0.3531 - val_accuracy: 0.9838\n",
      "Epoch 162/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.3407 - accuracy: 1.0000 - val_loss: 0.3524 - val_accuracy: 0.9832\n",
      "Epoch 163/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.3395 - accuracy: 1.0000 - val_loss: 0.3516 - val_accuracy: 0.9829\n",
      "Epoch 164/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.3384 - accuracy: 1.0000 - val_loss: 0.3506 - val_accuracy: 0.9827\n",
      "Epoch 165/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.3371 - accuracy: 1.0000 - val_loss: 0.3495 - val_accuracy: 0.9827\n",
      "Epoch 166/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3358 - accuracy: 1.0000 - val_loss: 0.3483 - val_accuracy: 0.9827\n",
      "Epoch 167/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3346 - accuracy: 1.0000 - val_loss: 0.3472 - val_accuracy: 0.9829\n",
      "Epoch 168/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3335 - accuracy: 1.0000 - val_loss: 0.3460 - val_accuracy: 0.9829\n",
      "Epoch 169/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3322 - accuracy: 1.0000 - val_loss: 0.3448 - val_accuracy: 0.9830\n",
      "Epoch 170/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3311 - accuracy: 1.0000 - val_loss: 0.3436 - val_accuracy: 0.9831\n",
      "Epoch 171/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3299 - accuracy: 1.0000 - val_loss: 0.3424 - val_accuracy: 0.9831\n",
      "Epoch 172/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3287 - accuracy: 1.0000 - val_loss: 0.3413 - val_accuracy: 0.9832\n",
      "Epoch 173/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3275 - accuracy: 1.0000 - val_loss: 0.3401 - val_accuracy: 0.9832\n",
      "Epoch 174/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.3264 - accuracy: 1.0000 - val_loss: 0.3390 - val_accuracy: 0.9833\n",
      "Epoch 175/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.3252 - accuracy: 1.0000 - val_loss: 0.3379 - val_accuracy: 0.9833\n",
      "Epoch 176/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.3241 - accuracy: 1.0000 - val_loss: 0.3368 - val_accuracy: 0.9834\n",
      "Epoch 177/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3229 - accuracy: 1.0000 - val_loss: 0.3357 - val_accuracy: 0.9835\n",
      "Epoch 178/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3218 - accuracy: 1.0000 - val_loss: 0.3346 - val_accuracy: 0.9835\n",
      "Epoch 179/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3207 - accuracy: 1.0000 - val_loss: 0.3335 - val_accuracy: 0.9835\n",
      "Epoch 180/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3196 - accuracy: 1.0000 - val_loss: 0.3325 - val_accuracy: 0.9836\n",
      "Epoch 181/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3184 - accuracy: 1.0000 - val_loss: 0.3314 - val_accuracy: 0.9837\n",
      "Epoch 182/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.3173 - accuracy: 1.0000 - val_loss: 0.3303 - val_accuracy: 0.9837\n",
      "Epoch 183/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.3162 - accuracy: 1.0000 - val_loss: 0.3293 - val_accuracy: 0.9837\n",
      "Epoch 184/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.3151 - accuracy: 1.0000 - val_loss: 0.3283 - val_accuracy: 0.9837\n",
      "Epoch 185/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.3140 - accuracy: 1.0000 - val_loss: 0.3272 - val_accuracy: 0.9837\n",
      "Epoch 186/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3129 - accuracy: 1.0000 - val_loss: 0.3262 - val_accuracy: 0.9838\n",
      "Epoch 187/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3118 - accuracy: 1.0000 - val_loss: 0.3252 - val_accuracy: 0.9838\n",
      "Epoch 188/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3108 - accuracy: 1.0000 - val_loss: 0.3242 - val_accuracy: 0.9838\n",
      "Epoch 189/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3097 - accuracy: 1.0000 - val_loss: 0.3232 - val_accuracy: 0.9838\n",
      "Epoch 190/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3086 - accuracy: 1.0000 - val_loss: 0.3222 - val_accuracy: 0.9838\n",
      "Epoch 191/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3075 - accuracy: 1.0000 - val_loss: 0.3212 - val_accuracy: 0.9839\n",
      "Epoch 192/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.3065 - accuracy: 1.0000 - val_loss: 0.3202 - val_accuracy: 0.9839\n",
      "Epoch 193/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3054 - accuracy: 1.0000 - val_loss: 0.3192 - val_accuracy: 0.9839\n",
      "Epoch 194/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3044 - accuracy: 1.0000 - val_loss: 0.3182 - val_accuracy: 0.9839\n",
      "Epoch 195/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3033 - accuracy: 1.0000 - val_loss: 0.3172 - val_accuracy: 0.9840\n",
      "Epoch 196/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3023 - accuracy: 1.0000 - val_loss: 0.3162 - val_accuracy: 0.9840\n",
      "Epoch 197/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3013 - accuracy: 1.0000 - val_loss: 0.3153 - val_accuracy: 0.9840\n",
      "Epoch 198/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3002 - accuracy: 1.0000 - val_loss: 0.3143 - val_accuracy: 0.9840\n",
      "Epoch 199/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.2992 - accuracy: 1.0000 - val_loss: 0.3134 - val_accuracy: 0.9840\n",
      "Epoch 200/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.2982 - accuracy: 1.0000 - val_loss: 0.3124 - val_accuracy: 0.9840\n"
     ]
    }
   ],
   "source": [
    "# Build the best model based on the parameter tuning\n",
    "w1=1\n",
    "w2=1\n",
    "batch_size=2048\n",
    "epochs=200\n",
    "cm_results = []\n",
    "final_model = Sequential()\n",
    "final_model.add(LSTM(20, return_sequences=True, input_shape=X_train.shape[1:]))\n",
    "final_model.add(Dropout(0.2))\n",
    "final_model.add(LSTM(12, activation='relu', return_sequences=True))\n",
    "final_model.add(Dropout(0.2))\n",
    "final_model.add(LSTM(8, activation='relu', return_sequences=False)) # this is the last LSTM, so should return_sequences=False\n",
    "final_model.add(Dense(units=1, kernel_initializer='uniform', activation='sigmoid'))\n",
    "bias_initializer = ones/zeros \n",
    "#bias_initializer = np.log([bias_initializer])\n",
    "print('bias_initializer', bias_initializer)\n",
    "bias_initializer = initializers.Constant(bias_initializer)\n",
    "final_model.add(Dense(units=1, activation='sigmoid', bias_initializer=bias_initializer))\n",
    "optimizer = optimizers.Adam(lr=lr)\n",
    "final_model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy']) #optimizer='rmsprop', optimizer='sgd', optimizer='adam'\n",
    "\n",
    "model_output = final_model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=1,\n",
    "                         class_weight=np.where(y_train == 1,w1,w2).flatten(), validation_data=(val_features, val_labels) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN | AUC Score: 1.0\n",
      "TEST | AUC Score: 0.5043501711927209\n",
      "VAL | AUC Score: 0.49924539943058316\n",
      "error\n",
      "error\n",
      "Confusion Matrix:\n",
      "false positive pct: 1.4141224948492226\n",
      "tn  fp  fn  tp\n",
      "42029 604 78 1\n",
      "[[42029   604]\n",
      " [   78     1]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99     42633\n",
      "           1       0.00      0.01      0.00        79\n",
      "\n",
      "    accuracy                           0.98     42712\n",
      "   macro avg       0.50      0.50      0.50     42712\n",
      "weighted avg       1.00      0.98      0.99     42712\n",
      "\n",
      "Specificity = 0.985832571013065\n",
      "Sensitivity = 0.012658227848101266\n",
      "lr 0.01 w1 1 w2 1 epochs 200 batch_size 2048\n",
      "           algo     TN   FP  FN  TP    lr  w1  w2  epochs  batch_size     SP  \\\n",
      "0  sequential_2  42029  604  78   1  0.01   1   1     200        2048  0.986   \n",
      "\n",
      "      SE     Avg  \n",
      "0  0.013  0.4995  \n"
     ]
    }
   ],
   "source": [
    "val_predict = final_model.predict_classes(X_val)\n",
    "\n",
    "### test AUC ###\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_train, train_predict, pos_label=1)\n",
    "print('TRAIN | AUC Score: ' + str((metrics.auc(fpr, tpr))))\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, test_predict, pos_label=1)\n",
    "print('TEST | AUC Score: ' + str((metrics.auc(fpr, tpr))))\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_val, val_predict, pos_label=1)\n",
    "print('VAL | AUC Score: ' + str((metrics.auc(fpr, tpr))))\n",
    "tn, fp, fn, tp = display_metrics(final_model, X_train, X_val, y_train, y_val, val_predict)\n",
    "cm_results.append([final_model.name, tn, fp, fn, tp, lr, w1, w2, epochs, batch_size])\n",
    "print('lr',lr,'w1',w1,'w2',w2,'epochs',epochs,'batch_size',batch_size)\n",
    "final_results = pd.DataFrame(cm_results, columns=('algo','TN','FP','FN','TP', 'lr', 'w1', 'w2', 'epochs', 'batch_size')) \n",
    "final_results['SP'] = round(final_results['TN']/(final_results['TN'] + final_results['FP']), 3)\n",
    "final_results['SE'] = round(final_results['TP']/(final_results['TP'] + final_results['FN']), 3)\n",
    "final_results['Avg'] = (final_results['SP'] + final_results['SE'])/2\n",
    "\n",
    "print(final_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "\n",
    "So far cannot get avg of SP and SE above 50%\n",
    "\n",
    "1) Adam is more consistent for optimization\n",
    "2) Stratify doesn't make much difference, include it for consistency\n",
    "3) lr needs to be less than 0.05, keep at 0.01\n",
    "\n",
    "4) Try wts later\n",
    "5) epochs\n",
    "6) batch size\n",
    "\n",
    "Details:\n",
    "\n",
    "SGD vs Adam and Stratify vs No Stratify:\n",
    "\n",
    "    SGD with Stratify produces inconsistent results, lots of 0's in the confusion matrix, esp TP\n",
    "    Adam with stratify is much more consistent, almost always has some values in all 4 categories, and FP has less variation, even if TP count is still low (~1-3)\n",
    "\n",
    "    SGD with Stratify:\n",
    "\n",
    "                 algo     TN     FP   FN   TP    lr   w1   w2  epochs  batch_size  \\\n",
    "    0    sequential_1  79719   5566  142    6  0.01  4.0  1.0      10           8   \n",
    "    13  sequential_14  85202     83  148    0  0.01  4.0  1.0      10           8   \n",
    "    23  sequential_24  85160    125  148    0  0.01  4.0  1.0      10           8   \n",
    "    22  sequential_23  40018  45267   69   79  0.01  4.0  1.0      10           8   \n",
    "    21  sequential_22  85279      6  148    0  0.01  4.0  1.0      10           8   \n",
    "    20  sequential_21  84649    636  148    0  0.01  4.0  1.0      10           8   \n",
    "    19  sequential_20  85221     64  148    0  0.01  4.0  1.0      10           8   \n",
    "    18  sequential_19  85252     33  148    0  0.01  4.0  1.0      10           8 \n",
    "\n",
    "\n",
    "    SGD without Stratify:\n",
    "\n",
    "                 algo     TN     FP   FN   TP    lr   w1   w2  epochs  batch_size  \\\n",
    "    0    sequential_1  85216     77  140    0  0.01  4.0  1.0      10           8   \n",
    "    13  sequential_14  85035    258  138    2  0.01  4.0  1.0      10           8   \n",
    "    23  sequential_24  85255     38  139    1  0.01  4.0  1.0      10           8   \n",
    "    22  sequential_23  83812   1481  138    2  0.01  4.0  1.0      10           8   \n",
    "    21  sequential_22  84769    524  140    0  0.01  4.0  1.0      10           8   \n",
    "    20  sequential_21  76723   8570  129   11  0.01  4.0  1.0      10           8   \n",
    "    19  sequential_20  85146    147  139    1  0.01  4.0  1.0      10           8\n",
    "\n",
    "    ADAM with Stratify:\n",
    "\n",
    "                 algo     TN    FP   FN  TP    lr   w1   w2  epochs  batch_size  \\\n",
    "    0    sequential_1  84840   445  147   1  0.01  4.0  1.0      10           8   \n",
    "    13  sequential_14  84984   301  148   0  0.01  4.0  1.0      10           8   \n",
    "    23  sequential_24  84613   672  148   0  0.01  4.0  1.0      10           8   \n",
    "    22  sequential_23  84447   838  147   1  0.01  4.0  1.0      10           8   \n",
    "    21  sequential_22  84386   899  148   0  0.01  4.0  1.0      10           8   \n",
    "    20  sequential_21  85037   248  148   0  0.01  4.0  1.0      10           8   \n",
    "    19  sequential_20  84207  1078  145   3  0.01  4.0  1.0      10           8   \n",
    "    18  sequential_19  85001   284  148   0  0.01  4.0  1.0      10           8   \n",
    "    17  sequential_18  84990   295  148   0  0.01  4.0  1.0      10           8   \n",
    "    16  sequential_17  84863   422  148   0  0.01  4.0  1.0      10           8   \n",
    "    15  sequential_16  84585   700  147   1  0.01  4.0  1.0      10           8   \n",
    "    14  sequential_15  84728   557  148   0  0.01  4.0  1.0      10           8   \n",
    "    12  sequential_13  84396   889  146   2  0.01  4.0  1.0      10           8   \n",
    "    ADAM without Stratify:\n",
    "\n",
    "                 algo     TN    FP   FN  TP    lr   w1   w2  epochs  batch_size  \\\n",
    "    0    sequential_1  84218  1055  159   1  0.01  4.0  1.0      10           8   \n",
    "    13  sequential_14  85036   237  160   0  0.01  4.0  1.0      10           8   \n",
    "    23  sequential_24  84721   552  159   1  0.01  4.0  1.0      10           8   \n",
    "    22  sequential_23  84609   664  160   0  0.01  4.0  1.0      10           8   \n",
    "    21  sequential_22  84730   543  160   0  0.01  4.0  1.0      10           8   \n",
    "    20  sequential_21  85046   227  160   0  0.01  4.0  1.0      10           8   \n",
    "    19  sequential_20  84771   502  158   2  0.01  4.0  1.0      10           8   \n",
    "    18  sequential_19  84612   661  160   0  0.01  4.0  1.0      10           8   \n",
    "    17  sequential_18  82808  2465  154   6  0.01  4.0  1.0      10           8   \n",
    "    16  sequential_17  84382   891  159   1  0.01  4.0  1.0      10           8   \n",
    "    15  sequential_16  84715   558  160   0  0.01  4.0  1.0      10           8   \n",
    "    14  sequential_15  84879   394  160   0  0.01  4.0  1.0      10           8   \n",
    "    12  sequential_13  85240    33  160   0  0.01  4.0  1.0      10           8   \n",
    "    1    sequential_2  84906   367  160   0  0.01  4.0  1.0      10           8   \n",
    "\n",
    "lr values (ADAM and Stratify): use value of 0.025\n",
    "\n",
    "need value less than 0.05 to show consistency in results\n",
    "\n",
    "             algo     TN     FP   FN   TP        lr   w1   w2  epochs  \\\n",
    "24  sequential_25  85285      0  148    0  0.100000  4.0  1.0      10   \n",
    "23  sequential_24  85285      0  148    0  0.100000  4.0  1.0      10   \n",
    "22  sequential_23  85285      0  148    0  0.100000  4.0  1.0      10   \n",
    "21  sequential_22      0  85285    0  148  0.100000  4.0  1.0      10   \n",
    "20  sequential_21      1  85284    0  148  0.100000  4.0  1.0      10   \n",
    "19  sequential_20  84877    408  144    4  0.075025  4.0  1.0      10   \n",
    "18  sequential_19  84508    777  144    4  0.075025  4.0  1.0      10   \n",
    "17  sequential_18      0  85285    0  148  0.075025  4.0  1.0      10   \n",
    "16  sequential_17  81305   3980  141    7  0.075025  4.0  1.0      10   \n",
    "15  sequential_16  84733    552  145    3  0.075025  4.0  1.0      10  \n",
    "**** cutoff here\n",
    "13  sequential_14  83804   1481  143    5  0.050050  4.0  1.0      10   \n",
    "14  sequential_15  83316   1969  143    5  0.050050  4.0  1.0      10   \n",
    "12  sequential_13  85011    274  147    1  0.050050  4.0  1.0      10   \n",
    "11  sequential_12  84644    641  144    4  0.050050  4.0  1.0      10   \n",
    "10  sequential_11  85131    154  147    1  0.050050  4.0  1.0      10   \n",
    "9   sequential_10  84520    765  145    3  0.025075  4.0  1.0      10   \n",
    "8    sequential_9  84118   1167  142    6  0.025075  4.0  1.0      10   \n",
    "7    sequential_8  84874    411  147    1  0.025075  4.0  1.0      10   \n",
    "6    sequential_7  84557    728  144    4  0.025075  4.0  1.0      10   \n",
    "5    sequential_6  84555    730  144    4  0.025075  4.0  1.0      10   \n",
    "1    sequential_2  85038    247  147    1  0.000100  4.0  1.0      10   \n",
    "4    sequential_5  85123    162  146    2  0.000100  4.0  1.0      10   \n",
    "3    sequential_4  85026    259  147    1  0.000100  4.0  1.0      10   \n",
    "2    sequential_3  84763    522  146    2  0.000100  4.0  1.0      10   \n",
    "0    sequential_1  84537    748  146    2  0.000100  4.0  1.0      10\n",
    "\n",
    "\n",
    "             algo     TN    FP   FN  TP        lr   w1   w2  epochs  \\\n",
    "24  sequential_25  84318   967  146   2  0.050000  4.0  1.0      10   \n",
    "23  sequential_24  85045   240  147   1  0.050000  4.0  1.0      10   \n",
    "22  sequential_23  84672   613  146   2  0.050000  4.0  1.0      10   \n",
    "21  sequential_22  82670  2615  142   6  0.050000  4.0  1.0      10   \n",
    "20  sequential_21  83087  2198  146   2  0.050000  4.0  1.0      10   \n",
    "19  sequential_20  84231  1054  148   0  0.037525  4.0  1.0      10   \n",
    "18  sequential_19  85006   279  148   0  0.037525  4.0  1.0      10   \n",
    "17  sequential_18  84610   675  148   0  0.037525  4.0  1.0      10   \n",
    "16  sequential_17  84759   526  148   0  0.037525  4.0  1.0      10   \n",
    "15  sequential_16  84522   763  148   0  0.037525  4.0  1.0      10   \n",
    "13  sequential_14  83834  1451  146   2  0.025050  4.0  1.0      10   \n",
    "14  sequential_15  83814  1471  146   2  0.025050  4.0  1.0      10   \n",
    "12  sequential_13  84764   521  148   0  0.025050  4.0  1.0      10   \n",
    "11  sequential_12  84809   476  147   1  0.025050  4.0  1.0      10   \n",
    "10  sequential_11  84723   562  147   1  0.025050  4.0  1.0      10   \n",
    "9   sequential_10  84879   406  148   0  0.012575  4.0  1.0      10   \n",
    "8    sequential_9  83464  1821  147   1  0.012575  4.0  1.0      10   \n",
    "7    sequential_8  84803   482  148   0  0.012575  4.0  1.0      10   \n",
    "6    sequential_7  82689  2596  144   4  0.012575  4.0  1.0      10   \n",
    "5    sequential_6  83254  2031  147   1  0.012575  4.0  1.0      10   \n",
    "1    sequential_2  84693   592  148   0  0.000100  4.0  1.0      10   \n",
    "4    sequential_5  85028   257  148   0  0.000100  4.0  1.0      10   \n",
    "3    sequential_4  84807   478  148   0  0.000100  4.0  1.0      10   \n",
    "2    sequential_3  84371   914  148   0  0.000100  4.0  1.0      10   \n",
    "0    sequential_1  84495   790  148   0  0.000100  4.0  1.0      10  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    }
   ],
   "source": [
    "print('finish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
