{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "some ideas: https://stackoverflow.com/questions/41488279/neural-network-always-predicts-the-same-class\n",
    "<pre>\n",
    "change the split and downsampling order\n",
    "\n",
    "now: start with an imbalanced dataset\n",
    "     downsample all the data to a small balanced subset\n",
    "     split and shape using stratify the small subset into 70/30 for Train/Test\n",
    "     means the Test dataset is also balanced\n",
    "     \n",
    "want: start with imbalanced dataset\n",
    "      split using sklearn:train_test_split into 70/30 for Train/Test\n",
    "      downsample and shape the Train dataset to become balanced\n",
    "      shape the Test dataset\n",
    "      means the Test dataset is imbalanced as it should be\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "colab = os.environ.get('COLAB_GPU', '10')\n",
    "if (int(colab) == 0):\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')  \n",
    "else:\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory  C:\\DataScience\\Repo\\Imbalanced_data\\input\\creditcardzip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Check if Google Colab path exists\n",
    "if os.path.exists(\"/content/drive/My Drive/MyDSNotebooks/Imbalanced_data/input/creditcardzip\") :\n",
    "    # Change the current working Directory    \n",
    "    os.chdir(\"/content/drive/My Drive/MyDSNotebooks/Imbalanced_data/input/creditcardzip\")\n",
    "# else check if Kaggle/local path exists\n",
    "elif os.path.exists(\"../input/creditcardzip\") :\n",
    "    # Change the current working Directory    \n",
    "    os.chdir(\"../input/creditcardzip\")\n",
    "else:\n",
    "    print(\"Can't change the Current Working Directory\") \n",
    "print(\"Current Working Directory \" , os.getcwd())\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics \n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers import Embedding\n",
    "#from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras import regularizers\n",
    "from keras import optimizers\n",
    "from keras.optimizers import SGD\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.optimizers import Adagrad\n",
    "\n",
    "a='''\n",
    "# to try ---------------------------------------------\n",
    "# snippet of using the LearningRateScheduler callback\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "def my_learning_rate(epoch, lrate)\n",
    "\treturn lrate \n",
    "lrs = LearningRateScheduler(my_learning_rate)\n",
    "model.fit(..., callbacks=[lrs])\n",
    "\n",
    "#or\n",
    "\n",
    "# snippet of using the ReduceLROnPlateau callback\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "...\n",
    "rlrop = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=100)\n",
    "model.fit(..., callbacks=[rlrop])'''\n",
    "\n",
    "# -------------------------------------------------------\n",
    "#initialize variables\n",
    "\n",
    "cm_results = []\n",
    "class_names=[0,1] # name  of classes 1=fraudulent transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_metrics(model_name, train_features, test_features, train_label, test_label, pred):\n",
    "    try:\n",
    "        print(model_name.score(test_features, test_label)) \n",
    "        print(\"Accuracy score (training): {0:.3f}\".format(model_name.score(train_features, train_label))) \n",
    "        print(\"Accuracy score (validation): {0:.3f}\".format(model_name.score(test_features, test_label))) \n",
    "    except Exception as e:\n",
    "        print(\"error\")  \n",
    "    try:\n",
    "        print(pd.Series(model_name.feature_importances_, index=train_features.columns[:]).nlargest(10).plot(kind='barh')) \n",
    "    except Exception as e:\n",
    "        print(\"error\")  \n",
    "        \n",
    "    print(\"Confusion Matrix:\")\n",
    "    tn, fp, fn, tp = confusion_matrix(test_label, pred).ravel()\n",
    "    total = tn+ fp+ fn+ tp \n",
    "    print(\"false positive pct:\",(fp/total)*100) \n",
    "    print(\"tn\", \" fp\", \" fn\", \" tp\") \n",
    "    print(tn, fp, fn, tp) \n",
    "    print(confusion_matrix(test_label, pred)) \n",
    "    print(\"Classification Report\") \n",
    "    print(classification_report(test_label, pred))\n",
    "    print(\"Specificity =\", tn/(tn+fp))\n",
    "    print(\"Sensitivity =\", tp/(tp+fn))\n",
    "    return tn, fp, fn, tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('creditcard.csv', na_filter=True)\n",
    "X = df.loc[:, df.columns != 'Class']\n",
    "y = df.loc[:, df.columns == 'Class']\n",
    "\n",
    "#print(y['Class'].value_counts())\n",
    "# split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = None, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    284315\n",
      "1       492\n",
      "Name: Class, dtype: int64\n",
      "Frequency of unique values of the Train array:\n",
      "[[  0   1]\n",
      " [366 366]]\n",
      "Frequency of unique values of the Test array:\n",
      "[[    0     1]\n",
      " [85317   126]]\n"
     ]
    }
   ],
   "source": [
    "# prepare the Train and Test data for the model. Need input to include arrays with certain shape\n",
    "\n",
    "# downsample the Train data\n",
    "\n",
    "# find the number of minority (value=1) samples in our dataset so we can down-sample our majority to it\n",
    "yes = len(y_train[y_train['Class'] ==1])\n",
    "\n",
    "# retrieve the indices of the minority and majority samples \n",
    "yes_ind = y_train[y_train['Class'] == 1].index\n",
    "no_ind = y_train[y_train['Class'] == 0].index\n",
    "\n",
    "# random sample the majority indices based on the amount of \n",
    "# minority samples\n",
    "new_no_ind = np.random.choice(no_ind, yes, replace = False)\n",
    "\n",
    "# merge the two indices together\n",
    "undersample_ind = np.concatenate([new_no_ind, yes_ind])\n",
    "\n",
    "# get undersampled dataframe from the merged indices of the dataset\n",
    "X_train = X_train.loc[undersample_ind]\n",
    "y_temp = y_train.loc[undersample_ind]\n",
    "y_train_original = np.array(y_temp['Class'], dtype='int')\n",
    "y_test_original = np.array(y_test['Class'], dtype='int')\n",
    "\n",
    "#show how the data was split between the 2 classes\n",
    "print(y['Class'].value_counts())\n",
    "unique_elements, counts_elements = np.unique(y_train_original, return_counts=True)\n",
    "print(\"Frequency of unique values of the Train array:\")\n",
    "print(np.asarray((unique_elements, counts_elements)))\n",
    "unique_elements, counts_elements = np.unique(y_test_original, return_counts=True)\n",
    "print(\"Frequency of unique values of the Test array:\")\n",
    "print(np.asarray((unique_elements, counts_elements)))\n",
    "\n",
    "#shape the Train data \n",
    "rolling_window_size = 10  ### this selects how many historical transactions should be analyzed to judge the transaction at hand -- RNN width\n",
    "\n",
    "X_train_interim = np.zeros([(X_train.shape[0]-rolling_window_size)*rolling_window_size,30])\n",
    "y_train = []\n",
    "for i in range((X_train.shape[0]-rolling_window_size)):\n",
    "    beg = 0+i\n",
    "    end = beg+rolling_window_size\n",
    "    s = np.array(X_train[beg:end], dtype='float')\n",
    "    X_train_interim[(rolling_window_size*i):(rolling_window_size*(i+1)),:] = s\n",
    "    y_train.append(y_train_original[end])\n",
    "\n",
    "y_train = np.array(y_train, dtype='int')\n",
    "X_train_interim = X_train_interim[:,1::]\n",
    "\n",
    "X_train_tensor = X_train_interim.reshape(int(np.shape(X_train_interim)[0]/rolling_window_size), rolling_window_size, np.shape(X_train_interim)[1])\n",
    "X_train = X_train_tensor\n",
    "\n",
    "#shape and split the Test data \n",
    "\n",
    "#X_test_interim = np.zeros([(X_test.shape[0]-rolling_window_size)*10,30])\n",
    "X_test_interim = np.zeros([(X_test.shape[0]-rolling_window_size)*rolling_window_size,30])\n",
    "y_test = []\n",
    "for i in range((X_test.shape[0]-rolling_window_size)):\n",
    "    beg = 0+i\n",
    "    end = beg+rolling_window_size\n",
    "    s = np.array(X_test[beg:end], dtype='float')\n",
    "    X_test_interim[(rolling_window_size*i):(rolling_window_size*(i+1)),:] = s\n",
    "    y_test.append(y_test_original[end]) \n",
    "\n",
    "y_test = np.array(y_test, dtype='int')\n",
    "X_test_interim = X_test_interim[:,1::]\n",
    "\n",
    "X_test_tensor = X_test_interim.reshape(int(np.shape(X_test_interim)[0]/rolling_window_size), rolling_window_size, np.shape(X_test_interim)[1])\n",
    "X_test = X_test_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "722/722 [==============================] - 2s 3ms/step - loss: 0.4426 - accuracy: 0.8864\n",
      "Epoch 2/20\n",
      "722/722 [==============================] - 1s 1ms/step - loss: 0.0319 - accuracy: 0.9917\n",
      "Epoch 3/20\n",
      "722/722 [==============================] - 1s 1ms/step - loss: 0.0152 - accuracy: 0.9958\n",
      "Epoch 4/20\n",
      "722/722 [==============================] - 1s 2ms/step - loss: 0.0138 - accuracy: 0.9958\n",
      "Epoch 5/20\n",
      "722/722 [==============================] - 1s 2ms/step - loss: 0.0099 - accuracy: 0.9972\n",
      "Epoch 6/20\n",
      "722/722 [==============================] - 1s 1ms/step - loss: 0.0055 - accuracy: 0.9986\n",
      "Epoch 7/20\n",
      "722/722 [==============================] - 1s 1ms/step - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 8/20\n",
      "722/722 [==============================] - 1s 2ms/step - loss: 6.0095e-04 - accuracy: 1.0000\n",
      "Epoch 9/20\n",
      "722/722 [==============================] - 1s 2ms/step - loss: 0.0066 - accuracy: 0.9972\n",
      "Epoch 10/20\n",
      "722/722 [==============================] - 1s 2ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 11/20\n",
      "722/722 [==============================] - 1s 2ms/step - loss: 2.5255e-04 - accuracy: 1.0000: 0s - loss: 3.4646e-04 \n",
      "Epoch 12/20\n",
      "722/722 [==============================] - 1s 2ms/step - loss: 1.6730e-04 - accuracy: 1.0000\n",
      "Epoch 13/20\n",
      "722/722 [==============================] - 1s 2ms/step - loss: 4.1275e-04 - accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "722/722 [==============================] - 1s 2ms/step - loss: 0.0023 - accuracy: 0.9986\n",
      "Epoch 15/20\n",
      "722/722 [==============================] - 1s 2ms/step - loss: 4.1843e-04 - accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "722/722 [==============================] - 1s 2ms/step - loss: 1.5627e-04 - accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "722/722 [==============================] - 1s 2ms/step - loss: 1.7084e-04 - accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "722/722 [==============================] - 1s 2ms/step - loss: 5.8914e-05 - accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "722/722 [==============================] - 1s 2ms/step - loss: 4.0776e-05 - accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "722/722 [==============================] - 1s 2ms/step - loss: 2.0923e-05 - accuracy: 1.0000\n",
      "TRAIN | AUC Score: 1.0\n",
      "TEST | AUC Score: 0.5022157600345791\n",
      "error\n",
      "error\n",
      "Confusion Matrix:\n",
      "false positive pct: 0.3499818571278077\n",
      "tn  fp  fn  tp\n",
      "85008 299 125 1\n",
      "[[85008   299]\n",
      " [  125     1]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     85307\n",
      "           1       0.00      0.01      0.00       126\n",
      "\n",
      "    accuracy                           1.00     85433\n",
      "   macro avg       0.50      0.50      0.50     85433\n",
      "weighted avg       1.00      1.00      1.00     85433\n",
      "\n",
      "Specificity = 0.9964950121326503\n",
      "Sensitivity = 0.007936507936507936\n",
      "lr 0.01 w1 4.0 w2 1.0 epochs 20 batch_size 8\n",
      "Epoch 1/20\n",
      "722/722 [==============================] - 2s 3ms/step - loss: 0.4963 - accuracy: 0.8366\n",
      "Epoch 2/20\n",
      "722/722 [==============================] - 1s 1ms/step - loss: 0.1952 - accuracy: 0.9917\n",
      "Epoch 3/20\n",
      "722/722 [==============================] - 1s 1ms/step - loss: 0.0250 - accuracy: 0.9945\n",
      "Epoch 4/20\n",
      "722/722 [==============================] - 1s 1ms/step - loss: 0.0135 - accuracy: 0.9958\n",
      "Epoch 5/20\n",
      "722/722 [==============================] - 1s 1ms/step - loss: 0.0064 - accuracy: 0.9986\n",
      "Epoch 6/20\n",
      "722/722 [==============================] - 1s 1ms/step - loss: 0.0047 - accuracy: 0.9986\n",
      "Epoch 7/20\n",
      "722/722 [==============================] - 1s 1ms/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 8/20\n",
      "722/722 [==============================] - 1s 1ms/step - loss: 0.0115 - accuracy: 0.9986\n",
      "Epoch 9/20\n",
      "722/722 [==============================] - 1s 1ms/step - loss: 0.0057 - accuracy: 0.9986\n",
      "Epoch 10/20\n",
      "722/722 [==============================] - 1s 2ms/step - loss: 0.0108 - accuracy: 0.9958\n",
      "Epoch 11/20\n",
      "722/722 [==============================] - 1s 1ms/step - loss: 0.0062 - accuracy: 0.9986\n",
      "Epoch 12/20\n",
      "722/722 [==============================] - 1s 1ms/step - loss: 7.2566e-04 - accuracy: 1.0000\n",
      "Epoch 13/20\n",
      "722/722 [==============================] - 1s 1ms/step - loss: 0.0164 - accuracy: 0.9958\n",
      "Epoch 14/20\n",
      "722/722 [==============================] - 1s 2ms/step - loss: 0.0054 - accuracy: 0.9986\n",
      "Epoch 15/20\n",
      "722/722 [==============================] - 1s 2ms/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "722/722 [==============================] - 1s 1ms/step - loss: 8.1047e-04 - accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "722/722 [==============================] - 1s 1ms/step - loss: 3.2311e-04 - accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "722/722 [==============================] - 1s 2ms/step - loss: 3.0444e-04 - accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "722/722 [==============================] - 1s 2ms/step - loss: 9.9388e-05 - accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "722/722 [==============================] - 1s 1ms/step - loss: 6.7571e-05 - accuracy: 1.0000\n",
      "TRAIN | AUC Score: 1.0\n",
      "TEST | AUC Score: 0.5000471220564531\n",
      "error\n",
      "error\n",
      "Confusion Matrix:\n",
      "false positive pct: 0.7830697739749277\n",
      "tn  fp  fn  tp\n",
      "84638 669 125 1\n",
      "[[84638   669]\n",
      " [  125     1]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00     85307\n",
      "           1       0.00      0.01      0.00       126\n",
      "\n",
      "    accuracy                           0.99     85433\n",
      "   macro avg       0.50      0.50      0.50     85433\n",
      "weighted avg       1.00      0.99      0.99     85433\n",
      "\n",
      "Specificity = 0.9921577361763981\n",
      "Sensitivity = 0.007936507936507936\n",
      "lr 0.01 w1 4.0 w2 1.0 epochs 20 batch_size 8\n",
      "Epoch 1/20\n",
      "722/722 [==============================] - 2s 3ms/step - loss: 0.4274 - accuracy: 0.9432\n",
      "Epoch 2/20\n",
      "722/722 [==============================] - 1s 2ms/step - loss: 0.0816 - accuracy: 0.9903\n",
      "Epoch 3/20\n",
      "722/722 [==============================] - 1s 2ms/step - loss: 0.0221 - accuracy: 0.9945\n",
      "Epoch 4/20\n",
      "722/722 [==============================] - 1s 1ms/step - loss: 0.0196 - accuracy: 0.9958\n",
      "Epoch 5/20\n",
      "722/722 [==============================] - 1s 2ms/step - loss: 0.0088 - accuracy: 0.9972\n",
      "Epoch 6/20\n",
      "722/722 [==============================] - 1s 2ms/step - loss: 0.0287 - accuracy: 0.9917\n",
      "Epoch 7/20\n",
      "722/722 [==============================] - 1s 2ms/step - loss: 0.0090 - accuracy: 0.9972\n",
      "Epoch 8/20\n",
      "722/722 [==============================] - 1s 2ms/step - loss: 0.0043 - accuracy: 0.9986: 0s - loss: 0.0\n",
      "Epoch 9/20\n",
      "722/722 [==============================] - 1s 2ms/step - loss: 0.0082 - accuracy: 0.9972\n",
      "Epoch 10/20\n",
      "722/722 [==============================] - 1s 2ms/step - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 11/20\n",
      "722/722 [==============================] - 1s 2ms/step - loss: 0.0089 - accuracy: 0.9958\n",
      "Epoch 12/20\n",
      "722/722 [==============================] - 1s 2ms/step - loss: 0.0079 - accuracy: 0.9958\n",
      "Epoch 13/20\n",
      "722/722 [==============================] - 1s 2ms/step - loss: 0.0032 - accuracy: 0.9986\n",
      "Epoch 14/20\n",
      "722/722 [==============================] - 1s 2ms/step - loss: 0.0047 - accuracy: 0.9986\n",
      "Epoch 15/20\n",
      "722/722 [==============================] - 1s 2ms/step - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "722/722 [==============================] - 1s 2ms/step - loss: 7.5353e-04 - accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "722/722 [==============================] - 1s 1ms/step - loss: 5.0164e-04 - accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "722/722 [==============================] - 1s 2ms/step - loss: 0.0285 - accuracy: 0.9972\n",
      "Epoch 19/20\n",
      "722/722 [==============================] - 1s 2ms/step - loss: 0.0073 - accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "722/722 [==============================] - 1s 2ms/step - loss: 0.0012 - accuracy: 1.0000\n",
      "TRAIN | AUC Score: 0.9986338797814207\n",
      "TEST | AUC Score: 0.5026260428953057\n",
      "error\n",
      "error\n",
      "Confusion Matrix:\n",
      "false positive pct: 0.2680463052918661\n",
      "tn  fp  fn  tp\n",
      "85078 229 125 1\n",
      "[[85078   229]\n",
      " [  125     1]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     85307\n",
      "           1       0.00      0.01      0.01       126\n",
      "\n",
      "    accuracy                           1.00     85433\n",
      "   macro avg       0.50      0.50      0.50     85433\n",
      "weighted avg       1.00      1.00      1.00     85433\n",
      "\n",
      "Specificity = 0.9973155778541034\n",
      "Sensitivity = 0.007936507936507936\n",
      "lr 0.01 w1 4.0 w2 1.0 epochs 20 batch_size 8\n",
      "Epoch 1/20\n",
      "722/722 [==============================] - 2s 3ms/step - loss: 0.4487 - accuracy: 0.9335\n",
      "Epoch 2/20\n",
      "722/722 [==============================] - 1s 2ms/step - loss: 0.0406 - accuracy: 0.9931\n",
      "Epoch 3/20\n",
      "722/722 [==============================] - 1s 2ms/step - loss: 0.0216 - accuracy: 0.9945\n",
      "Epoch 4/20\n",
      "722/722 [==============================] - 1s 1ms/step - loss: 0.0155 - accuracy: 0.9958\n",
      "Epoch 5/20\n",
      "722/722 [==============================] - 1s 2ms/step - loss: 0.0123 - accuracy: 0.9958\n",
      "Epoch 6/20\n",
      "722/722 [==============================] - 1s 2ms/step - loss: 0.0105 - accuracy: 0.9958\n",
      "Epoch 7/20\n",
      "722/722 [==============================] - 1s 2ms/step - loss: 0.0073 - accuracy: 0.9986\n",
      "Epoch 8/20\n",
      "722/722 [==============================] - 1s 2ms/step - loss: 0.0100 - accuracy: 0.9972\n",
      "Epoch 9/20\n",
      "722/722 [==============================] - 1s 1ms/step - loss: 0.0053 - accuracy: 0.9972\n",
      "Epoch 10/20\n",
      "722/722 [==============================] - 1s 2ms/step - loss: 0.0036 - accuracy: 0.9972\n",
      "Epoch 11/20\n",
      "722/722 [==============================] - 1s 2ms/step - loss: 0.0101 - accuracy: 0.9958\n",
      "Epoch 12/20\n",
      "722/722 [==============================] - 1s 2ms/step - loss: 0.0046 - accuracy: 0.9958\n",
      "Epoch 13/20\n",
      "722/722 [==============================] - 1s 2ms/step - loss: 0.0050 - accuracy: 0.9972\n",
      "Epoch 14/20\n",
      "722/722 [==============================] - 1s 1ms/step - loss: 0.0043 - accuracy: 0.9972\n",
      "Epoch 15/20\n",
      "722/722 [==============================] - 1s 2ms/step - loss: 0.0042 - accuracy: 0.9972\n",
      "Epoch 16/20\n",
      "722/722 [==============================] - 1s 2ms/step - loss: 0.0038 - accuracy: 0.9972: 0s - loss: 0.0042 - \n",
      "Epoch 17/20\n",
      "722/722 [==============================] - 1s 2ms/step - loss: 0.0030 - accuracy: 0.9972\n",
      "Epoch 18/20\n",
      "722/722 [==============================] - 1s 2ms/step - loss: 0.0029 - accuracy: 0.9972\n",
      "Epoch 19/20\n",
      "722/722 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9986\n",
      "Epoch 20/20\n",
      "722/722 [==============================] - 1s 2ms/step - loss: 0.0022 - accuracy: 0.9986\n",
      "TRAIN | AUC Score: 0.9986338797814207\n",
      "TEST | AUC Score: 0.49883948562251634\n",
      "error\n",
      "error\n",
      "Confusion Matrix:\n",
      "false positive pct: 0.2317605609073777\n",
      "tn  fp  fn  tp\n",
      "85109 198 126 0\n",
      "[[85109   198]\n",
      " [  126     0]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     85307\n",
      "           1       0.00      0.00      0.00       126\n",
      "\n",
      "    accuracy                           1.00     85433\n",
      "   macro avg       0.50      0.50      0.50     85433\n",
      "weighted avg       1.00      1.00      1.00     85433\n",
      "\n",
      "Specificity = 0.9976789712450327\n",
      "Sensitivity = 0.0\n",
      "lr 0.01 w1 4.0 w2 1.0 epochs 20 batch_size 8\n"
     ]
    }
   ],
   "source": [
    "# run model\n",
    "\n",
    "### Hyperparameters Tuning\n",
    "# First test optimal epochs holding everything else constant\n",
    "# Dropout: 0.1-0.6\n",
    "# GradientClipping: 0.1-10\n",
    "# BatchSize: 32,64,128,256,512 (power of 2)\n",
    "\n",
    "# setting variables\n",
    "lr = 0.01\n",
    "w1 = 4.0\n",
    "w2 = 1.0\n",
    "epochs = 20\n",
    "batch_size = 8\n",
    "min_ = 0\n",
    "max_ = 2\n",
    "\n",
    "for x in np.linspace(.0001, .01, num=2):\n",
    "    for n in np.linspace(0, 3, num=2):\n",
    "        ### Train LSTM using Keras 2 API ###\n",
    "        model = Sequential()\n",
    "        #model.add(LSTM(20, return_sequences=True, input_shape=X_train.shape[1:], kernel_initializer='lecun_uniform', activation='relu', kernel_regularizer=regularizers.l1(0.1), recurrent_regularizer=regularizers.l1(0.01), bias_regularizer=None, activity_regularizer=None, dropout=0.2, recurrent_dropout=0.2))#, return_sequences=True))\n",
    "        #model.add(LSTM(20, return_sequences=True, input_shape=X_train.shape[1:], activation='relu', kernel_regularizer=regularizers.l1(0.1), recurrent_regularizer=regularizers.l1(0.01), bias_regularizer=None, activity_regularizer=None, dropout=0.2, recurrent_dropout=0.2))#, return_sequences=True))\n",
    "        # defaults seem to work best, otherwise get very low score, AUC of 0.5 and all 0 or all 1 predictions\n",
    "        model.add(LSTM(20, return_sequences=True, input_shape=X_train.shape[1:]))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(LSTM(12, activation='relu', return_sequences=True))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(LSTM(8, activation='relu', return_sequences=False)) # this is the last LSTM, so should return_sequences=False\n",
    "        #model.add(Dense(1, kernel_initializer='lecun_uniform', activation='sigmoid'))\n",
    "        model.add(Dense(units=1, kernel_initializer='uniform', activation='sigmoid'))\n",
    "\n",
    "        #optimizer = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False) # ValueError: None values not supported.\n",
    "        optimizer = optimizers.Adam(lr=0.001)\n",
    "        #model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "        #model.compile(loss='mean_squared_error', optimizer=adam)\n",
    "        #model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy']) #optimizer='rmsprop', optimizer='sgd', optimizer='adam'\n",
    "\n",
    "        a='''\n",
    "        lr_schedule = optimizers.schedules.ExponentialDecay(\n",
    "            initial_learning_rate=1e-2,\n",
    "            decay_steps=10000,\n",
    "            decay_rate=0.9)\n",
    "        optimizer = optimizers.SGD(learning_rate=lr_schedule)'''\n",
    "\n",
    "        #optimizer = optimizers.SGD(learning_rate=lr)\n",
    "        #optimizer = optimizers.SGD(learning_rate=0.01, momentum=0.9,decay=0.01)\n",
    "\n",
    "        model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy']) #optimizer='rmsprop', optimizer='sgd', optimizer='adam'\n",
    "\n",
    "        #print(model.summary())\n",
    "\n",
    "        #model.fit(X_train, y_train, epochs=200, batch_size=10000, class_weight={0 : 1., 1: float(int(1/np.mean(y_train)))}, validation_split=0.3)\n",
    "        #model.fit(X_train, y_train, epochs=4, batch_size=8)#, class_weight=np.where(y_train == 1,4.0,1.0).flatten() )\n",
    "        model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, class_weight=np.where(y_train == 1,w1,w2).flatten() )\n",
    "\n",
    "        train_predict = model.predict_classes(X_train)\n",
    "        test_predict = model.predict_classes(X_test)\n",
    "\n",
    "        ### test AUC ###\n",
    "\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(y_train, train_predict, pos_label=1)\n",
    "        print('TRAIN | AUC Score: ' + str((metrics.auc(fpr, tpr))))\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(y_test, test_predict, pos_label=1)\n",
    "        print('TEST | AUC Score: ' + str((metrics.auc(fpr, tpr))))\n",
    "        tn, fp, fn, tp = display_metrics(model, X_train, X_test, y_train, y_test, test_predict)\n",
    "        cm_results.append([model.name, tn, fp, fn, tp, lr, w1, w2, epochs, batch_size])\n",
    "        print('lr',lr,'w1',w1,'w2',w2,'epochs',epochs,'batch_size',batch_size)\n",
    "        del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [algo, TN, FP, FN, TP, lr, w1, w2, epochs, batch_size, SP, SE, Avg]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "final_results = pd.DataFrame(cm_results, columns=('algo','TN','FP','FN','TP', 'lr', 'w1', 'w2', 'epochs', 'batch_size')) \n",
    "#sp = round((tn1 + tn2)/(tn1 + tn2 +fp2), 3)\n",
    "#se = round(tp2/(tp2 + fn1 + fn2), 3)\n",
    "final_results['SP'] = round(final_results['TN']/(final_results['TN'] + final_results['FP']), 3)\n",
    "final_results['SE'] = round(final_results['TP']/(final_results['TP'] + final_results['FN']), 3)\n",
    "final_results['Avg'] = (final_results['SP'] + final_results['SE'])/2\n",
    "\n",
    "filtered = final_results[~final_results.algo.str.contains('a', regex= True, na=False)]\n",
    "sort = filtered.sort_values(filtered.columns[7], ascending = False)\n",
    "print(sort)\n",
    "sort.to_csv('results_lstm.csv', sep=',', mode='a', encoding='utf-8', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort = final_results.sort_values(final_results.columns[5], ascending = False)\n",
    "print(sort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
