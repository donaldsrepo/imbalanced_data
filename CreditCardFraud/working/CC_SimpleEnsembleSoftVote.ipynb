{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ExtraTreesClassifier is the best classifier for this model, RandomForestClassifier is #2\n",
    "Sometimes the Voting Classifier has a worse score, need to be careful to apply this. \n",
    "Setting the train data to a balanced between 0/1 really hurts several models => too many FP values\n",
    "Trying different values for the RUS sampling_strategy, so far 0.1 seems okay. .002 is close to normal ratio in dataset. The higher the value, the faster the algorithm. Using .002 takes several hours, but using 1.0 takes a few minutes. KNN and other models will be heavily impacted by the amount of data modelled..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[MLENS] backend: threading\n"
     ]
    }
   ],
   "source": [
    "from mlens.preprocessing import Subset\n",
    "import numpy as np\n",
    "import datetime\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn.datasets\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.neighbors import  KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from collections import Counter\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "StartTime = datetime.datetime.now()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://storage.googleapis.com/download.tensorflow.org/data/creditcard.csv')\n",
    "X = df.loc[:, df.columns != 'Class']\n",
    "y = df.loc[:, df.columns == 'Class']\n",
    "y = np.array(y).ravel()\n",
    "\n",
    "#dataset = sklearn.datasets.load_breast_cancer(return_X_y=False)\n",
    "#X = pd.DataFrame(dataset.data, columns=dataset.feature_names)\n",
    "#y = dataset.target \n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,y,test_size=0.20,random_state=201, stratify=y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 39400), (1, 394)]\n"
     ]
    }
   ],
   "source": [
    "sampling = 'true'\n",
    "\n",
    "if (sampling == 'true'):\n",
    "    # change sample based on this library: https://github.com/scikit-learn-contrib/imbalanced-learn\n",
    "    # https://imbalanced-learn.org/stable/\n",
    "    rus = RandomUnderSampler(random_state=0, sampling_strategy=0.01, replacement=False) # ss of 1.0 gives equally sized 0 and 1 counts\n",
    "    x_train, y_train = rus.fit_resample(x_train, y_train)\n",
    "    print(sorted(Counter(y_train).items()))\n",
    "    x_train    \n",
    "else:\n",
    "    print(\"no sampling\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "validate the splits has equal pct of 1 value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTimer():\n",
    "    # usage:\n",
    "    #with MyTimer():                            \n",
    "    #    rf.fit(X_train, y_train)\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.start = time.time()\n",
    "    def __enter__(self):\n",
    "        return self\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        end = time.time()\n",
    "        runtime = end - self.start\n",
    "        msg = 'The function took {time} seconds to complete'\n",
    "        print(msg.format(time=runtime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_clf = DecisionTreeClassifier()\n",
    "ab_clf = AdaBoostClassifier()\n",
    "bc_clf = BaggingClassifier(n_estimators=10)\n",
    "et_clf = ExtraTreesClassifier(n_estimators=10)\n",
    "gbc_clf = GradientBoostingClassifier()\n",
    "lgb_clf = LGBMClassifier()\n",
    "gnb_clf = GaussianNB()\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=2)\n",
    "#dbs_clf = DBSCAN(eps=3, min_samples=10)\n",
    "svm_clf = SVC(gamma='scale', probability=True)\n",
    "lr_clf  = LogisticRegression(solver='liblinear')\n",
    "gb_clf  = GradientBoostingClassifier()\n",
    "#gb_clf  = GradientBoostingClassifier(n_estimators=20, learning_rate=0.1, max_features=5, \n",
    "#                                    max_depth=3, random_state=None, subsample = 0.5, criterion='mse', \n",
    "#                                    min_samples_split = 10, min_samples_leaf = 10)\n",
    "\n",
    "rf_clf  = RandomForestClassifier(n_estimators = 1000)\n",
    "xgb_clf = XGBClassifier() # learning_rate =0.1, n_estimators=1000, max_depth=5, min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8, objective= 'binary:logistic', nthread=4, scale_pos_weight=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "setup an ensemble using different classifiers, try to find a case where 2 classifiers beat 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The function took 24.22335958480835 seconds to complete\n",
      "KNeighborsClassifier 0.9982795547909132\n",
      "0.5662122227900727\n",
      "[[56851    13]\n",
      " [   85    13]]\n",
      "SVC 0.9982795547909132\n",
      "0.5\n",
      "[[56864     0]\n",
      " [   98     0]]\n",
      "VotingClassifier 0.9982795547909132\n",
      "0.5662122227900727\n",
      "[[56851    13]\n",
      " [   85    13]]\n",
      "The function took 57.432759284973145 seconds to complete\n"
     ]
    }
   ],
   "source": [
    "# add models here to the ensemble\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('knn', knn_clf), ('svc', svm_clf)], voting='soft')\n",
    "\n",
    "with MyTimer(): \n",
    "    voting_clf.fit(x_train, y_train)\n",
    "\n",
    "# compare single models to ensemble voting model\n",
    "# add models here\n",
    "with MyTimer(): \n",
    "    for clf in (knn_clf, svm_clf, voting_clf):\n",
    "        clf.fit(x_train, y_train)\n",
    "        y_pred = clf.predict(x_test)\n",
    "        print(clf.__class__.__name__, accuracy_score(y_test, y_pred))\n",
    "        print(roc_auc_score(y_test, y_pred))\n",
    "        print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score is bad for all, and ensemble is no better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The function took 48.56696057319641 seconds to complete\n",
      "KNeighborsClassifier 0.9982795547909132\n",
      "0.5662122227900727\n",
      "[[56851    13]\n",
      " [   85    13]]\n",
      "GradientBoostingClassifier 0.9977002212000983\n",
      "0.9020764186372354\n",
      "[[56752   112]\n",
      " [   19    79]]\n",
      "VotingClassifier 0.9984199992977775\n",
      "0.622308293041471\n",
      "[[56848    16]\n",
      " [   74    24]]\n",
      "The function took 111.18328142166138 seconds to complete\n"
     ]
    }
   ],
   "source": [
    "# add models here\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('knn', knn_clf), ('gb', gb_clf)], voting='soft')\n",
    "\n",
    "with MyTimer(): \n",
    "    voting_clf.fit(x_train, y_train)\n",
    "\n",
    "# compare single models to ensemble voting model\n",
    "# add models here\n",
    "with MyTimer(): \n",
    "    for clf in (knn_clf, gb_clf, voting_clf):\n",
    "        clf.fit(x_train, y_train)\n",
    "        y_pred = clf.predict(x_test)\n",
    "        print(clf.__class__.__name__, accuracy_score(y_test, y_pred))\n",
    "        print(roc_auc_score(y_test, y_pred))\n",
    "        print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score is bad for all, and ensemble is worse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The function took 67.38122987747192 seconds to complete\n",
      "SVC 0.9982795547909132\n",
      "0.5\n",
      "[[56864     0]\n",
      " [   98     0]]\n",
      "GradientBoostingClassifier 0.9977879990168884\n",
      "0.9021203831842247\n",
      "[[56757   107]\n",
      " [   19    79]]\n",
      "VotingClassifier 0.9982444436641972\n",
      "0.8768827592939258\n",
      "[[56788    76]\n",
      " [   24    74]]\n",
      "The function took 143.0255491733551 seconds to complete\n"
     ]
    }
   ],
   "source": [
    "# add models here\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('svc', svm_clf), ('gb', gb_clf)], voting='soft')\n",
    "\n",
    "with MyTimer(): \n",
    "    voting_clf.fit(x_train, y_train)\n",
    "\n",
    "# compare single models to ensemble voting model\n",
    "# add models here\n",
    "with MyTimer(): \n",
    "    for clf in (svm_clf, gb_clf, voting_clf):\n",
    "        clf.fit(x_train, y_train)\n",
    "        y_pred = clf.predict(x_test)\n",
    "        print(clf.__class__.__name__, accuracy_score(y_test, y_pred))\n",
    "        print(roc_auc_score(y_test, y_pred))\n",
    "        print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score is bad for all, and ensemble is no better\n",
    "svc is horrible here, can't predict the minority case at all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The function took 368.2173798084259 seconds to complete\n",
      "RandomForestClassifier 0.999420666409185\n",
      "0.9182178674790119\n",
      "[[56847    17]\n",
      " [   16    82]]\n",
      "GradientBoostingClassifier 0.9977704434535304\n",
      "0.9021115902748269\n",
      "[[56756   108]\n",
      " [   19    79]]\n",
      "VotingClassifier 0.9980337769039008\n",
      "0.9022434839157949\n",
      "[[56771    93]\n",
      " [   19    79]]\n",
      "The function took 765.7208182811737 seconds to complete\n"
     ]
    }
   ],
   "source": [
    "# add models here\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('rf', rf_clf), ('gb', gb_clf)], voting='soft')\n",
    "\n",
    "wt1 = 4.0\n",
    "wt0 = 1.0\n",
    "\n",
    "with MyTimer(): \n",
    "    #voting_clf.fit(x_train, y_train, sample_weight=np.where(y_train == 1, wt1, wt0).flatten())\n",
    "    voting_clf.fit(x_train, y_train)\n",
    "\n",
    "# compare single models to ensemble voting model\n",
    "# add models here\n",
    "with MyTimer(): \n",
    "    for clf in (rf_clf, gb_clf, voting_clf):\n",
    "        #if (clf.__class__.__name__ == \"RandomForestClassifier\"):\n",
    "        #    print(\"found rf\")\n",
    "        #    clf.fit(x_train, y_train, sample_weight=np.where(y_train == 1, wt1, wt0).flatten())\n",
    "        #else:\n",
    "        #    print(\"not rf\")\n",
    "        #    clf.fit(x_train, y_train)\n",
    "        clf.fit(x_train, y_train)\n",
    "        y_pred = clf.predict(x_test)\n",
    "        print(clf.__class__.__name__, accuracy_score(y_test, y_pred))\n",
    "        print(roc_auc_score(y_test, y_pred))\n",
    "        print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score is good for RF only, and ensemble is much worse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The function took 61.94969034194946 seconds to complete\n",
      "GradientBoostingClassifier 0.9976651100733822\n",
      "0.9020588328184397\n",
      "[[56750   114]\n",
      " [   19    79]]\n",
      "XGBClassifier 0.9992802219023208\n",
      "0.902867780483043\n",
      "[[56842    22]\n",
      " [   19    79]]\n",
      "VotingClassifier 0.9984375548611355\n",
      "0.9024457208319457\n",
      "[[56794    70]\n",
      " [   19    79]]\n",
      "The function took 151.5291690826416 seconds to complete\n"
     ]
    }
   ],
   "source": [
    "# add models here\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('gbc', gbc_clf), ('xgb', xgb_clf)], voting='soft')\n",
    "\n",
    "with MyTimer(): \n",
    "    voting_clf.fit(x_train, y_train)\n",
    "\n",
    "# compare single models to ensemble voting model\n",
    "# add models here\n",
    "with MyTimer(): \n",
    "    for clf in (gbc_clf, xgb_clf, voting_clf):\n",
    "        clf.fit(x_train, y_train)\n",
    "        y_pred = clf.predict(x_test)\n",
    "        print(clf.__class__.__name__, accuracy_score(y_test, y_pred))\n",
    "        print(roc_auc_score(y_test, y_pred))\n",
    "        print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score is okay for xgb, and ensemble is no better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The function took 2.382884979248047 seconds to complete\n",
      "LGBMClassifier 0.9993504441557529\n",
      "0.892716456306777\n",
      "[[56848    16]\n",
      " [   21    77]]\n",
      "GaussianNB 0.9932762192338752\n",
      "0.8183686389581156\n",
      "[[56516   348]\n",
      " [   35    63]]\n",
      "VotingClassifier 0.9959973315543695\n",
      "0.8706640189840709\n",
      "[[56661   203]\n",
      " [   25    73]]\n",
      "The function took 5.700852870941162 seconds to complete\n"
     ]
    }
   ],
   "source": [
    "# add models here\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lgb', lgb_clf), ('gnb', gnb_clf)], voting='soft')\n",
    "\n",
    "with MyTimer(): \n",
    "    voting_clf.fit(x_train, y_train)\n",
    "\n",
    "# compare single models to ensemble voting model\n",
    "# add models here\n",
    "with MyTimer(): \n",
    "    for clf in (lgb_clf, gnb_clf, voting_clf):\n",
    "        clf.fit(x_train, y_train)\n",
    "        y_pred = clf.predict(x_test)\n",
    "        print(clf.__class__.__name__, accuracy_score(y_test, y_pred))\n",
    "        print(roc_auc_score(y_test, y_pred))\n",
    "        print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score is bad for all, and ensemble is a little better -- gnb is the only model to predict so many 1's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The function took 334.1916048526764 seconds to complete\n",
      "XGBClassifier 0.9992802219023208\n",
      "0.902867780483043\n",
      "[[56842    22]\n",
      " [   19    79]]\n",
      "RandomForestClassifier 0.999420666409185\n",
      "0.913124619572083\n",
      "[[56848    16]\n",
      " [   17    81]]\n",
      "VotingClassifier 0.9993679997191109\n",
      "0.9130982408438896\n",
      "[[56845    19]\n",
      " [   17    81]]\n",
      "The function took 668.3260383605957 seconds to complete\n"
     ]
    }
   ],
   "source": [
    "# add models here\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('xgb', xgb_clf), ('rf', rf_clf)], voting='soft')\n",
    "    #estimators=[('lr', lr_clf), ('svc', svm_clf), ('rf', rf_clf), ('gb', gb_clf)], voting='soft')\n",
    "\n",
    "with MyTimer(): \n",
    "    voting_clf.fit(x_train, y_train)\n",
    "\n",
    "# compare single models to ensemble voting model\n",
    "# add models here\n",
    "with MyTimer(): \n",
    "    for clf in (xgb_clf, rf_clf, voting_clf):\n",
    "        #clf.fit(x_train, y_train, sample_weight=sw, preprocessing=preprocess_cases)\n",
    "        clf.fit(x_train, y_train)\n",
    "        #clf.fit(x_train, y_train)\n",
    "        y_pred = clf.predict(x_test)\n",
    "        print(clf.__class__.__name__, accuracy_score(y_test, y_pred))\n",
    "        print(roc_auc_score(y_test, y_pred))\n",
    "        print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "voila, score is good for both individually and slightly better for the ensemble (1 fewer wrong predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The function took 0.0 seconds to complete\n",
      "The function took 368.059366941452 seconds to complete\n",
      "XGBClassifier 0.9991397773954567\n",
      "0.9180771809286461\n",
      "[[56831    33]\n",
      " [   16    82]]\n",
      "RandomForestClassifier 0.9995435553526912\n",
      "0.9182794178447969\n",
      "[[56854    10]\n",
      " [   16    82]]\n",
      "VotingClassifier 0.9993504441557529\n",
      "0.9130894479344918\n",
      "[[56844    20]\n",
      " [   17    81]]\n",
      "The function took 637.7981021404266 seconds to complete\n"
     ]
    }
   ],
   "source": [
    "with MyTimer(): \n",
    "    sw = np.where(y_train == 1, 4, 1).flatten()\n",
    "\n",
    "# add models here\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('xgb', xgb_clf), ('rf', rf_clf)], voting='soft')\n",
    "    #estimators=[('lr', lr_clf), ('svc', svm_clf), ('rf', rf_clf), ('gb', gb_clf)], voting='soft')\n",
    "\n",
    "with MyTimer(): \n",
    "    voting_clf.fit(x_train, y_train)\n",
    "\n",
    "# compare single models to ensemble voting model\n",
    "# add models here\n",
    "with MyTimer(): \n",
    "    for clf in (xgb_clf, rf_clf, voting_clf):\n",
    "        #clf.fit(x_train, y_train, sample_weight=sw, preprocessing=preprocess_cases)\n",
    "        clf.fit(x_train, y_train, sample_weight=sw)\n",
    "        #clf.fit(x_train, y_train)\n",
    "        y_pred = clf.predict(x_test)\n",
    "        print(clf.__class__.__name__, accuracy_score(y_test, y_pred))\n",
    "        print(roc_auc_score(y_test, y_pred))\n",
    "        print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "weights have an effect on XGB, causing more FP than without "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The function took 0.0 seconds to complete\n",
      "The function took 323.5075113773346 seconds to complete\n",
      "XGBClassifier 0.9993153330290369\n",
      "0.8876056225810527\n",
      "[[56847    17]\n",
      " [   22    76]]\n",
      "RandomForestClassifier 0.999420666409185\n",
      "0.913124619572083\n",
      "[[56848    16]\n",
      " [   17    81]]\n",
      "VotingClassifier 0.9993153330290369\n",
      "0.8926988704879814\n",
      "[[56846    18]\n",
      " [   21    77]]\n",
      "The function took 686.2084558010101 seconds to complete\n"
     ]
    }
   ],
   "source": [
    "with MyTimer(): \n",
    "    sw = np.where(y_train == 1, 1, 4).flatten()\n",
    "\n",
    "# add models here\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('xgb', xgb_clf), ('rf', rf_clf)], voting='soft')\n",
    "    #estimators=[('lr', lr_clf), ('svc', svm_clf), ('rf', rf_clf), ('gb', gb_clf)], voting='soft')\n",
    "\n",
    "with MyTimer(): \n",
    "    voting_clf.fit(x_train, y_train)\n",
    "\n",
    "# compare single models to ensemble voting model\n",
    "# add models here\n",
    "with MyTimer(): \n",
    "    for clf in (xgb_clf, rf_clf, voting_clf):\n",
    "        #clf.fit(x_train, y_train, sample_weight=sw, preprocessing=preprocess_cases)\n",
    "        clf.fit(x_train, y_train, sample_weight=sw)\n",
    "        #clf.fit(x_train, y_train)\n",
    "        y_pred = clf.predict(x_test)\n",
    "        print(clf.__class__.__name__, accuracy_score(y_test, y_pred))\n",
    "        print(roc_auc_score(y_test, y_pred))\n",
    "        print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The function took 355.15858721733093 seconds to complete\n",
      "XGBClassifier 0.9992802219023208\n",
      "0.902867780483043\n",
      "[[56842    22]\n",
      " [   19    79]]\n",
      "RandomForestClassifier 0.999420666409185\n",
      "0.9182178674790119\n",
      "[[56847    17]\n",
      " [   16    82]]\n",
      "VotingClassifier 0.999420666409185\n",
      "0.9029381237582259\n",
      "[[56850    14]\n",
      " [   19    79]]\n",
      "The function took 699.4767827987671 seconds to complete\n"
     ]
    }
   ],
   "source": [
    "# add models here\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('xgb', xgb_clf), ('rf', rf_clf)], voting='hard')\n",
    "    #estimators=[('lr', lr_clf), ('svc', svm_clf), ('rf', rf_clf), ('gb', gb_clf)], voting='soft')\n",
    "\n",
    "with MyTimer(): \n",
    "    voting_clf.fit(x_train, y_train)\n",
    "\n",
    "# compare single models to ensemble voting model\n",
    "# add models here\n",
    "with MyTimer(): \n",
    "    for clf in (xgb_clf, rf_clf, voting_clf):\n",
    "        #clf.fit(x_train, y_train, sample_weight=sw, preprocessing=preprocess_cases)\n",
    "        clf.fit(x_train, y_train)\n",
    "        #clf.fit(x_train, y_train)\n",
    "        y_pred = clf.predict(x_test)\n",
    "        print(clf.__class__.__name__, accuracy_score(y_test, y_pred))\n",
    "        print(roc_auc_score(y_test, y_pred))\n",
    "        print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "soft voting gives better results here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "try 2 more sets of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The function took 13.398539304733276 seconds to complete\n",
      "DecisionTreeClassifier 0.9977704434535304\n",
      "0.9173913339956129\n",
      "[[56753   111]\n",
      " [   16    82]]\n",
      "AdaBoostClassifier 0.9992451107756047\n",
      "0.907943442571176\n",
      "[[56839    25]\n",
      " [   18    80]]\n",
      "VotingClassifier 0.9977353323268143\n",
      "0.9173737481768172\n",
      "[[56751   113]\n",
      " [   16    82]]\n",
      "The function took 25.79205322265625 seconds to complete\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# add models here\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('dt', dt_clf), ('ab', ab_clf)], voting='soft')\n",
    "    #estimators=[('lr', lr_clf), ('svc', svm_clf), ('rf', rf_clf), ('gb', gb_clf)], voting='soft')\n",
    "\n",
    "with MyTimer(): \n",
    "    voting_clf.fit(x_train, y_train)\n",
    "\n",
    "# compare single models to ensemble voting model\n",
    "# add models here\n",
    "with MyTimer(): \n",
    "    for clf in (dt_clf, ab_clf, voting_clf):\n",
    "        #clf.fit(x_train, y_train, sample_weight=sw, preprocessing=preprocess_cases)\n",
    "        clf.fit(x_train, y_train)\n",
    "        #clf.fit(x_train, y_train)\n",
    "        y_pred = clf.predict(x_test)\n",
    "        print(clf.__class__.__name__, accuracy_score(y_test, y_pred))\n",
    "        print(roc_auc_score(y_test, y_pred))\n",
    "        print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The function took 19.32780933380127 seconds to complete\n",
      "BaggingClassifier 0.9993504441557529\n",
      "0.9181826958414203\n",
      "[[56843    21]\n",
      " [   16    82]]\n",
      "ExtraTreesClassifier 0.9995611109160493\n",
      "0.9081017149403374\n",
      "[[56857     7]\n",
      " [   18    80]]\n",
      "VotingClassifier 0.9994557775359011\n",
      "0.8978624616700929\n",
      "[[56853    11]\n",
      " [   20    78]]\n",
      "The function took 37.19050312042236 seconds to complete\n"
     ]
    }
   ],
   "source": [
    "# add models here\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('bc', bc_clf), ('et',et_clf)], voting='soft')\n",
    "    #estimators=[('lr', lr_clf), ('svc', svm_clf), ('rf', rf_clf), ('gb', gb_clf)], voting='soft')\n",
    "\n",
    "with MyTimer(): \n",
    "    voting_clf.fit(x_train, y_train)\n",
    "\n",
    "# compare single models to ensemble voting model\n",
    "# add models here\n",
    "with MyTimer(): \n",
    "    for clf in (bc_clf, et_clf, voting_clf):\n",
    "        #clf.fit(x_train, y_train, sample_weight=sw, preprocessing=preprocess_cases)\n",
    "        clf.fit(x_train, y_train)\n",
    "        #clf.fit(x_train, y_train)\n",
    "        y_pred = clf.predict(x_test)\n",
    "        print(clf.__class__.__name__, accuracy_score(y_test, y_pred))\n",
    "        print(roc_auc_score(y_test, y_pred))\n",
    "        print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bc and et have good results, combine them with xgb and rf so we have 4 models to use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The function took 27.85802388191223 seconds to complete\n",
      "XGBClassifier 0.9992802219023208\n",
      "0.902867780483043\n",
      "[[56842    22]\n",
      " [   19    79]]\n",
      "BaggingClassifier 0.9992099996488887\n",
      "0.9130191046593088\n",
      "[[56836    28]\n",
      " [   17    81]]\n",
      "ExtraTreesClassifier 0.9994557775359011\n",
      "0.9029557095770216\n",
      "[[56852    12]\n",
      " [   19    79]]\n",
      "VotingClassifier 0.9993679997191109\n",
      "0.9130982408438896\n",
      "[[56845    19]\n",
      " [   17    81]]\n",
      "The function took 59.76839470863342 seconds to complete\n"
     ]
    }
   ],
   "source": [
    "# add models here\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('xgb', xgb_clf), ('bc', bc_clf), ('et',et_clf)], voting='soft')\n",
    "    #estimators=[('lr', lr_clf), ('svc', svm_clf), ('rf', rf_clf), ('gb', gb_clf)], voting='soft')\n",
    "\n",
    "with MyTimer(): \n",
    "    voting_clf.fit(x_train, y_train)\n",
    "\n",
    "# compare single models to ensemble voting model\n",
    "# add models here\n",
    "with MyTimer(): \n",
    "    for clf in (xgb_clf, bc_clf, et_clf, voting_clf):\n",
    "        #clf.fit(x_train, y_train, sample_weight=sw, preprocessing=preprocess_cases)\n",
    "        clf.fit(x_train, y_train)\n",
    "        #clf.fit(x_train, y_train)\n",
    "        y_pred = clf.predict(x_test)\n",
    "        print(clf.__class__.__name__, accuracy_score(y_test, y_pred))\n",
    "        print(roc_auc_score(y_test, y_pred))\n",
    "        print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The function took 361.6446259021759 seconds to complete\n",
      "RandomForestClassifier 0.999403110845827\n",
      "0.9131158266626853\n",
      "[[56847    17]\n",
      " [   17    81]]\n",
      "XGBClassifier 0.9992802219023208\n",
      "0.902867780483043\n",
      "[[56842    22]\n",
      " [   19    79]]\n",
      "BaggingClassifier 0.999385555282469\n",
      "0.908013785846359\n",
      "[[56847    17]\n",
      " [   18    80]]\n",
      "ExtraTreesClassifier 0.9994382219725431\n",
      "0.8927604208537663\n",
      "[[56853    11]\n",
      " [   21    77]]\n",
      "VotingClassifier 0.999385555282469\n",
      "0.9131070337532875\n",
      "[[56846    18]\n",
      " [   17    81]]\n",
      "The function took 690.471239566803 seconds to complete\n"
     ]
    }
   ],
   "source": [
    "# add models here\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('rf', rf_clf), ('xgb', xgb_clf), ('bc', bc_clf), ('et', et_clf)], voting='soft')\n",
    "    #estimators=[('lr', lr_clf), ('svc', svm_clf), ('rf', rf_clf), ('gb', gb_clf)], voting='soft')\n",
    "\n",
    "with MyTimer(): \n",
    "    voting_clf.fit(x_train, y_train)\n",
    "\n",
    "# compare single models to ensemble voting model\n",
    "# add models here\n",
    "with MyTimer(): \n",
    "    for clf in (rf_clf, xgb_clf, bc_clf, et_clf, voting_clf):\n",
    "        #clf.fit(x_train, y_train, sample_weight=sw, preprocessing=preprocess_cases)\n",
    "        clf.fit(x_train, y_train)\n",
    "        #clf.fit(x_train, y_train)\n",
    "        y_pred = clf.predict(x_test)\n",
    "        print(clf.__class__.__name__, accuracy_score(y_test, y_pred))\n",
    "        print(roc_auc_score(y_test, y_pred))\n",
    "        print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The function took 330.05251955986023 seconds to complete\n",
      "RandomForestClassifier 0.9994382219725431\n",
      "0.9182266603884097\n",
      "[[56848    16]\n",
      " [   16    82]]\n",
      "XGBClassifier 0.9992802219023208\n",
      "0.902867780483043\n",
      "[[56842    22]\n",
      " [   19    79]]\n",
      "BaggingClassifier 0.9992099996488887\n",
      "0.8977393609385229\n",
      "[[56839    25]\n",
      " [   20    78]]\n",
      "ExtraTreesClassifier 0.9994382219725431\n",
      "0.8978536687606951\n",
      "[[56852    12]\n",
      " [   20    78]]\n",
      "VotingClassifier 0.999385555282469\n",
      "0.9029205379394302\n",
      "[[56848    16]\n",
      " [   19    79]]\n",
      "The function took 692.7915947437286 seconds to complete\n"
     ]
    }
   ],
   "source": [
    "# add models here\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('rf', rf_clf), ('xgb', xgb_clf), ('bc', bc_clf), ('et', et_clf)], voting='hard')\n",
    "    #estimators=[('lr', lr_clf), ('svc', svm_clf), ('rf', rf_clf), ('gb', gb_clf)], voting='soft')\n",
    "\n",
    "with MyTimer(): \n",
    "    voting_clf.fit(x_train, y_train)\n",
    "\n",
    "# compare single models to ensemble voting model\n",
    "# add models here\n",
    "with MyTimer(): \n",
    "    for clf in (rf_clf, xgb_clf, bc_clf, et_clf, voting_clf):\n",
    "        #clf.fit(x_train, y_train, sample_weight=sw, preprocessing=preprocess_cases)\n",
    "        clf.fit(x_train, y_train)\n",
    "        #clf.fit(x_train, y_train)\n",
    "        y_pred = clf.predict(x_test)\n",
    "        print(clf.__class__.__name__, accuracy_score(y_test, y_pred))\n",
    "        print(roc_auc_score(y_test, y_pred))\n",
    "        print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "good improvement, 3 fewer errors by using voting..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The function took 292.0507206916809 seconds to complete\n",
      "RandomForestClassifier 0.9994382219725431\n",
      "0.9131334124814809\n",
      "[[56849    15]\n",
      " [   17    81]]\n",
      "ExtraTreesClassifier 0.9994557775359011\n",
      "0.8978624616700929\n",
      "[[56853    11]\n",
      " [   20    78]]\n",
      "VotingClassifier 0.9995084442259752\n",
      "0.9131685841190725\n",
      "[[56853    11]\n",
      " [   17    81]]\n",
      "The function took 583.407958984375 seconds to complete\n"
     ]
    }
   ],
   "source": [
    "# add models here\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('rf', rf_clf), ('et', et_clf)], voting='soft')\n",
    "    #estimators=[('lr', lr_clf), ('svc', svm_clf), ('rf', rf_clf), ('gb', gb_clf)], voting='soft')\n",
    "\n",
    "with MyTimer(): \n",
    "    voting_clf.fit(x_train, y_train)\n",
    "\n",
    "# compare single models to ensemble voting model\n",
    "# add models here\n",
    "with MyTimer(): \n",
    "    for clf in (rf_clf, et_clf, voting_clf):\n",
    "        #clf.fit(x_train, y_train, sample_weight=sw, preprocessing=preprocess_cases)\n",
    "        clf.fit(x_train, y_train)\n",
    "        #clf.fit(x_train, y_train)\n",
    "        y_pred = clf.predict(x_test)\n",
    "        print(clf.__class__.__name__, accuracy_score(y_test, y_pred))\n",
    "        print(roc_auc_score(y_test, y_pred))\n",
    "        print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The function took 282.4328303337097 seconds to complete\n",
      "RandomForestClassifier 0.999403110845827\n",
      "0.9131158266626853\n",
      "[[56847    17]\n",
      " [   17    81]]\n",
      "ExtraTreesClassifier 0.9994382219725431\n",
      "0.8978536687606951\n",
      "[[56852    12]\n",
      " [   20    78]]\n",
      "VotingClassifier 0.9994382219725431\n",
      "0.8927604208537663\n",
      "[[56853    11]\n",
      " [   21    77]]\n",
      "The function took 699.7399299144745 seconds to complete\n"
     ]
    }
   ],
   "source": [
    "# add models here\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('rf', rf_clf), ('et', et_clf)], voting='hard')\n",
    "    #estimators=[('lr', lr_clf), ('svc', svm_clf), ('rf', rf_clf), ('gb', gb_clf)], voting='soft')\n",
    "\n",
    "with MyTimer(): \n",
    "    voting_clf.fit(x_train, y_train)\n",
    "\n",
    "# compare single models to ensemble voting model\n",
    "# add models here\n",
    "with MyTimer(): \n",
    "    for clf in (rf_clf, et_clf, voting_clf):\n",
    "        #clf.fit(x_train, y_train, sample_weight=sw, preprocessing=preprocess_cases)\n",
    "        clf.fit(x_train, y_train)\n",
    "        #clf.fit(x_train, y_train)\n",
    "        y_pred = clf.predict(x_test)\n",
    "        print(clf.__class__.__name__, accuracy_score(y_test, y_pred))\n",
    "        print(roc_auc_score(y_test, y_pred))\n",
    "        print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start:  2020-07-06 12:40:17.382809\n",
      "End:  2020-07-06 15:27:24.600744\n"
     ]
    }
   ],
   "source": [
    "print(\"Start: \", StartTime)\n",
    "print(\"End: \", datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
