{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas_profiling as pp\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from matplotlib import pyplot\n",
    "import zipfile\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import MaxPool1D\n",
    "from keras.layers import Flatten\n",
    "#from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "colab = os.environ.get('COLAB_GPU', '10')\n",
    "if (int(colab) == 0):\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    #Path=\"drive/My Drive/Colab Notebooks/StockAnalysis\"\n",
    "    Path=\"\" # ???\n",
    "    DataPath=\"/content\"\n",
    "    RootPath=\"/root\"    \n",
    "else:\n",
    "    #Path=\"/c/DataScience/Repo/Imbalanced_data\"\n",
    "    Path=\"/DataScience/Repo/Imbalanced_data\"\n",
    "    DataPath=Path\n",
    "    RootPath=\"/Users/iowahawk89\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Data Set\n",
    "#df = pd.read_csv('creditcard.csv')\n",
    "df = pd.read_csv('https://storage.googleapis.com/download.tensorflow.org/data/creditcard.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.loc[:, df.columns != 'Class']\n",
    "y = df.loc[:, df.columns == 'Class']\n",
    "X_train, X_test1, y_train, y_test1 = train_test_split(X,y, test_size = 0.4, random_state = 42)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test1,y_test1, test_size = 0.4, random_state = 42)\n",
    "class_names=[0,1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downsampling\n",
    "yes = len(y_train[y_train['Class'] ==1]) \n",
    "yes_ind = y_train[y_train['Class'] == 1].index\n",
    "no_ind = y_train[y_train['Class'] == 0].index\n",
    "new_no_ind = np.random.choice(no_ind, yes, replace = False)\n",
    "undersample_ind = np.concatenate([new_no_ind, yes_ind])\n",
    "X_train = X_train.loc[undersample_ind]\n",
    "y_train = y_train.loc[undersample_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_metrics(model_name, train_features, test_features, train_label, test_label, pred):\n",
    "    try:\n",
    "        print(model_name.score(test_features, test_label)) \n",
    "        print(\"Accuracy score (training): {0:.3f}\".format(model_name.score(train_features, train_label))) \n",
    "        print(\"Accuracy score (validation): {0:.3f}\".format(model_name.score(test_features, test_label))) \n",
    "    except Exception as e:\n",
    "        print(\"error\")  \n",
    "    try:\n",
    "        print(pd.Series(model_name.feature_importances_, index=train_features.columns[:]).nlargest(10).plot(kind='barh')) \n",
    "    except Exception as e:\n",
    "        print(\"error\")  \n",
    "\n",
    "    print(\"Confusion Matrix:\")\n",
    "    tn, fp, fn, tp = confusion_matrix(test_label, pred).ravel()\n",
    "    total = tn+ fp+ fn+ tp \n",
    "    print(\"false positive pct:\",(fp/total)*100) \n",
    "    print(\"tn\", \" fp\", \" fn\", \" tp\") \n",
    "    print(tn, fp, fn, tp) \n",
    "    print(confusion_matrix(test_label, pred)) \n",
    "    print(\"Classification Report\") \n",
    "    print(classification_report(test_label, pred))\n",
    "    print(\"Specificity =\", tn/(tn+fp))\n",
    "    print(\"Sensitivity =\", tp/(tp+fn))\n",
    "    return tn, fp, fn, tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time_Series</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>72290</th>\n",
       "      <td>54644.0</td>\n",
       "      <td>-0.552402</td>\n",
       "      <td>0.604515</td>\n",
       "      <td>2.444300</td>\n",
       "      <td>0.250465</td>\n",
       "      <td>-0.632696</td>\n",
       "      <td>0.121463</td>\n",
       "      <td>-0.008882</td>\n",
       "      <td>0.219440</td>\n",
       "      <td>0.351241</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.099959</td>\n",
       "      <td>0.052644</td>\n",
       "      <td>0.309141</td>\n",
       "      <td>-0.058075</td>\n",
       "      <td>0.434920</td>\n",
       "      <td>-0.334822</td>\n",
       "      <td>0.271962</td>\n",
       "      <td>-0.043964</td>\n",
       "      <td>0.062385</td>\n",
       "      <td>9.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212990</th>\n",
       "      <td>139068.0</td>\n",
       "      <td>-1.108267</td>\n",
       "      <td>2.027164</td>\n",
       "      <td>-1.297249</td>\n",
       "      <td>-0.248834</td>\n",
       "      <td>0.543001</td>\n",
       "      <td>-1.792010</td>\n",
       "      <td>0.544730</td>\n",
       "      <td>0.535613</td>\n",
       "      <td>-0.563645</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.355535</td>\n",
       "      <td>0.275368</td>\n",
       "      <td>0.551664</td>\n",
       "      <td>-0.363162</td>\n",
       "      <td>-0.185057</td>\n",
       "      <td>0.153307</td>\n",
       "      <td>-0.152869</td>\n",
       "      <td>-0.215549</td>\n",
       "      <td>0.068467</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52084</th>\n",
       "      <td>45264.0</td>\n",
       "      <td>1.255760</td>\n",
       "      <td>-0.144373</td>\n",
       "      <td>-1.008024</td>\n",
       "      <td>-0.528532</td>\n",
       "      <td>1.933700</td>\n",
       "      <td>3.338554</td>\n",
       "      <td>-0.572602</td>\n",
       "      <td>0.833685</td>\n",
       "      <td>0.009527</td>\n",
       "      <td>...</td>\n",
       "      <td>0.072650</td>\n",
       "      <td>-0.347213</td>\n",
       "      <td>-1.230347</td>\n",
       "      <td>0.124469</td>\n",
       "      <td>0.983740</td>\n",
       "      <td>0.279849</td>\n",
       "      <td>0.046523</td>\n",
       "      <td>-0.027521</td>\n",
       "      <td>0.019498</td>\n",
       "      <td>28.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23217</th>\n",
       "      <td>32641.0</td>\n",
       "      <td>-0.677805</td>\n",
       "      <td>0.825292</td>\n",
       "      <td>2.449632</td>\n",
       "      <td>1.378949</td>\n",
       "      <td>-0.006691</td>\n",
       "      <td>0.683210</td>\n",
       "      <td>0.307176</td>\n",
       "      <td>0.357547</td>\n",
       "      <td>-0.709962</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.084112</td>\n",
       "      <td>0.084616</td>\n",
       "      <td>0.246048</td>\n",
       "      <td>-0.071007</td>\n",
       "      <td>-0.017692</td>\n",
       "      <td>-0.331228</td>\n",
       "      <td>-0.273236</td>\n",
       "      <td>0.134634</td>\n",
       "      <td>0.146676</td>\n",
       "      <td>37.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19342</th>\n",
       "      <td>30199.0</td>\n",
       "      <td>1.103256</td>\n",
       "      <td>0.211745</td>\n",
       "      <td>0.412680</td>\n",
       "      <td>1.120798</td>\n",
       "      <td>-0.220757</td>\n",
       "      <td>-0.342137</td>\n",
       "      <td>0.058428</td>\n",
       "      <td>0.142478</td>\n",
       "      <td>-0.206612</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.250141</td>\n",
       "      <td>-0.390954</td>\n",
       "      <td>-1.295671</td>\n",
       "      <td>0.276683</td>\n",
       "      <td>0.105581</td>\n",
       "      <td>0.059542</td>\n",
       "      <td>-0.833215</td>\n",
       "      <td>0.010379</td>\n",
       "      <td>0.016976</td>\n",
       "      <td>19.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42674</th>\n",
       "      <td>41194.0</td>\n",
       "      <td>-7.896886</td>\n",
       "      <td>5.381020</td>\n",
       "      <td>-8.451162</td>\n",
       "      <td>7.963928</td>\n",
       "      <td>-7.862419</td>\n",
       "      <td>-2.376820</td>\n",
       "      <td>-11.949723</td>\n",
       "      <td>5.051356</td>\n",
       "      <td>-6.912076</td>\n",
       "      <td>...</td>\n",
       "      <td>0.645974</td>\n",
       "      <td>2.557944</td>\n",
       "      <td>0.926278</td>\n",
       "      <td>0.032795</td>\n",
       "      <td>0.638073</td>\n",
       "      <td>0.361887</td>\n",
       "      <td>0.444577</td>\n",
       "      <td>1.101923</td>\n",
       "      <td>0.205958</td>\n",
       "      <td>1.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33276</th>\n",
       "      <td>37167.0</td>\n",
       "      <td>-7.923891</td>\n",
       "      <td>-5.198360</td>\n",
       "      <td>-3.000024</td>\n",
       "      <td>4.420666</td>\n",
       "      <td>2.272194</td>\n",
       "      <td>-3.394483</td>\n",
       "      <td>-5.283435</td>\n",
       "      <td>0.131619</td>\n",
       "      <td>0.658176</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.169811</td>\n",
       "      <td>-0.734308</td>\n",
       "      <td>-0.599926</td>\n",
       "      <td>-4.908301</td>\n",
       "      <td>0.410170</td>\n",
       "      <td>-1.167660</td>\n",
       "      <td>0.520508</td>\n",
       "      <td>1.937421</td>\n",
       "      <td>-1.552593</td>\n",
       "      <td>12.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182992</th>\n",
       "      <td>125612.0</td>\n",
       "      <td>1.889618</td>\n",
       "      <td>1.073099</td>\n",
       "      <td>-1.678018</td>\n",
       "      <td>4.173268</td>\n",
       "      <td>1.015516</td>\n",
       "      <td>-0.009389</td>\n",
       "      <td>-0.079706</td>\n",
       "      <td>0.064071</td>\n",
       "      <td>-0.714517</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.153570</td>\n",
       "      <td>0.203728</td>\n",
       "      <td>0.733796</td>\n",
       "      <td>-0.036560</td>\n",
       "      <td>0.334306</td>\n",
       "      <td>0.147171</td>\n",
       "      <td>0.279556</td>\n",
       "      <td>0.031669</td>\n",
       "      <td>0.035883</td>\n",
       "      <td>3.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154697</th>\n",
       "      <td>102625.0</td>\n",
       "      <td>-4.221221</td>\n",
       "      <td>2.871121</td>\n",
       "      <td>-5.888716</td>\n",
       "      <td>6.890952</td>\n",
       "      <td>-3.404894</td>\n",
       "      <td>-1.154394</td>\n",
       "      <td>-7.739928</td>\n",
       "      <td>2.851363</td>\n",
       "      <td>-2.507569</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.227882</td>\n",
       "      <td>1.620591</td>\n",
       "      <td>1.567947</td>\n",
       "      <td>-0.578007</td>\n",
       "      <td>-0.059045</td>\n",
       "      <td>-1.829169</td>\n",
       "      <td>-0.072429</td>\n",
       "      <td>0.136734</td>\n",
       "      <td>-0.599848</td>\n",
       "      <td>7.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226814</th>\n",
       "      <td>144808.0</td>\n",
       "      <td>-2.405207</td>\n",
       "      <td>2.943823</td>\n",
       "      <td>-7.616654</td>\n",
       "      <td>3.533374</td>\n",
       "      <td>-5.417494</td>\n",
       "      <td>-0.112632</td>\n",
       "      <td>-1.329372</td>\n",
       "      <td>1.709417</td>\n",
       "      <td>-2.322716</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.338707</td>\n",
       "      <td>0.652683</td>\n",
       "      <td>0.414132</td>\n",
       "      <td>0.023869</td>\n",
       "      <td>-0.260616</td>\n",
       "      <td>0.405316</td>\n",
       "      <td>0.029107</td>\n",
       "      <td>0.519807</td>\n",
       "      <td>-0.469537</td>\n",
       "      <td>667.55</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>602 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Time_Series        V1        V2        V3        V4        V5  \\\n",
       "72290       54644.0 -0.552402  0.604515  2.444300  0.250465 -0.632696   \n",
       "212990     139068.0 -1.108267  2.027164 -1.297249 -0.248834  0.543001   \n",
       "52084       45264.0  1.255760 -0.144373 -1.008024 -0.528532  1.933700   \n",
       "23217       32641.0 -0.677805  0.825292  2.449632  1.378949 -0.006691   \n",
       "19342       30199.0  1.103256  0.211745  0.412680  1.120798 -0.220757   \n",
       "...             ...       ...       ...       ...       ...       ...   \n",
       "42674       41194.0 -7.896886  5.381020 -8.451162  7.963928 -7.862419   \n",
       "33276       37167.0 -7.923891 -5.198360 -3.000024  4.420666  2.272194   \n",
       "182992     125612.0  1.889618  1.073099 -1.678018  4.173268  1.015516   \n",
       "154697     102625.0 -4.221221  2.871121 -5.888716  6.890952 -3.404894   \n",
       "226814     144808.0 -2.405207  2.943823 -7.616654  3.533374 -5.417494   \n",
       "\n",
       "              V6         V7        V8        V9  ...       V20       V21  \\\n",
       "72290   0.121463  -0.008882  0.219440  0.351241  ... -0.099959  0.052644   \n",
       "212990 -1.792010   0.544730  0.535613 -0.563645  ... -0.355535  0.275368   \n",
       "52084   3.338554  -0.572602  0.833685  0.009527  ...  0.072650 -0.347213   \n",
       "23217   0.683210   0.307176  0.357547 -0.709962  ... -0.084112  0.084616   \n",
       "19342  -0.342137   0.058428  0.142478 -0.206612  ... -0.250141 -0.390954   \n",
       "...          ...        ...       ...       ...  ...       ...       ...   \n",
       "42674  -2.376820 -11.949723  5.051356 -6.912076  ...  0.645974  2.557944   \n",
       "33276  -3.394483  -5.283435  0.131619  0.658176  ... -2.169811 -0.734308   \n",
       "182992 -0.009389  -0.079706  0.064071 -0.714517  ... -0.153570  0.203728   \n",
       "154697 -1.154394  -7.739928  2.851363 -2.507569  ... -0.227882  1.620591   \n",
       "226814 -0.112632  -1.329372  1.709417 -2.322716  ... -0.338707  0.652683   \n",
       "\n",
       "             V22       V23       V24       V25       V26       V27       V28  \\\n",
       "72290   0.309141 -0.058075  0.434920 -0.334822  0.271962 -0.043964  0.062385   \n",
       "212990  0.551664 -0.363162 -0.185057  0.153307 -0.152869 -0.215549  0.068467   \n",
       "52084  -1.230347  0.124469  0.983740  0.279849  0.046523 -0.027521  0.019498   \n",
       "23217   0.246048 -0.071007 -0.017692 -0.331228 -0.273236  0.134634  0.146676   \n",
       "19342  -1.295671  0.276683  0.105581  0.059542 -0.833215  0.010379  0.016976   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "42674   0.926278  0.032795  0.638073  0.361887  0.444577  1.101923  0.205958   \n",
       "33276  -0.599926 -4.908301  0.410170 -1.167660  0.520508  1.937421 -1.552593   \n",
       "182992  0.733796 -0.036560  0.334306  0.147171  0.279556  0.031669  0.035883   \n",
       "154697  1.567947 -0.578007 -0.059045 -1.829169 -0.072429  0.136734 -0.599848   \n",
       "226814  0.414132  0.023869 -0.260616  0.405316  0.029107  0.519807 -0.469537   \n",
       "\n",
       "        Amount  \n",
       "72290     9.95  \n",
       "212990    1.00  \n",
       "52084    28.62  \n",
       "23217    37.16  \n",
       "19342    19.27  \n",
       "...        ...  \n",
       "42674     1.52  \n",
       "33276    12.31  \n",
       "182992    3.22  \n",
       "154697    7.59  \n",
       "226814  667.55  \n",
       "\n",
       "[602 rows x 30 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows, ncols = X_train.shape # (602,30)\n",
    "X_train_arr = X_train.to_numpy()\n",
    "y_train_arr = y_train.to_numpy()\n",
    "X_train_arr = X_train_arr.reshape(nrows, ncols, 1)\n",
    "\n",
    "nrows, ncols = X_test.shape # (602,30)\n",
    "X_test_arr = X_test.to_numpy()\n",
    "y_test_arr = y_test.to_numpy()\n",
    "X_test_arr = X_test_arr.reshape(nrows, ncols, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 1)\n"
     ]
    }
   ],
   "source": [
    "input_shape = (ncols, 1)\n",
    "print(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.backend import sigmoid\n",
    "def swish(x, beta = 1):\n",
    "    return (x * sigmoid(beta * x))\n",
    "\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "from keras.layers import Activation\n",
    "get_custom_objects().update({'swish': Activation(swish)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a 1D Convolution, which is useful for non-image data, such as time series related data (audio, sensor)\n",
    "Be sure to remember to include an activation function in the initial Conv1D layer, otherwise the model will predict all minority case of 1's (FP rate is almost 100%) !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 21, 32)            352       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 21, 32)            128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 10, 32)            0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 10, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 9, 64)             4160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 9, 64)             256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 4, 64)             0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 21,409\n",
      "Trainable params: 21,217\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/200\n",
      "602/602 [==============================] - 1s 1ms/step - loss: 1.8125 - accuracy: 0.5565\n",
      "Epoch 2/200\n",
      "602/602 [==============================] - 0s 154us/step - loss: 1.5766 - accuracy: 0.5914\n",
      "Epoch 3/200\n",
      "602/602 [==============================] - 0s 157us/step - loss: 1.4203 - accuracy: 0.6329\n",
      "Epoch 4/200\n",
      "602/602 [==============================] - 0s 162us/step - loss: 1.3977 - accuracy: 0.6379\n",
      "Epoch 5/200\n",
      "602/602 [==============================] - 0s 149us/step - loss: 1.4011 - accuracy: 0.6412\n",
      "Epoch 6/200\n",
      "602/602 [==============================] - 0s 443us/step - loss: 1.2126 - accuracy: 0.6429\n",
      "Epoch 7/200\n",
      "602/602 [==============================] - 0s 227us/step - loss: 1.1114 - accuracy: 0.6811\n",
      "Epoch 8/200\n",
      "602/602 [==============================] - 0s 185us/step - loss: 1.2167 - accuracy: 0.6910\n",
      "Epoch 9/200\n",
      "602/602 [==============================] - 0s 188us/step - loss: 1.1156 - accuracy: 0.7110\n",
      "Epoch 10/200\n",
      "602/602 [==============================] - 0s 162us/step - loss: 1.2309 - accuracy: 0.6711\n",
      "Epoch 11/200\n",
      "602/602 [==============================] - 0s 219us/step - loss: 1.0424 - accuracy: 0.7226\n",
      "Epoch 12/200\n",
      "602/602 [==============================] - 0s 187us/step - loss: 1.0350 - accuracy: 0.7159\n",
      "Epoch 13/200\n",
      "602/602 [==============================] - 0s 167us/step - loss: 0.8756 - accuracy: 0.7608\n",
      "Epoch 14/200\n",
      "602/602 [==============================] - 0s 154us/step - loss: 0.9372 - accuracy: 0.7625\n",
      "Epoch 15/200\n",
      "602/602 [==============================] - 0s 169us/step - loss: 0.9024 - accuracy: 0.7824\n",
      "Epoch 16/200\n",
      "602/602 [==============================] - 0s 167us/step - loss: 0.9542 - accuracy: 0.7741\n",
      "Epoch 17/200\n",
      "602/602 [==============================] - 0s 159us/step - loss: 0.9224 - accuracy: 0.7708\n",
      "Epoch 18/200\n",
      "602/602 [==============================] - 0s 187us/step - loss: 0.9171 - accuracy: 0.7691\n",
      "Epoch 19/200\n",
      "602/602 [==============================] - 0s 449us/step - loss: 0.8682 - accuracy: 0.7774\n",
      "Epoch 20/200\n",
      "602/602 [==============================] - 0s 193us/step - loss: 0.8119 - accuracy: 0.7791\n",
      "Epoch 21/200\n",
      "602/602 [==============================] - 0s 169us/step - loss: 0.8337 - accuracy: 0.8140\n",
      "Epoch 22/200\n",
      "602/602 [==============================] - 0s 152us/step - loss: 0.8642 - accuracy: 0.7957\n",
      "Epoch 23/200\n",
      "602/602 [==============================] - 0s 156us/step - loss: 0.9427 - accuracy: 0.8007\n",
      "Epoch 24/200\n",
      "602/602 [==============================] - 0s 164us/step - loss: 0.8847 - accuracy: 0.7874\n",
      "Epoch 25/200\n",
      "602/602 [==============================] - 0s 172us/step - loss: 0.8070 - accuracy: 0.7874\n",
      "Epoch 26/200\n",
      "602/602 [==============================] - 0s 157us/step - loss: 0.8482 - accuracy: 0.7708\n",
      "Epoch 27/200\n",
      "602/602 [==============================] - 0s 161us/step - loss: 0.7666 - accuracy: 0.7990\n",
      "Epoch 28/200\n",
      "602/602 [==============================] - 0s 341us/step - loss: 0.7669 - accuracy: 0.82230s - loss: 0.7635 - accuracy: 0.81\n",
      "Epoch 29/200\n",
      "602/602 [==============================] - 0s 248us/step - loss: 0.7250 - accuracy: 0.8206\n",
      "Epoch 30/200\n",
      "602/602 [==============================] - 0s 174us/step - loss: 0.8361 - accuracy: 0.7791\n",
      "Epoch 31/200\n",
      "602/602 [==============================] - 0s 159us/step - loss: 0.7839 - accuracy: 0.7791\n",
      "Epoch 32/200\n",
      "602/602 [==============================] - 0s 152us/step - loss: 0.7687 - accuracy: 0.8023\n",
      "Epoch 33/200\n",
      "602/602 [==============================] - 0s 165us/step - loss: 0.8463 - accuracy: 0.7990\n",
      "Epoch 34/200\n",
      "602/602 [==============================] - 0s 156us/step - loss: 0.7887 - accuracy: 0.7973\n",
      "Epoch 35/200\n",
      "602/602 [==============================] - 0s 157us/step - loss: 0.8168 - accuracy: 0.7841\n",
      "Epoch 36/200\n",
      "602/602 [==============================] - 0s 157us/step - loss: 0.7242 - accuracy: 0.8007\n",
      "Epoch 37/200\n",
      "602/602 [==============================] - 0s 165us/step - loss: 0.7880 - accuracy: 0.7724\n",
      "Epoch 38/200\n",
      "602/602 [==============================] - 0s 156us/step - loss: 0.8036 - accuracy: 0.8056\n",
      "Epoch 39/200\n",
      "602/602 [==============================] - 0s 360us/step - loss: 0.7707 - accuracy: 0.8090\n",
      "Epoch 40/200\n",
      "602/602 [==============================] - 0s 234us/step - loss: 0.7027 - accuracy: 0.8339\n",
      "Epoch 41/200\n",
      "602/602 [==============================] - 0s 162us/step - loss: 0.7624 - accuracy: 0.8056\n",
      "Epoch 42/200\n",
      "602/602 [==============================] - 0s 161us/step - loss: 0.6743 - accuracy: 0.8056\n",
      "Epoch 43/200\n",
      "602/602 [==============================] - 0s 162us/step - loss: 0.6839 - accuracy: 0.8306\n",
      "Epoch 44/200\n",
      "602/602 [==============================] - 0s 156us/step - loss: 0.7668 - accuracy: 0.8007\n",
      "Epoch 45/200\n",
      "602/602 [==============================] - 0s 159us/step - loss: 0.7112 - accuracy: 0.8173\n",
      "Epoch 46/200\n",
      "602/602 [==============================] - 0s 169us/step - loss: 0.7494 - accuracy: 0.8140\n",
      "Epoch 47/200\n",
      "602/602 [==============================] - 0s 157us/step - loss: 0.6404 - accuracy: 0.8405\n",
      "Epoch 48/200\n",
      "602/602 [==============================] - 0s 162us/step - loss: 0.7336 - accuracy: 0.8306\n",
      "Epoch 49/200\n",
      "602/602 [==============================] - 0s 159us/step - loss: 0.6741 - accuracy: 0.8422\n",
      "Epoch 50/200\n",
      "602/602 [==============================] - 0s 159us/step - loss: 0.7290 - accuracy: 0.8223\n",
      "Epoch 51/200\n",
      "602/602 [==============================] - 0s 159us/step - loss: 0.7609 - accuracy: 0.8073\n",
      "Epoch 52/200\n",
      "602/602 [==============================] - 0s 161us/step - loss: 0.5965 - accuracy: 0.8571\n",
      "Epoch 53/200\n",
      "602/602 [==============================] - 0s 167us/step - loss: 0.5742 - accuracy: 0.8322\n",
      "Epoch 54/200\n",
      "602/602 [==============================] - 0s 154us/step - loss: 0.7106 - accuracy: 0.8389\n",
      "Epoch 55/200\n",
      "602/602 [==============================] - 0s 159us/step - loss: 0.6210 - accuracy: 0.8405\n",
      "Epoch 56/200\n",
      "602/602 [==============================] - 0s 164us/step - loss: 0.8062 - accuracy: 0.7957\n",
      "Epoch 57/200\n",
      "602/602 [==============================] - 0s 156us/step - loss: 0.6785 - accuracy: 0.8156\n",
      "Epoch 58/200\n",
      "602/602 [==============================] - 0s 174us/step - loss: 0.7060 - accuracy: 0.8272\n",
      "Epoch 59/200\n",
      "602/602 [==============================] - 0s 157us/step - loss: 0.5843 - accuracy: 0.8372\n",
      "Epoch 60/200\n",
      "602/602 [==============================] - 0s 157us/step - loss: 0.5718 - accuracy: 0.8272\n",
      "Epoch 61/200\n",
      "602/602 [==============================] - 0s 165us/step - loss: 0.6131 - accuracy: 0.8439\n",
      "Epoch 62/200\n",
      "602/602 [==============================] - 0s 161us/step - loss: 0.5638 - accuracy: 0.8654\n",
      "Epoch 63/200\n",
      "602/602 [==============================] - 0s 224us/step - loss: 0.6473 - accuracy: 0.8389\n",
      "Epoch 64/200\n",
      "602/602 [==============================] - 0s 370us/step - loss: 0.6633 - accuracy: 0.8355\n",
      "Epoch 65/200\n",
      "602/602 [==============================] - 0s 177us/step - loss: 0.6392 - accuracy: 0.8588\n",
      "Epoch 66/200\n",
      "602/602 [==============================] - 0s 169us/step - loss: 0.5637 - accuracy: 0.8754\n",
      "Epoch 67/200\n",
      "602/602 [==============================] - 0s 152us/step - loss: 0.6709 - accuracy: 0.8405\n",
      "Epoch 68/200\n",
      "602/602 [==============================] - 0s 162us/step - loss: 0.5715 - accuracy: 0.8704\n",
      "Epoch 69/200\n",
      "602/602 [==============================] - 0s 156us/step - loss: 0.6673 - accuracy: 0.8654\n",
      "Epoch 70/200\n",
      "602/602 [==============================] - 0s 167us/step - loss: 0.5803 - accuracy: 0.8455\n",
      "Epoch 71/200\n",
      "602/602 [==============================] - 0s 162us/step - loss: 0.6231 - accuracy: 0.8621\n",
      "Epoch 72/200\n",
      "602/602 [==============================] - 0s 159us/step - loss: 0.6943 - accuracy: 0.8621\n",
      "Epoch 73/200\n",
      "602/602 [==============================] - 0s 157us/step - loss: 0.5637 - accuracy: 0.8555\n",
      "Epoch 74/200\n",
      "602/602 [==============================] - 0s 165us/step - loss: 0.5222 - accuracy: 0.8704\n",
      "Epoch 75/200\n",
      "602/602 [==============================] - 0s 159us/step - loss: 0.5878 - accuracy: 0.8571\n",
      "Epoch 76/200\n",
      "602/602 [==============================] - 0s 169us/step - loss: 0.5548 - accuracy: 0.8571\n",
      "Epoch 77/200\n",
      "602/602 [==============================] - 0s 157us/step - loss: 0.5610 - accuracy: 0.8621\n",
      "Epoch 78/200\n",
      "602/602 [==============================] - 0s 159us/step - loss: 0.5654 - accuracy: 0.8571\n",
      "Epoch 79/200\n",
      "602/602 [==============================] - 0s 165us/step - loss: 0.5822 - accuracy: 0.8704\n",
      "Epoch 80/200\n",
      "602/602 [==============================] - 0s 178us/step - loss: 0.5432 - accuracy: 0.8654\n",
      "Epoch 81/200\n",
      "602/602 [==============================] - 0s 167us/step - loss: 0.7071 - accuracy: 0.8638\n",
      "Epoch 82/200\n",
      "602/602 [==============================] - 0s 159us/step - loss: 0.5467 - accuracy: 0.8738\n",
      "Epoch 83/200\n",
      "602/602 [==============================] - 0s 157us/step - loss: 0.5186 - accuracy: 0.8937\n",
      "Epoch 84/200\n",
      "602/602 [==============================] - 0s 161us/step - loss: 0.5488 - accuracy: 0.8904\n",
      "Epoch 85/200\n",
      "602/602 [==============================] - 0s 156us/step - loss: 0.5595 - accuracy: 0.8854\n",
      "Epoch 86/200\n",
      "602/602 [==============================] - 0s 164us/step - loss: 0.6127 - accuracy: 0.8771\n",
      "Epoch 87/200\n",
      "602/602 [==============================] - 0s 170us/step - loss: 0.5544 - accuracy: 0.8738\n",
      "Epoch 88/200\n",
      "602/602 [==============================] - 0s 170us/step - loss: 0.6219 - accuracy: 0.8571\n",
      "Epoch 89/200\n",
      "602/602 [==============================] - 0s 185us/step - loss: 0.6661 - accuracy: 0.8571\n",
      "Epoch 90/200\n",
      "602/602 [==============================] - 0s 203us/step - loss: 0.5881 - accuracy: 0.8605\n",
      "Epoch 91/200\n",
      "602/602 [==============================] - 0s 203us/step - loss: 0.5396 - accuracy: 0.8605\n",
      "Epoch 92/200\n",
      "602/602 [==============================] - 0s 193us/step - loss: 0.6342 - accuracy: 0.8588\n",
      "Epoch 93/200\n",
      "602/602 [==============================] - 0s 187us/step - loss: 0.5905 - accuracy: 0.8621\n",
      "Epoch 94/200\n",
      "602/602 [==============================] - 0s 177us/step - loss: 0.5240 - accuracy: 0.8854\n",
      "Epoch 95/200\n",
      "602/602 [==============================] - 0s 185us/step - loss: 0.5144 - accuracy: 0.8854\n",
      "Epoch 96/200\n",
      "602/602 [==============================] - 0s 195us/step - loss: 0.5816 - accuracy: 0.8721\n",
      "Epoch 97/200\n",
      "602/602 [==============================] - 0s 183us/step - loss: 0.5808 - accuracy: 0.8771\n",
      "Epoch 98/200\n",
      "602/602 [==============================] - 0s 307us/step - loss: 0.5955 - accuracy: 0.8804\n",
      "Epoch 99/200\n",
      "602/602 [==============================] - 0s 318us/step - loss: 0.5602 - accuracy: 0.8804\n",
      "Epoch 100/200\n",
      "602/602 [==============================] - 0s 229us/step - loss: 0.5527 - accuracy: 0.8787\n",
      "Epoch 101/200\n",
      "602/602 [==============================] - 0s 200us/step - loss: 0.5435 - accuracy: 0.8804\n",
      "Epoch 102/200\n",
      "602/602 [==============================] - 0s 211us/step - loss: 0.5557 - accuracy: 0.8671\n",
      "Epoch 103/200\n",
      "602/602 [==============================] - 0s 193us/step - loss: 0.5264 - accuracy: 0.8837\n",
      "Epoch 104/200\n",
      "602/602 [==============================] - 0s 190us/step - loss: 0.5584 - accuracy: 0.8837\n",
      "Epoch 105/200\n",
      "602/602 [==============================] - 0s 206us/step - loss: 0.5258 - accuracy: 0.8771\n",
      "Epoch 106/200\n",
      "602/602 [==============================] - 0s 188us/step - loss: 0.5463 - accuracy: 0.8771\n",
      "Epoch 107/200\n",
      "602/602 [==============================] - 0s 187us/step - loss: 0.4626 - accuracy: 0.8953\n",
      "Epoch 108/200\n",
      "602/602 [==============================] - 0s 436us/step - loss: 0.5018 - accuracy: 0.8837\n",
      "Epoch 109/200\n",
      "602/602 [==============================] - 0s 255us/step - loss: 0.5272 - accuracy: 0.8837\n",
      "Epoch 110/200\n",
      "602/602 [==============================] - 0s 188us/step - loss: 0.5127 - accuracy: 0.8704\n",
      "Epoch 111/200\n",
      "602/602 [==============================] - 0s 216us/step - loss: 0.5069 - accuracy: 0.8904\n",
      "Epoch 112/200\n",
      "602/602 [==============================] - 0s 217us/step - loss: 0.5401 - accuracy: 0.8688\n",
      "Epoch 113/200\n",
      "602/602 [==============================] - 0s 245us/step - loss: 0.6102 - accuracy: 0.8837\n",
      "Epoch 114/200\n",
      "602/602 [==============================] - 0s 183us/step - loss: 0.5617 - accuracy: 0.8854\n",
      "Epoch 115/200\n",
      "602/602 [==============================] - 0s 201us/step - loss: 0.7002 - accuracy: 0.8771\n",
      "Epoch 116/200\n",
      "602/602 [==============================] - 0s 425us/step - loss: 0.5098 - accuracy: 0.8821\n",
      "Epoch 117/200\n",
      "602/602 [==============================] - 0s 204us/step - loss: 0.4671 - accuracy: 0.8970\n",
      "Epoch 118/200\n",
      "602/602 [==============================] - 0s 193us/step - loss: 0.5410 - accuracy: 0.8821\n",
      "Epoch 119/200\n",
      "602/602 [==============================] - 0s 188us/step - loss: 0.6489 - accuracy: 0.8588\n",
      "Epoch 120/200\n",
      "602/602 [==============================] - 0s 198us/step - loss: 0.6009 - accuracy: 0.8804\n",
      "Epoch 121/200\n",
      "602/602 [==============================] - 0s 214us/step - loss: 0.5598 - accuracy: 0.8870\n",
      "Epoch 122/200\n",
      "602/602 [==============================] - 0s 187us/step - loss: 0.5455 - accuracy: 0.8970\n",
      "Epoch 123/200\n",
      "602/602 [==============================] - 0s 185us/step - loss: 0.5445 - accuracy: 0.8821\n",
      "Epoch 124/200\n",
      "602/602 [==============================] - 0s 193us/step - loss: 0.5941 - accuracy: 0.8870\n",
      "Epoch 125/200\n",
      "602/602 [==============================] - 0s 191us/step - loss: 0.4730 - accuracy: 0.8953\n",
      "Epoch 126/200\n",
      "602/602 [==============================] - 0s 198us/step - loss: 0.5066 - accuracy: 0.8970\n",
      "Epoch 127/200\n",
      "602/602 [==============================] - 0s 193us/step - loss: 0.6008 - accuracy: 0.8787\n",
      "Epoch 128/200\n",
      "602/602 [==============================] - 0s 164us/step - loss: 0.5102 - accuracy: 0.8704\n",
      "Epoch 129/200\n",
      "602/602 [==============================] - 0s 159us/step - loss: 0.5338 - accuracy: 0.8522\n",
      "Epoch 130/200\n",
      "602/602 [==============================] - 0s 169us/step - loss: 0.4758 - accuracy: 0.8654\n",
      "Epoch 131/200\n",
      "602/602 [==============================] - 0s 159us/step - loss: 0.6086 - accuracy: 0.8621\n",
      "Epoch 132/200\n",
      "602/602 [==============================] - 0s 170us/step - loss: 0.5408 - accuracy: 0.8704\n",
      "Epoch 133/200\n",
      "602/602 [==============================] - 0s 162us/step - loss: 0.4478 - accuracy: 0.8953\n",
      "Epoch 134/200\n",
      "602/602 [==============================] - 0s 157us/step - loss: 0.6107 - accuracy: 0.8738\n",
      "Epoch 135/200\n",
      "602/602 [==============================] - 0s 165us/step - loss: 0.4749 - accuracy: 0.8920\n",
      "Epoch 136/200\n",
      "602/602 [==============================] - 0s 165us/step - loss: 0.5064 - accuracy: 0.8738\n",
      "Epoch 137/200\n",
      "602/602 [==============================] - 0s 165us/step - loss: 0.5383 - accuracy: 0.8870\n",
      "Epoch 138/200\n",
      "602/602 [==============================] - 0s 211us/step - loss: 0.6210 - accuracy: 0.8854\n",
      "Epoch 139/200\n",
      "602/602 [==============================] - 0s 174us/step - loss: 0.4901 - accuracy: 0.8837\n",
      "Epoch 140/200\n",
      "602/602 [==============================] - 0s 161us/step - loss: 0.4803 - accuracy: 0.8987\n",
      "Epoch 141/200\n",
      "602/602 [==============================] - 0s 162us/step - loss: 0.4930 - accuracy: 0.8920\n",
      "Epoch 142/200\n",
      "602/602 [==============================] - 0s 167us/step - loss: 0.5025 - accuracy: 0.8804\n",
      "Epoch 143/200\n",
      "602/602 [==============================] - 0s 164us/step - loss: 0.5973 - accuracy: 0.8870\n",
      "Epoch 144/200\n",
      "602/602 [==============================] - 0s 165us/step - loss: 0.4794 - accuracy: 0.8987\n",
      "Epoch 145/200\n",
      "602/602 [==============================] - 0s 167us/step - loss: 0.5433 - accuracy: 0.8854\n",
      "Epoch 146/200\n",
      "602/602 [==============================] - 0s 167us/step - loss: 0.5634 - accuracy: 0.8887\n",
      "Epoch 147/200\n",
      "602/602 [==============================] - 0s 164us/step - loss: 0.5160 - accuracy: 0.8987\n",
      "Epoch 148/200\n",
      "602/602 [==============================] - 0s 165us/step - loss: 0.5608 - accuracy: 0.8854\n",
      "Epoch 149/200\n",
      "602/602 [==============================] - 0s 165us/step - loss: 0.5214 - accuracy: 0.8771\n",
      "Epoch 150/200\n",
      "602/602 [==============================] - 0s 157us/step - loss: 0.4612 - accuracy: 0.8854\n",
      "Epoch 151/200\n",
      "602/602 [==============================] - 0s 161us/step - loss: 0.4364 - accuracy: 0.8904\n",
      "Epoch 152/200\n",
      "602/602 [==============================] - 0s 161us/step - loss: 0.4397 - accuracy: 0.9020\n",
      "Epoch 153/200\n",
      "602/602 [==============================] - 0s 162us/step - loss: 0.5040 - accuracy: 0.8953\n",
      "Epoch 154/200\n",
      "602/602 [==============================] - 0s 161us/step - loss: 0.5094 - accuracy: 0.8887\n",
      "Epoch 155/200\n",
      "602/602 [==============================] - 0s 172us/step - loss: 0.5179 - accuracy: 0.8987\n",
      "Epoch 156/200\n",
      "602/602 [==============================] - 0s 161us/step - loss: 0.4801 - accuracy: 0.9103\n",
      "Epoch 157/200\n",
      "602/602 [==============================] - 0s 164us/step - loss: 0.5156 - accuracy: 0.8605\n",
      "Epoch 158/200\n",
      "602/602 [==============================] - 0s 161us/step - loss: 0.5238 - accuracy: 0.8771\n",
      "Epoch 159/200\n",
      "602/602 [==============================] - 0s 178us/step - loss: 0.5062 - accuracy: 0.8771\n",
      "Epoch 160/200\n",
      "602/602 [==============================] - 0s 172us/step - loss: 0.4989 - accuracy: 0.8870\n",
      "Epoch 161/200\n",
      "602/602 [==============================] - 0s 175us/step - loss: 0.3960 - accuracy: 0.9020\n",
      "Epoch 162/200\n",
      "602/602 [==============================] - 0s 164us/step - loss: 0.5284 - accuracy: 0.9020\n",
      "Epoch 163/200\n",
      "602/602 [==============================] - 0s 165us/step - loss: 0.5435 - accuracy: 0.8920\n",
      "Epoch 164/200\n",
      "602/602 [==============================] - 0s 164us/step - loss: 0.5570 - accuracy: 0.8953\n",
      "Epoch 165/200\n",
      "602/602 [==============================] - 0s 172us/step - loss: 0.5223 - accuracy: 0.9020\n",
      "Epoch 166/200\n",
      "602/602 [==============================] - 0s 162us/step - loss: 0.5429 - accuracy: 0.8787\n",
      "Epoch 167/200\n",
      "602/602 [==============================] - 0s 167us/step - loss: 0.4542 - accuracy: 0.8887\n",
      "Epoch 168/200\n",
      "602/602 [==============================] - 0s 156us/step - loss: 0.5636 - accuracy: 0.9103\n",
      "Epoch 169/200\n",
      "602/602 [==============================] - 0s 174us/step - loss: 0.4820 - accuracy: 0.8920\n",
      "Epoch 170/200\n",
      "602/602 [==============================] - 0s 164us/step - loss: 0.4954 - accuracy: 0.8721\n",
      "Epoch 171/200\n",
      "602/602 [==============================] - 0s 177us/step - loss: 0.5162 - accuracy: 0.8953\n",
      "Epoch 172/200\n",
      "602/602 [==============================] - 0s 339us/step - loss: 0.4338 - accuracy: 0.8937\n",
      "Epoch 173/200\n",
      "602/602 [==============================] - 0s 216us/step - loss: 0.4260 - accuracy: 0.9120\n",
      "Epoch 174/200\n",
      "602/602 [==============================] - 0s 164us/step - loss: 0.4736 - accuracy: 0.8987\n",
      "Epoch 175/200\n",
      "602/602 [==============================] - 0s 162us/step - loss: 0.5185 - accuracy: 0.8771\n",
      "Epoch 176/200\n",
      "602/602 [==============================] - 0s 174us/step - loss: 0.4793 - accuracy: 0.8804\n",
      "Epoch 177/200\n",
      "602/602 [==============================] - 0s 185us/step - loss: 0.4848 - accuracy: 0.8953\n",
      "Epoch 178/200\n",
      "602/602 [==============================] - 0s 178us/step - loss: 0.4616 - accuracy: 0.8937\n",
      "Epoch 179/200\n",
      "602/602 [==============================] - 0s 180us/step - loss: 0.4238 - accuracy: 0.9120\n",
      "Epoch 180/200\n",
      "602/602 [==============================] - 0s 170us/step - loss: 0.4655 - accuracy: 0.8987\n",
      "Epoch 181/200\n",
      "602/602 [==============================] - 0s 178us/step - loss: 0.5166 - accuracy: 0.8904\n",
      "Epoch 182/200\n",
      "602/602 [==============================] - 0s 247us/step - loss: 0.5110 - accuracy: 0.9003\n",
      "Epoch 183/200\n",
      "602/602 [==============================] - 0s 298us/step - loss: 0.4927 - accuracy: 0.9086\n",
      "Epoch 184/200\n",
      "602/602 [==============================] - 0s 185us/step - loss: 0.5026 - accuracy: 0.8970\n",
      "Epoch 185/200\n",
      "602/602 [==============================] - 0s 172us/step - loss: 0.5018 - accuracy: 0.9053\n",
      "Epoch 186/200\n",
      "602/602 [==============================] - 0s 180us/step - loss: 0.5374 - accuracy: 0.8937\n",
      "Epoch 187/200\n",
      "602/602 [==============================] - 0s 183us/step - loss: 0.4846 - accuracy: 0.9003\n",
      "Epoch 188/200\n",
      "602/602 [==============================] - 0s 209us/step - loss: 0.4426 - accuracy: 0.9003\n",
      "Epoch 189/200\n",
      "602/602 [==============================] - 0s 216us/step - loss: 0.4513 - accuracy: 0.9037\n",
      "Epoch 190/200\n",
      "602/602 [==============================] - 0s 212us/step - loss: 0.4742 - accuracy: 0.8920\n",
      "Epoch 191/200\n",
      "602/602 [==============================] - 0s 203us/step - loss: 0.4942 - accuracy: 0.8953\n",
      "Epoch 192/200\n",
      "602/602 [==============================] - 0s 214us/step - loss: 0.4161 - accuracy: 0.9053\n",
      "Epoch 193/200\n",
      "602/602 [==============================] - 0s 209us/step - loss: 0.5110 - accuracy: 0.8854\n",
      "Epoch 194/200\n",
      "602/602 [==============================] - 0s 221us/step - loss: 0.3949 - accuracy: 0.9086\n",
      "Epoch 195/200\n",
      "602/602 [==============================] - 0s 232us/step - loss: 0.4490 - accuracy: 0.9070\n",
      "Epoch 196/200\n",
      "602/602 [==============================] - 0s 496us/step - loss: 0.5144 - accuracy: 0.89370s - loss: 0.5885 - accuracy: 0.\n",
      "Epoch 197/200\n",
      "602/602 [==============================] - 0s 295us/step - loss: 0.4534 - accuracy: 0.8887\n",
      "Epoch 198/200\n",
      "602/602 [==============================] - 0s 221us/step - loss: 0.5139 - accuracy: 0.9003\n",
      "Epoch 199/200\n",
      "602/602 [==============================] - 0s 214us/step - loss: 0.4878 - accuracy: 0.8887\n",
      "Epoch 200/200\n",
      "602/602 [==============================] - 0s 234us/step - loss: 0.3630 - accuracy: 0.9203\n"
     ]
    }
   ],
   "source": [
    "epochs = 200 # maybe 200?\n",
    "model = Sequential()\n",
    "#model.add(Conv1D(32, 2, activation = 'relu', input_shape = input_shape))\n",
    "#model.add(Conv1D(filters=32, kernel_size=2, input_shape = (30) ))\n",
    "model.add(Conv1D(filters=32, kernel_size=10, strides=1, activation='swish', padding='valid', input_shape=input_shape ))\n",
    "# TypeError: 'int' object is not iterable\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool1D(2))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv1D(64, 2, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool1D(2))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "#model.compile(optimizer='adam', lr=0.001, loss = 'binary_crossentropy', metrics=['accuracy'])\n",
    "# or\n",
    "opt = keras.optimizers.RMSprop(learning_rate=0.0001, decay=1e-6)\n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "#class_weight = {1: 0.75,\n",
    "#                0: 0.25}\n",
    "\n",
    "#history = model.fit(X_train_arr, y_train_arr, epochs=epochs, validation_data=(X_test, y_test), verbose=1)\n",
    "history = model.fit(X_train_arr, y_train_arr, epochs=epochs, verbose=1, sample_weight=np.where(y_train_arr == 1,6.0,1.0).flatten())\n",
    "#history = model.fit(X_train_arr, y_train_arr, epochs=epochs, verbose=1, sample_weight=class_weight)\n",
    "#plot_learningCurve(history, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error\n",
      "error\n",
      "Confusion Matrix:\n",
      "false positive pct: 15.352654601846297\n",
      "tn  fp  fn  tp\n",
      "57740 10494 7 112\n",
      "[[57740 10494]\n",
      " [    7   112]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.85      0.92     68234\n",
      "           1       0.01      0.94      0.02       119\n",
      "\n",
      "    accuracy                           0.85     68353\n",
      "   macro avg       0.51      0.89      0.47     68353\n",
      "weighted avg       1.00      0.85      0.92     68353\n",
      "\n",
      "Specificity = 0.8462057039012809\n",
      "Sensitivity = 0.9411764705882353\n"
     ]
    }
   ],
   "source": [
    "# predict probabilities for test set\n",
    "yhat_probs = model.predict(X_test_arr, verbose=0)\n",
    "# predict crisp classes for test set\n",
    "yhat_classes = model.predict_classes(X_test_arr, verbose=0)\n",
    "# reduce to 1d array\n",
    "yhat_probs = yhat_probs[:, 0]\n",
    "yhat_classes = yhat_classes[:, 0]\n",
    "\n",
    "tn, fp, fn, tp = display_metrics(model, X_train_arr, X_test_arr, y_train_arr, y_test_arr, yhat_classes)\n",
    "#visualize(y_test_arr, yhat_classes, 'CNN Simple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST | AUC Score: 0.8936910872447581\n"
     ]
    }
   ],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve(y_test_arr, yhat_classes, pos_label=1)\n",
    "print('TEST | AUC Score: ' + str((metrics.auc(fpr, tpr))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
