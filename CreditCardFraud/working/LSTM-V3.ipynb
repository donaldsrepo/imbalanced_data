{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "some ideas: https://stackoverflow.com/questions/41488279/neural-network-always-predicts-the-same-class\n",
    "\n",
    "more ideas: https://www.tensorflow.org/tutorials/structured_data/imbalanced_data\n",
    "<pre>\n",
    "\n",
    "---------------------- TODO --------------------------------------\n",
    "\n",
    "1) \n",
    " \n",
    "\n",
    "---------------------- DONE --------------------------------------\n",
    "change the split and downsampling order\n",
    "\n",
    "now: start with an imbalanced dataset\n",
    "     downsample all the data to a small balanced subset\n",
    "     split and shape using stratify the small subset into 70/30 for Train/Test\n",
    "     means the Test dataset is also balanced\n",
    "     \n",
    "want: start with imbalanced dataset\n",
    "      split using sklearn:train_test_split into 70/30 for Train/Test\n",
    "      downsample and shape the Train dataset to become balanced\n",
    "      shape the Test dataset\n",
    "      means the Test dataset is imbalanced as it should be\n",
    "      \n",
    "      normalize the 3 datasets before building the model using StandardScaler\n",
    "      create a validation dataset to give a true unbiased result of my final models performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "colab = os.environ.get('COLAB_GPU', '10')\n",
    "if (int(colab) == 0):\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')  \n",
    "else:\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory  C:\\DataScience\\Repo\\Imbalanced_data\\input\\creditcardzip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Check if Google Colab path exists\n",
    "if os.path.exists(\"/content/drive/My Drive/MyDSNotebooks/Imbalanced_data/input/creditcardzip\") :\n",
    "    # Change the current working Directory    \n",
    "    os.chdir(\"/content/drive/My Drive/MyDSNotebooks/Imbalanced_data/input/creditcardzip\")\n",
    "# else check if Kaggle/local path exists\n",
    "elif os.path.exists(\"../input/creditcardzip\") :\n",
    "    # Change the current working Directory    \n",
    "    os.chdir(\"../input/creditcardzip\")\n",
    "else:\n",
    "    print(\"Can't change the Current Working Directory\") \n",
    "print(\"Current Working Directory \" , os.getcwd())\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers import Embedding\n",
    "#from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras import regularizers\n",
    "from keras import optimizers\n",
    "from keras import initializers\n",
    "from keras.optimizers import SGD\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.optimizers import Adagrad\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "# -------------------------------------------------------\n",
    "#initialize variables\n",
    "mpl.rcParams['figure.figsize'] = (12, 10)\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "cm_results = []\n",
    "class_names=[0,1] # name  of classes 1=fraudulent transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_learning_rate(epoch, lrate):\n",
    "\treturn lrate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_metrics(model_name, train_features, test_features, train_label, test_label, pred):\n",
    "    try:\n",
    "        print(model_name.score(test_features, test_label)) \n",
    "        print(\"Accuracy score (training): {0:.3f}\".format(model_name.score(train_features, train_label))) \n",
    "        print(\"Accuracy score (validation): {0:.3f}\".format(model_name.score(test_features, test_label))) \n",
    "    except Exception as e:\n",
    "        print(\"error\")  \n",
    "    try:\n",
    "        print(pd.Series(model_name.feature_importances_, index=train_features.columns[:]).nlargest(10).plot(kind='barh')) \n",
    "    except Exception as e:\n",
    "        print(\"error\")  \n",
    "        \n",
    "    print(\"Confusion Matrix:\")\n",
    "    tn, fp, fn, tp = confusion_matrix(test_label, pred).ravel()\n",
    "    total = tn+ fp+ fn+ tp \n",
    "    print(\"false positive pct:\",(fp/total)*100) \n",
    "    print(\"tn\", \" fp\", \" fn\", \" tp\") \n",
    "    print(tn, fp, fn, tp) \n",
    "    print(confusion_matrix(test_label, pred)) \n",
    "    print(\"Classification Report\") \n",
    "    print(classification_report(test_label, pred))\n",
    "    print(\"Specificity =\", tn/(tn+fp))\n",
    "    print(\"Sensitivity =\", tp/(tp+fn))\n",
    "    return tn, fp, fn, tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CalcPct(df,title):\n",
    "    unique_elements, counts_elements = np.unique(df, return_counts=True)\n",
    "    calc_pct = round(counts_elements[1]/(counts_elements[0]+counts_elements[1]) * 100,6)\n",
    "    print(title)\n",
    "    print(np.asarray((unique_elements, counts_elements)))\n",
    "    return calc_pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(history, label, n):\n",
    "    # Use a log scale to show the wide range of values.\n",
    "    plt.semilogy(history.epoch,  history.history['loss'],\n",
    "               color=colors[n], label='Train '+label)\n",
    "    plt.semilogy(history.epoch,  history.history['val_loss'],\n",
    "          color=colors[n], label='Val '+label,\n",
    "          linestyle=\"--\")\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284802</th>\n",
       "      <td>172786.0</td>\n",
       "      <td>-11.881118</td>\n",
       "      <td>10.071785</td>\n",
       "      <td>-9.834783</td>\n",
       "      <td>-2.066656</td>\n",
       "      <td>-5.364473</td>\n",
       "      <td>-2.606837</td>\n",
       "      <td>-4.918215</td>\n",
       "      <td>7.305334</td>\n",
       "      <td>1.914428</td>\n",
       "      <td>...</td>\n",
       "      <td>0.213454</td>\n",
       "      <td>0.111864</td>\n",
       "      <td>1.014480</td>\n",
       "      <td>-0.509348</td>\n",
       "      <td>1.436807</td>\n",
       "      <td>0.250034</td>\n",
       "      <td>0.943651</td>\n",
       "      <td>0.823731</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284803</th>\n",
       "      <td>172787.0</td>\n",
       "      <td>-0.732789</td>\n",
       "      <td>-0.055080</td>\n",
       "      <td>2.035030</td>\n",
       "      <td>-0.738589</td>\n",
       "      <td>0.868229</td>\n",
       "      <td>1.058415</td>\n",
       "      <td>0.024330</td>\n",
       "      <td>0.294869</td>\n",
       "      <td>0.584800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.214205</td>\n",
       "      <td>0.924384</td>\n",
       "      <td>0.012463</td>\n",
       "      <td>-1.016226</td>\n",
       "      <td>-0.606624</td>\n",
       "      <td>-0.395255</td>\n",
       "      <td>0.068472</td>\n",
       "      <td>-0.053527</td>\n",
       "      <td>24.79</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284804</th>\n",
       "      <td>172788.0</td>\n",
       "      <td>1.919565</td>\n",
       "      <td>-0.301254</td>\n",
       "      <td>-3.249640</td>\n",
       "      <td>-0.557828</td>\n",
       "      <td>2.630515</td>\n",
       "      <td>3.031260</td>\n",
       "      <td>-0.296827</td>\n",
       "      <td>0.708417</td>\n",
       "      <td>0.432454</td>\n",
       "      <td>...</td>\n",
       "      <td>0.232045</td>\n",
       "      <td>0.578229</td>\n",
       "      <td>-0.037501</td>\n",
       "      <td>0.640134</td>\n",
       "      <td>0.265745</td>\n",
       "      <td>-0.087371</td>\n",
       "      <td>0.004455</td>\n",
       "      <td>-0.026561</td>\n",
       "      <td>67.88</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284805</th>\n",
       "      <td>172788.0</td>\n",
       "      <td>-0.240440</td>\n",
       "      <td>0.530483</td>\n",
       "      <td>0.702510</td>\n",
       "      <td>0.689799</td>\n",
       "      <td>-0.377961</td>\n",
       "      <td>0.623708</td>\n",
       "      <td>-0.686180</td>\n",
       "      <td>0.679145</td>\n",
       "      <td>0.392087</td>\n",
       "      <td>...</td>\n",
       "      <td>0.265245</td>\n",
       "      <td>0.800049</td>\n",
       "      <td>-0.163298</td>\n",
       "      <td>0.123205</td>\n",
       "      <td>-0.569159</td>\n",
       "      <td>0.546668</td>\n",
       "      <td>0.108821</td>\n",
       "      <td>0.104533</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284806</th>\n",
       "      <td>172792.0</td>\n",
       "      <td>-0.533413</td>\n",
       "      <td>-0.189733</td>\n",
       "      <td>0.703337</td>\n",
       "      <td>-0.506271</td>\n",
       "      <td>-0.012546</td>\n",
       "      <td>-0.649617</td>\n",
       "      <td>1.577006</td>\n",
       "      <td>-0.414650</td>\n",
       "      <td>0.486180</td>\n",
       "      <td>...</td>\n",
       "      <td>0.261057</td>\n",
       "      <td>0.643078</td>\n",
       "      <td>0.376777</td>\n",
       "      <td>0.008797</td>\n",
       "      <td>-0.473649</td>\n",
       "      <td>-0.818267</td>\n",
       "      <td>-0.002415</td>\n",
       "      <td>0.013649</td>\n",
       "      <td>217.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>284807 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Time         V1         V2        V3        V4        V5  \\\n",
       "0            0.0  -1.359807  -0.072781  2.536347  1.378155 -0.338321   \n",
       "1            0.0   1.191857   0.266151  0.166480  0.448154  0.060018   \n",
       "2            1.0  -1.358354  -1.340163  1.773209  0.379780 -0.503198   \n",
       "3            1.0  -0.966272  -0.185226  1.792993 -0.863291 -0.010309   \n",
       "4            2.0  -1.158233   0.877737  1.548718  0.403034 -0.407193   \n",
       "...          ...        ...        ...       ...       ...       ...   \n",
       "284802  172786.0 -11.881118  10.071785 -9.834783 -2.066656 -5.364473   \n",
       "284803  172787.0  -0.732789  -0.055080  2.035030 -0.738589  0.868229   \n",
       "284804  172788.0   1.919565  -0.301254 -3.249640 -0.557828  2.630515   \n",
       "284805  172788.0  -0.240440   0.530483  0.702510  0.689799 -0.377961   \n",
       "284806  172792.0  -0.533413  -0.189733  0.703337 -0.506271 -0.012546   \n",
       "\n",
       "              V6        V7        V8        V9  ...       V21       V22  \\\n",
       "0       0.462388  0.239599  0.098698  0.363787  ... -0.018307  0.277838   \n",
       "1      -0.082361 -0.078803  0.085102 -0.255425  ... -0.225775 -0.638672   \n",
       "2       1.800499  0.791461  0.247676 -1.514654  ...  0.247998  0.771679   \n",
       "3       1.247203  0.237609  0.377436 -1.387024  ... -0.108300  0.005274   \n",
       "4       0.095921  0.592941 -0.270533  0.817739  ... -0.009431  0.798278   \n",
       "...          ...       ...       ...       ...  ...       ...       ...   \n",
       "284802 -2.606837 -4.918215  7.305334  1.914428  ...  0.213454  0.111864   \n",
       "284803  1.058415  0.024330  0.294869  0.584800  ...  0.214205  0.924384   \n",
       "284804  3.031260 -0.296827  0.708417  0.432454  ...  0.232045  0.578229   \n",
       "284805  0.623708 -0.686180  0.679145  0.392087  ...  0.265245  0.800049   \n",
       "284806 -0.649617  1.577006 -0.414650  0.486180  ...  0.261057  0.643078   \n",
       "\n",
       "             V23       V24       V25       V26       V27       V28  Amount  \\\n",
       "0      -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053  149.62   \n",
       "1       0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724    2.69   \n",
       "2       0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752  378.66   \n",
       "3      -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458  123.50   \n",
       "4      -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153   69.99   \n",
       "...          ...       ...       ...       ...       ...       ...     ...   \n",
       "284802  1.014480 -0.509348  1.436807  0.250034  0.943651  0.823731    0.77   \n",
       "284803  0.012463 -1.016226 -0.606624 -0.395255  0.068472 -0.053527   24.79   \n",
       "284804 -0.037501  0.640134  0.265745 -0.087371  0.004455 -0.026561   67.88   \n",
       "284805 -0.163298  0.123205 -0.569159  0.546668  0.108821  0.104533   10.00   \n",
       "284806  0.376777  0.008797 -0.473649 -0.818267 -0.002415  0.013649  217.00   \n",
       "\n",
       "        Class  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           0  \n",
       "4           0  \n",
       "...       ...  \n",
       "284802      0  \n",
       "284803      0  \n",
       "284804      0  \n",
       "284805      0  \n",
       "284806      0  \n",
       "\n",
       "[284807 rows x 31 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('https://storage.googleapis.com/download.tensorflow.org/data/creditcard.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try some data cleansing\n",
    "\n",
    "temp_df = df.copy()\n",
    "temp_df = temp_df.drop(['Time'], axis=1)\n",
    "temp_df['Log_Amount'] = np.log(temp_df.pop('Amount')+0.001)\n",
    "df = temp_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Class</th>\n",
       "      <th>Log_Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>0.090794</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>0</td>\n",
       "      <td>5.008105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>-0.166974</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>0</td>\n",
       "      <td>0.989913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>0.207643</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>0</td>\n",
       "      <td>5.936641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>-0.054952</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>0</td>\n",
       "      <td>4.816249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>0.753074</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>0</td>\n",
       "      <td>4.248367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284802</th>\n",
       "      <td>-11.881118</td>\n",
       "      <td>10.071785</td>\n",
       "      <td>-9.834783</td>\n",
       "      <td>-2.066656</td>\n",
       "      <td>-5.364473</td>\n",
       "      <td>-2.606837</td>\n",
       "      <td>-4.918215</td>\n",
       "      <td>7.305334</td>\n",
       "      <td>1.914428</td>\n",
       "      <td>4.356170</td>\n",
       "      <td>...</td>\n",
       "      <td>0.213454</td>\n",
       "      <td>0.111864</td>\n",
       "      <td>1.014480</td>\n",
       "      <td>-0.509348</td>\n",
       "      <td>1.436807</td>\n",
       "      <td>0.250034</td>\n",
       "      <td>0.943651</td>\n",
       "      <td>0.823731</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.260067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284803</th>\n",
       "      <td>-0.732789</td>\n",
       "      <td>-0.055080</td>\n",
       "      <td>2.035030</td>\n",
       "      <td>-0.738589</td>\n",
       "      <td>0.868229</td>\n",
       "      <td>1.058415</td>\n",
       "      <td>0.024330</td>\n",
       "      <td>0.294869</td>\n",
       "      <td>0.584800</td>\n",
       "      <td>-0.975926</td>\n",
       "      <td>...</td>\n",
       "      <td>0.214205</td>\n",
       "      <td>0.924384</td>\n",
       "      <td>0.012463</td>\n",
       "      <td>-1.016226</td>\n",
       "      <td>-0.606624</td>\n",
       "      <td>-0.395255</td>\n",
       "      <td>0.068472</td>\n",
       "      <td>-0.053527</td>\n",
       "      <td>0</td>\n",
       "      <td>3.210481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284804</th>\n",
       "      <td>1.919565</td>\n",
       "      <td>-0.301254</td>\n",
       "      <td>-3.249640</td>\n",
       "      <td>-0.557828</td>\n",
       "      <td>2.630515</td>\n",
       "      <td>3.031260</td>\n",
       "      <td>-0.296827</td>\n",
       "      <td>0.708417</td>\n",
       "      <td>0.432454</td>\n",
       "      <td>-0.484782</td>\n",
       "      <td>...</td>\n",
       "      <td>0.232045</td>\n",
       "      <td>0.578229</td>\n",
       "      <td>-0.037501</td>\n",
       "      <td>0.640134</td>\n",
       "      <td>0.265745</td>\n",
       "      <td>-0.087371</td>\n",
       "      <td>0.004455</td>\n",
       "      <td>-0.026561</td>\n",
       "      <td>0</td>\n",
       "      <td>4.217756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284805</th>\n",
       "      <td>-0.240440</td>\n",
       "      <td>0.530483</td>\n",
       "      <td>0.702510</td>\n",
       "      <td>0.689799</td>\n",
       "      <td>-0.377961</td>\n",
       "      <td>0.623708</td>\n",
       "      <td>-0.686180</td>\n",
       "      <td>0.679145</td>\n",
       "      <td>0.392087</td>\n",
       "      <td>-0.399126</td>\n",
       "      <td>...</td>\n",
       "      <td>0.265245</td>\n",
       "      <td>0.800049</td>\n",
       "      <td>-0.163298</td>\n",
       "      <td>0.123205</td>\n",
       "      <td>-0.569159</td>\n",
       "      <td>0.546668</td>\n",
       "      <td>0.108821</td>\n",
       "      <td>0.104533</td>\n",
       "      <td>0</td>\n",
       "      <td>2.302685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284806</th>\n",
       "      <td>-0.533413</td>\n",
       "      <td>-0.189733</td>\n",
       "      <td>0.703337</td>\n",
       "      <td>-0.506271</td>\n",
       "      <td>-0.012546</td>\n",
       "      <td>-0.649617</td>\n",
       "      <td>1.577006</td>\n",
       "      <td>-0.414650</td>\n",
       "      <td>0.486180</td>\n",
       "      <td>-0.915427</td>\n",
       "      <td>...</td>\n",
       "      <td>0.261057</td>\n",
       "      <td>0.643078</td>\n",
       "      <td>0.376777</td>\n",
       "      <td>0.008797</td>\n",
       "      <td>-0.473649</td>\n",
       "      <td>-0.818267</td>\n",
       "      <td>-0.002415</td>\n",
       "      <td>0.013649</td>\n",
       "      <td>0</td>\n",
       "      <td>5.379902</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>284807 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               V1         V2        V3        V4        V5        V6  \\\n",
       "0       -1.359807  -0.072781  2.536347  1.378155 -0.338321  0.462388   \n",
       "1        1.191857   0.266151  0.166480  0.448154  0.060018 -0.082361   \n",
       "2       -1.358354  -1.340163  1.773209  0.379780 -0.503198  1.800499   \n",
       "3       -0.966272  -0.185226  1.792993 -0.863291 -0.010309  1.247203   \n",
       "4       -1.158233   0.877737  1.548718  0.403034 -0.407193  0.095921   \n",
       "...           ...        ...       ...       ...       ...       ...   \n",
       "284802 -11.881118  10.071785 -9.834783 -2.066656 -5.364473 -2.606837   \n",
       "284803  -0.732789  -0.055080  2.035030 -0.738589  0.868229  1.058415   \n",
       "284804   1.919565  -0.301254 -3.249640 -0.557828  2.630515  3.031260   \n",
       "284805  -0.240440   0.530483  0.702510  0.689799 -0.377961  0.623708   \n",
       "284806  -0.533413  -0.189733  0.703337 -0.506271 -0.012546 -0.649617   \n",
       "\n",
       "              V7        V8        V9       V10  ...       V21       V22  \\\n",
       "0       0.239599  0.098698  0.363787  0.090794  ... -0.018307  0.277838   \n",
       "1      -0.078803  0.085102 -0.255425 -0.166974  ... -0.225775 -0.638672   \n",
       "2       0.791461  0.247676 -1.514654  0.207643  ...  0.247998  0.771679   \n",
       "3       0.237609  0.377436 -1.387024 -0.054952  ... -0.108300  0.005274   \n",
       "4       0.592941 -0.270533  0.817739  0.753074  ... -0.009431  0.798278   \n",
       "...          ...       ...       ...       ...  ...       ...       ...   \n",
       "284802 -4.918215  7.305334  1.914428  4.356170  ...  0.213454  0.111864   \n",
       "284803  0.024330  0.294869  0.584800 -0.975926  ...  0.214205  0.924384   \n",
       "284804 -0.296827  0.708417  0.432454 -0.484782  ...  0.232045  0.578229   \n",
       "284805 -0.686180  0.679145  0.392087 -0.399126  ...  0.265245  0.800049   \n",
       "284806  1.577006 -0.414650  0.486180 -0.915427  ...  0.261057  0.643078   \n",
       "\n",
       "             V23       V24       V25       V26       V27       V28  Class  \\\n",
       "0      -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053      0   \n",
       "1       0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724      0   \n",
       "2       0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752      0   \n",
       "3      -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458      0   \n",
       "4      -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153      0   \n",
       "...          ...       ...       ...       ...       ...       ...    ...   \n",
       "284802  1.014480 -0.509348  1.436807  0.250034  0.943651  0.823731      0   \n",
       "284803  0.012463 -1.016226 -0.606624 -0.395255  0.068472 -0.053527      0   \n",
       "284804 -0.037501  0.640134  0.265745 -0.087371  0.004455 -0.026561      0   \n",
       "284805 -0.163298  0.123205 -0.569159  0.546668  0.108821  0.104533      0   \n",
       "284806  0.376777  0.008797 -0.473649 -0.818267 -0.002415  0.013649      0   \n",
       "\n",
       "        Log_Amount  \n",
       "0         5.008105  \n",
       "1         0.989913  \n",
       "2         5.936641  \n",
       "3         4.816249  \n",
       "4         4.248367  \n",
       "...            ...  \n",
       "284802   -0.260067  \n",
       "284803    3.210481  \n",
       "284804    4.217756  \n",
       "284805    2.302685  \n",
       "284806    5.379902  \n",
       "\n",
       "[284807 rows x 30 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original\n",
      "[[     0      1]\n",
      " [284315    492]]\n",
      "Train\n",
      "[[     0      1]\n",
      " [199020    344]]\n",
      "Train\n",
      "[[    0     1]\n",
      " [42647    74]]\n",
      "Train\n",
      "[[    0     1]\n",
      " [42648    74]]\n"
     ]
    }
   ],
   "source": [
    "X = df.loc[:, df.columns != 'Class']\n",
    "y = df.loc[:, df.columns == 'Class']\n",
    "OrigPct = CalcPct(y,\"Original\")\n",
    "\n",
    "#print(y['Class'].value_counts())\n",
    "# split the dataset\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = None, shuffle=True)\n",
    "test_size = 0.3\n",
    "val_size = 0.5\n",
    "\n",
    "strat = True\n",
    "if (strat == True):\n",
    "    stratify=y['Class']\n",
    "else:\n",
    "    stratify=\"None\"\n",
    "# stratify will ensure that Train, Test and Validation get the same pct of minority classes (.17%)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = None, shuffle=True, stratify=stratify)\n",
    "X_train, X_test1, y_train, y_test1 = train_test_split(X,y, test_size = test_size, random_state = None, shuffle=True, stratify=stratify)\n",
    "# then split Test1 into Test and Validate\n",
    "# Validate will be used as a final benchmark, once all the parameter tuning is completed\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test1,y_test1, test_size = val_size, random_state = None, shuffle=True)\n",
    "\n",
    "TrainPct = CalcPct(y_train,\"Train\")\n",
    "TestPct = CalcPct(y_test,\"Train\")\n",
    "ValPct = CalcPct(y_val,\"Train\")\n",
    "zeros, ones = np.bincount(y_train['Class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final data validation\n",
    "\n",
    "# Form np arrays of labels and features.\n",
    "\n",
    "train_labels = np.array(y_train).flatten()\n",
    "bool_train_labels = train_labels != 0 # has an extra ,1 in the bool_train_labels.shape\n",
    "val_labels = np.array(y_val)\n",
    "test_labels = np.array(y_test)\n",
    "train_features = np.array(X_train)\n",
    "val_features = np.array(X_val)\n",
    "test_features = np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pos_df = pd.DataFrame(train_features[ bool_train_labels], columns = X.columns)\n",
    "neg_df = pd.DataFrame(train_features[~bool_train_labels], columns = X.columns)\n",
    "\n",
    "sns.jointplot(pos_df['V5'], pos_df['V6'],\n",
    "              kind='hex', xlim = (-5,5), ylim = (-5,5))\n",
    "plt.suptitle(\"Positive distribution\")\n",
    "\n",
    "sns.jointplot(neg_df['V5'], neg_df['V6'],\n",
    "              kind='hex', xlim = (-5,5), ylim = (-5,5))\n",
    "_ = plt.suptitle(\"Negative distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainDownSampled\n",
      "[[  0   1]\n",
      " [344 344]]\n",
      "Test\n",
      "[[    0     1]\n",
      " [42647    74]]\n",
      "Val\n",
      "[[    0     1]\n",
      " [42648    74]]\n"
     ]
    }
   ],
   "source": [
    "# prepare the Train, Test and Validation data for the model. Need input to include arrays with certain shape\n",
    "\n",
    "# downsample the Train data\n",
    "\n",
    "# find the number of minority (value=1) samples in our dataset so we can down-sample our majority to it\n",
    "yes = len(y_train[y_train['Class'] ==1])\n",
    "\n",
    "# retrieve the indices of the minority and majority samples \n",
    "yes_ind = y_train[y_train['Class'] == 1].index\n",
    "no_ind = y_train[y_train['Class'] == 0].index\n",
    "\n",
    "# random sample the majority indices based on the amount of \n",
    "# minority samples\n",
    "new_no_ind = np.random.choice(no_ind, yes, replace = False)\n",
    "\n",
    "# merge the two indices together\n",
    "undersample_ind = np.concatenate([new_no_ind, yes_ind])\n",
    "\n",
    "# get undersampled dataframe from the merged indices of the dataset\n",
    "X_train = X_train.loc[undersample_ind]\n",
    "y_temp = y_train.loc[undersample_ind]\n",
    "y_train_original = np.array(y_temp['Class'], dtype='int')\n",
    "TrainDownPct = CalcPct(y_train_original,\"TrainDownSampled\")\n",
    "y_test_original = np.array(y_test['Class'], dtype='int')\n",
    "TestPct = CalcPct(y_test_original,\"Test\")\n",
    "y_val_original = np.array(y_val['Class'], dtype='int')\n",
    "ValPct = CalcPct(y_val_original,\"Val\")\n",
    "\n",
    "# normalize the features\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "X_val = sc.transform(X_val)\n",
    "\n",
    "# handle any extreme fliers, set to 5 or -5\n",
    "X_train = np.clip(X_train, -5, 5)\n",
    "X_test = np.clip(X_test, -5, 5)\n",
    "X_val = np.clip(X_val, -5, 5)\n",
    "\n",
    "#shape the Train data \n",
    "rolling_window_size = 10  ### this selects how many historical transactions should be analyzed to judge the transaction at hand -- RNN width\n",
    "\n",
    "X_train_interim = np.zeros([(X_train.shape[0]-rolling_window_size)*rolling_window_size,X_train.shape[1]]) # change 30 to a variable\n",
    "y_train = []\n",
    "for i in range((X_train.shape[0]-rolling_window_size)):\n",
    "    beg = 0+i\n",
    "    end = beg+rolling_window_size\n",
    "    s = np.array(X_train[beg:end], dtype='float')\n",
    "    X_train_interim[(rolling_window_size*i):(rolling_window_size*(i+1)),:] = s\n",
    "    y_train.append(y_train_original[end])\n",
    "\n",
    "y_train = np.array(y_train, dtype='int')\n",
    "X_train_interim = X_train_interim[:,1::]\n",
    "\n",
    "X_train_tensor = X_train_interim.reshape(int(np.shape(X_train_interim)[0]/rolling_window_size), rolling_window_size, np.shape(X_train_interim)[1])\n",
    "X_train = X_train_tensor\n",
    "\n",
    "#shape the Test data for LSTM !!!\n",
    "\n",
    "X_test_interim = np.zeros([(X_test.shape[0]-rolling_window_size)*rolling_window_size,X_test.shape[1]])\n",
    "y_test = []\n",
    "for i in range((X_test.shape[0]-rolling_window_size)):\n",
    "    beg = 0+i\n",
    "    end = beg+rolling_window_size\n",
    "    s = np.array(X_test[beg:end], dtype='float')\n",
    "    X_test_interim[(rolling_window_size*i):(rolling_window_size*(i+1)),:] = s\n",
    "    y_test.append(y_test_original[end]) \n",
    "\n",
    "y_test = np.array(y_test, dtype='int')\n",
    "X_test_interim = X_test_interim[:,1::]\n",
    "\n",
    "X_test_tensor = X_test_interim.reshape(int(np.shape(X_test_interim)[0]/rolling_window_size), rolling_window_size, np.shape(X_test_interim)[1])\n",
    "X_test = X_test_tensor\n",
    "\n",
    "#shape the Val data for LSTM\n",
    "\n",
    "X_val_interim = np.zeros([(X_val.shape[0]-rolling_window_size)*rolling_window_size,X_val.shape[1]])\n",
    "y_val = []\n",
    "for i in range((X_val.shape[0]-rolling_window_size)):\n",
    "    beg = 0+i\n",
    "    end = beg+rolling_window_size\n",
    "    s = np.array(X_val[beg:end], dtype='float')\n",
    "    X_val_interim[(rolling_window_size*i):(rolling_window_size*(i+1)),:] = s\n",
    "    y_val.append(y_val_original[end]) \n",
    "\n",
    "y_val = np.array(y_val, dtype='int')\n",
    "X_val_interim = X_val_interim[:,1::]\n",
    "\n",
    "X_val_tensor = X_val_interim.reshape(int(np.shape(X_val_interim)[0]/rolling_window_size), rolling_window_size, np.shape(X_val_interim)[1])\n",
    "X_val = X_val_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final data validation\n",
    "\n",
    "# Form np arrays of labels and features.\n",
    "\n",
    "train_labels = np.array(y_train)\n",
    "bool_train_labels = train_labels != 0\n",
    "val_labels = np.array(y_val)\n",
    "test_labels = np.array(y_test)\n",
    "\n",
    "train_features = np.array(X_train)\n",
    "val_features = np.array(X_val)\n",
    "test_features = np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bias_initializer 0.0017284695005527083\n",
      "Initial Loss: 0.7331\n",
      "['loss', 'tp', 'fp', 'tn', 'fn', 'accuracy', 'precision', 'recall', 'auc']\n",
      "[0.7331303954124451, 0.0, 0.0, 334.0, 344.0, 0.49262535572052, 0.0, 0.0, 0.5]\n",
      "Train on 678 samples, validate on 42712 samples\n",
      "Epoch 1/100\n",
      "678/678 [==============================] - 3s 4ms/step - loss: 0.7331 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 668.0000 - fn: 688.0000 - accuracy: 0.4926 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5000 - val_loss: 0.4652 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 24991.0918 - val_fn: 727.8182 - val_accuracy: 0.9587 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.4993\n",
      "Epoch 2/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.7289 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 43640.0000 - fn: 1106.0000 - accuracy: 0.9753 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.6514 - val_loss: 0.4715 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 67963.0938 - val_fn: 1145.8182 - val_accuracy: 0.9829 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.4965\n",
      "Epoch 3/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.7234 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 86612.0000 - fn: 1524.0000 - accuracy: 0.9827 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5531 - val_loss: 0.4779 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 110935.0938 - val_fn: 1563.8182 - val_accuracy: 0.9859 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.4726\n",
      "Epoch 4/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.7153 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 129584.0000 - fn: 1942.0000 - accuracy: 0.9852 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5284 - val_loss: 0.4842 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 153907.0938 - val_fn: 1981.8182 - val_accuracy: 0.9872 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.4798\n",
      "Epoch 5/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.7014 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 172556.0000 - fn: 2360.0000 - accuracy: 0.9865 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5303 - val_loss: 0.4904 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 196879.0938 - val_fn: 2399.8181 - val_accuracy: 0.9879 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.4988\n",
      "Epoch 6/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.6769 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 215528.0000 - fn: 2778.0000 - accuracy: 0.9873 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5434 - val_loss: 0.4963 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 239851.0938 - val_fn: 2817.8181 - val_accuracy: 0.9884 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5242\n",
      "Epoch 7/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.6350 - tp: 22.0000 - fp: 0.0000e+00 - tn: 258500.0000 - fn: 3174.0000 - accuracy: 0.9879 - precision: 1.0000 - recall: 0.0069 - auc: 0.5642 - val_loss: 0.5013 - val_tp: 22.0000 - val_fp: 0.0000e+00 - val_tn: 282823.0938 - val_fn: 3213.8181 - val_accuracy: 0.9887 - val_precision: 1.0000 - val_recall: 0.0068 - val_auc: 0.5477\n",
      "Epoch 8/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.6006 - tp: 254.0000 - fp: 0.0000e+00 - tn: 301472.0000 - fn: 3360.0000 - accuracy: 0.9890 - precision: 1.0000 - recall: 0.0703 - auc: 0.5809 - val_loss: 0.5048 - val_tp: 254.0000 - val_fp: 0.0000e+00 - val_tn: 325795.0938 - val_fn: 3399.8181 - val_accuracy: 0.9897 - val_precision: 1.0000 - val_recall: 0.0695 - val_auc: 0.5696\n",
      "Epoch 9/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.5868 - tp: 569.0000 - fp: 0.0000e+00 - tn: 344444.0000 - fn: 3463.0000 - accuracy: 0.9901 - precision: 1.0000 - recall: 0.1411 - auc: 0.5993 - val_loss: 0.5067 - val_tp: 569.0000 - val_fp: 0.7273 - val_tn: 368766.3750 - val_fn: 3502.8181 - val_accuracy: 0.9906 - val_precision: 0.9987 - val_recall: 0.1397 - val_auc: 0.5904\n",
      "Epoch 10/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.5822 - tp: 904.0000 - fp: 1.0000 - tn: 387415.0000 - fn: 3546.0000 - accuracy: 0.9909 - precision: 0.9989 - recall: 0.2031 - auc: 0.6164 - val_loss: 0.5068 - val_tp: 904.0000 - val_fp: 17.5455 - val_tn: 411721.5312 - val_fn: 3585.8181 - val_accuracy: 0.9913 - val_precision: 0.9810 - val_recall: 0.2013 - val_auc: 0.6097\n",
      "Epoch 11/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.5799 - tp: 1242.0000 - fp: 26.0000 - tn: 430362.0000 - fn: 3626.0000 - accuracy: 0.9916 - precision: 0.9795 - recall: 0.2551 - auc: 0.6328 - val_loss: 0.5045 - val_tp: 1242.0000 - val_fp: 63.0000 - val_tn: 454648.0938 - val_fn: 3665.8181 - val_accuracy: 0.9919 - val_precision: 0.9519 - val_recall: 0.2531 - val_auc: 0.6278\n",
      "Epoch 12/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.5771 - tp: 1581.0000 - fp: 84.0000 - tn: 473276.0000 - fn: 3705.0000 - accuracy: 0.9921 - precision: 0.9495 - recall: 0.2991 - auc: 0.6487 - val_loss: 0.4985 - val_tp: 1581.0000 - val_fp: 136.6364 - val_tn: 497546.4688 - val_fn: 3744.8181 - val_accuracy: 0.9923 - val_precision: 0.9207 - val_recall: 0.2969 - val_auc: 0.6457\n",
      "Epoch 13/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.5721 - tp: 1920.0000 - fp: 170.0000 - tn: 516162.0000 - fn: 3784.0000 - accuracy: 0.9924 - precision: 0.9187 - recall: 0.3366 - auc: 0.6646 - val_loss: 0.4857 - val_tp: 1920.0000 - val_fp: 240.2727 - val_tn: 540414.8125 - val_fn: 3823.8181 - val_accuracy: 0.9926 - val_precision: 0.8890 - val_recall: 0.3343 - val_auc: 0.6641\n",
      "Epoch 14/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.5640 - tp: 2260.0000 - fp: 285.0000 - tn: 559019.0000 - fn: 3862.0000 - accuracy: 0.9927 - precision: 0.8880 - recall: 0.3692 - auc: 0.6822 - val_loss: 0.4591 - val_tp: 2260.0000 - val_fp: 365.0909 - val_tn: 583262.0000 - val_fn: 3901.8181 - val_accuracy: 0.9928 - val_precision: 0.8611 - val_recall: 0.3668 - val_auc: 0.6894\n",
      "Epoch 15/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.5480 - tp: 2600.0000 - fp: 416.0000 - tn: 601860.0000 - fn: 3940.0000 - accuracy: 0.9928 - precision: 0.8621 - recall: 0.3976 - auc: 0.7100 - val_loss: 0.4088 - val_tp: 2600.0000 - val_fp: 495.7273 - val_tn: 626103.3750 - val_fn: 3979.8181 - val_accuracy: 0.9929 - val_precision: 0.8400 - val_recall: 0.3952 - val_auc: 0.7168\n",
      "Epoch 16/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.5258 - tp: 2940.0000 - fp: 547.0000 - tn: 644701.0000 - fn: 4018.0000 - accuracy: 0.9930 - precision: 0.8431 - recall: 0.4225 - auc: 0.7346 - val_loss: 0.3488 - val_tp: 2940.0000 - val_fp: 616.6364 - val_tn: 668954.4375 - val_fn: 4057.8181 - val_accuracy: 0.9931 - val_precision: 0.8267 - val_recall: 0.4201 - val_auc: 0.7400\n",
      "Epoch 17/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.5029 - tp: 3280.0000 - fp: 663.0000 - tn: 687557.0000 - fn: 4096.0000 - accuracy: 0.9932 - precision: 0.8319 - recall: 0.4447 - auc: 0.7549 - val_loss: 0.3365 - val_tp: 3280.0000 - val_fp: 725.8182 - val_tn: 711817.2500 - val_fn: 4135.8184 - val_accuracy: 0.9932 - val_precision: 0.8189 - val_recall: 0.4423 - val_auc: 0.7591\n",
      "Epoch 18/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4892 - tp: 3619.0000 - fp: 766.0000 - tn: 730426.0000 - fn: 4175.0000 - accuracy: 0.9933 - precision: 0.8253 - recall: 0.4643 - auc: 0.7718 - val_loss: 0.3358 - val_tp: 3619.0000 - val_fp: 823.0909 - val_tn: 754692.0000 - val_fn: 4214.8184 - val_accuracy: 0.9934 - val_precision: 0.8147 - val_recall: 0.4620 - val_auc: 0.7752\n",
      "Epoch 19/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4841 - tp: 3958.0000 - fp: 859.0000 - tn: 773305.0000 - fn: 4254.0000 - accuracy: 0.9935 - precision: 0.8217 - recall: 0.4820 - auc: 0.7860 - val_loss: 0.3353 - val_tp: 3958.0000 - val_fp: 909.6364 - val_tn: 797577.4375 - val_fn: 4293.8184 - val_accuracy: 0.9935 - val_precision: 0.8131 - val_recall: 0.4797 - val_auc: 0.7887\n",
      "Epoch 20/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4811 - tp: 4297.0000 - fp: 941.0000 - tn: 816195.0000 - fn: 4333.0000 - accuracy: 0.9936 - precision: 0.8204 - recall: 0.4979 - auc: 0.7981 - val_loss: 0.3348 - val_tp: 4297.0000 - val_fp: 987.3636 - val_tn: 840471.7500 - val_fn: 4372.8184 - val_accuracy: 0.9937 - val_precision: 0.8132 - val_recall: 0.4956 - val_auc: 0.8002\n",
      "Epoch 21/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4793 - tp: 4635.0000 - fp: 1017.0000 - tn: 859091.0000 - fn: 4413.0000 - accuracy: 0.9938 - precision: 0.8201 - recall: 0.5123 - auc: 0.8084 - val_loss: 0.3343 - val_tp: 4635.0000 - val_fp: 1057.6364 - val_tn: 883373.4375 - val_fn: 4452.8184 - val_accuracy: 0.9938 - val_precision: 0.8142 - val_recall: 0.5100 - val_auc: 0.8100\n",
      "Epoch 22/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4767 - tp: 4973.0000 - fp: 1084.0000 - tn: 901996.0000 - fn: 4493.0000 - accuracy: 0.9939 - precision: 0.8210 - recall: 0.5254 - auc: 0.8172 - val_loss: 0.3338 - val_tp: 4973.0000 - val_fp: 1118.9091 - val_tn: 926284.1875 - val_fn: 4532.8184 - val_accuracy: 0.9940 - val_precision: 0.8163 - val_recall: 0.5232 - val_auc: 0.8185\n",
      "Epoch 23/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4744 - tp: 5311.0000 - fp: 1141.0000 - tn: 944911.0000 - fn: 4573.0000 - accuracy: 0.9940 - precision: 0.8232 - recall: 0.5373 - auc: 0.8250 - val_loss: 0.3334 - val_tp: 5311.0000 - val_fp: 1173.7273 - val_tn: 969201.3750 - val_fn: 4612.8184 - val_accuracy: 0.9941 - val_precision: 0.8190 - val_recall: 0.5352 - val_auc: 0.8260\n",
      "Epoch 24/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4741 - tp: 5647.0000 - fp: 1195.0000 - tn: 987829.0000 - fn: 4655.0000 - accuracy: 0.9941 - precision: 0.8253 - recall: 0.5481 - auc: 0.8316 - val_loss: 0.3329 - val_tp: 5647.0000 - val_fp: 1223.0909 - val_tn: 1012124.0000 - val_fn: 4694.8184 - val_accuracy: 0.9942 - val_precision: 0.8220 - val_recall: 0.5460 - val_auc: 0.8323\n",
      "Epoch 25/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4728 - tp: 5982.0000 - fp: 1242.0000 - tn: 1030754.0000 - fn: 4738.0000 - accuracy: 0.9943 - precision: 0.8281 - recall: 0.5580 - auc: 0.8373 - val_loss: 0.3326 - val_tp: 5982.0000 - val_fp: 1274.4546 - val_tn: 1055044.6250 - val_fn: 4777.8184 - val_accuracy: 0.9943 - val_precision: 0.8244 - val_recall: 0.5560 - val_auc: 0.8379\n",
      "Epoch 26/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4697 - tp: 6318.0000 - fp: 1295.0000 - tn: 1073673.0000 - fn: 4820.0000 - accuracy: 0.9944 - precision: 0.8299 - recall: 0.5672 - auc: 0.8424 - val_loss: 0.3323 - val_tp: 6318.0000 - val_fp: 1328.6364 - val_tn: 1097962.5000 - val_fn: 4859.8184 - val_accuracy: 0.9944 - val_precision: 0.8262 - val_recall: 0.5652 - val_auc: 0.8429\n",
      "Epoch 27/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4654 - tp: 6656.0000 - fp: 1350.0000 - tn: 1116590.0000 - fn: 4900.0000 - accuracy: 0.9945 - precision: 0.8314 - recall: 0.5760 - auc: 0.8471 - val_loss: 0.3319 - val_tp: 6656.0000 - val_fp: 1383.0000 - val_tn: 1140880.1250 - val_fn: 4939.8184 - val_accuracy: 0.9945 - val_precision: 0.8280 - val_recall: 0.5740 - val_auc: 0.8474\n",
      "Epoch 28/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4631 - tp: 6994.0000 - fp: 1405.0000 - tn: 1159507.0000 - fn: 4980.0000 - accuracy: 0.9946 - precision: 0.8327 - recall: 0.5841 - auc: 0.8513 - val_loss: 0.3315 - val_tp: 6994.0000 - val_fp: 1438.7273 - val_tn: 1183796.3750 - val_fn: 5019.8184 - val_accuracy: 0.9946 - val_precision: 0.8294 - val_recall: 0.5822 - val_auc: 0.8515\n",
      "Epoch 29/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4609 - tp: 7332.0000 - fp: 1461.0000 - tn: 1202423.0000 - fn: 5060.0000 - accuracy: 0.9946 - precision: 0.8338 - recall: 0.5917 - auc: 0.8551 - val_loss: 0.3311 - val_tp: 7332.0000 - val_fp: 1495.9091 - val_tn: 1226711.1250 - val_fn: 5099.8184 - val_accuracy: 0.9947 - val_precision: 0.8306 - val_recall: 0.5898 - val_auc: 0.8551\n",
      "Epoch 30/100\n",
      "678/678 [==============================] - 2s 4ms/step - loss: 0.4586 - tp: 7670.0000 - fp: 1519.0000 - tn: 1245337.0000 - fn: 5140.0000 - accuracy: 0.9947 - precision: 0.8347 - recall: 0.5988 - auc: 0.8584 - val_loss: 0.3308 - val_tp: 7670.0000 - val_fp: 1556.5454 - val_tn: 1269622.5000 - val_fn: 5179.8184 - val_accuracy: 0.9948 - val_precision: 0.8313 - val_recall: 0.5969 - val_auc: 0.8584\n",
      "Epoch 31/100\n",
      "678/678 [==============================] - 2s 3ms/step - loss: 0.4564 - tp: 8008.0000 - fp: 1581.0000 - tn: 1288247.0000 - fn: 5220.0000 - accuracy: 0.9948 - precision: 0.8351 - recall: 0.6054 - auc: 0.8615 - val_loss: 0.3304 - val_tp: 8008.0000 - val_fp: 1619.1818 - val_tn: 1312531.8750 - val_fn: 5259.8184 - val_accuracy: 0.9948 - val_precision: 0.8318 - val_recall: 0.6036 - val_auc: 0.8614\n",
      "Epoch 32/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4541 - tp: 8346.0000 - fp: 1644.0000 - tn: 1331156.0000 - fn: 5300.0000 - accuracy: 0.9948 - precision: 0.8354 - recall: 0.6116 - auc: 0.8642 - val_loss: 0.3300 - val_tp: 8346.0000 - val_fp: 1683.9091 - val_tn: 1355439.1250 - val_fn: 5339.8184 - val_accuracy: 0.9949 - val_precision: 0.8321 - val_recall: 0.6098 - val_auc: 0.8641\n",
      "Epoch 33/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4519 - tp: 8684.0000 - fp: 1710.0000 - tn: 1374062.0000 - fn: 5380.0000 - accuracy: 0.9949 - precision: 0.8355 - recall: 0.6175 - auc: 0.8667 - val_loss: 0.3296 - val_tp: 8684.0000 - val_fp: 1750.5454 - val_tn: 1398344.5000 - val_fn: 5419.8184 - val_accuracy: 0.9949 - val_precision: 0.8322 - val_recall: 0.6157 - val_auc: 0.8665\n",
      "Epoch 34/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4504 - tp: 9022.0000 - fp: 1778.0000 - tn: 1416966.0000 - fn: 5460.0000 - accuracy: 0.9949 - precision: 0.8354 - recall: 0.6230 - auc: 0.8690 - val_loss: 0.3292 - val_tp: 9022.0000 - val_fp: 1820.1818 - val_tn: 1441246.8750 - val_fn: 5499.8184 - val_accuracy: 0.9950 - val_precision: 0.8321 - val_recall: 0.6213 - val_auc: 0.8688\n",
      "Epoch 35/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4474 - tp: 9360.0000 - fp: 1847.0000 - tn: 1459869.0000 - fn: 5540.0000 - accuracy: 0.9950 - precision: 0.8352 - recall: 0.6282 - auc: 0.8711 - val_loss: 0.3288 - val_tp: 9360.0000 - val_fp: 1891.1818 - val_tn: 1484147.8750 - val_fn: 5579.8184 - val_accuracy: 0.9950 - val_precision: 0.8319 - val_recall: 0.6265 - val_auc: 0.8708\n",
      "Epoch 36/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4442 - tp: 9699.0000 - fp: 1920.0000 - tn: 1502768.0000 - fn: 5619.0000 - accuracy: 0.9950 - precision: 0.8348 - recall: 0.6332 - auc: 0.8730 - val_loss: 0.3283 - val_tp: 9699.0000 - val_fp: 1964.9091 - val_tn: 1527046.1250 - val_fn: 5658.8184 - val_accuracy: 0.9951 - val_precision: 0.8315 - val_recall: 0.6315 - val_auc: 0.8727\n",
      "Epoch 37/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4420 - tp: 10038.0000 - fp: 1994.0000 - tn: 1545666.0000 - fn: 5698.0000 - accuracy: 0.9951 - precision: 0.8343 - recall: 0.6379 - auc: 0.8748 - val_loss: 0.3279 - val_tp: 10038.0000 - val_fp: 2039.3636 - val_tn: 1569943.6250 - val_fn: 5737.8184 - val_accuracy: 0.9951 - val_precision: 0.8311 - val_recall: 0.6363 - val_auc: 0.8745\n",
      "Epoch 38/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4398 - tp: 10377.0000 - fp: 2069.0000 - tn: 1588563.0000 - fn: 5777.0000 - accuracy: 0.9951 - precision: 0.8338 - recall: 0.6424 - auc: 0.8765 - val_loss: 0.3274 - val_tp: 10377.0000 - val_fp: 2115.2727 - val_tn: 1612839.8750 - val_fn: 5816.8184 - val_accuracy: 0.9951 - val_precision: 0.8307 - val_recall: 0.6408 - val_auc: 0.8761\n",
      "Epoch 39/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4376 - tp: 10716.0000 - fp: 2145.0000 - tn: 1631459.0000 - fn: 5856.0000 - accuracy: 0.9952 - precision: 0.8332 - recall: 0.6466 - auc: 0.8780 - val_loss: 0.3269 - val_tp: 10716.0000 - val_fp: 2191.2727 - val_tn: 1655735.8750 - val_fn: 5895.8184 - val_accuracy: 0.9952 - val_precision: 0.8302 - val_recall: 0.6451 - val_auc: 0.8776\n",
      "Epoch 40/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4355 - tp: 11055.0000 - fp: 2221.0000 - tn: 1674355.0000 - fn: 5935.0000 - accuracy: 0.9952 - precision: 0.8327 - recall: 0.6507 - auc: 0.8794 - val_loss: 0.3264 - val_tp: 11055.0000 - val_fp: 2268.0908 - val_tn: 1698630.8750 - val_fn: 5974.8184 - val_accuracy: 0.9952 - val_precision: 0.8298 - val_recall: 0.6492 - val_auc: 0.8790\n",
      "Epoch 41/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4333 - tp: 11394.0000 - fp: 2298.0000 - tn: 1717250.0000 - fn: 6014.0000 - accuracy: 0.9952 - precision: 0.8322 - recall: 0.6545 - auc: 0.8807 - val_loss: 0.3260 - val_tp: 11394.0000 - val_fp: 2345.0908 - val_tn: 1741526.0000 - val_fn: 6053.8184 - val_accuracy: 0.9952 - val_precision: 0.8293 - val_recall: 0.6530 - val_auc: 0.8804\n",
      "Epoch 42/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4312 - tp: 11733.0000 - fp: 2375.0000 - tn: 1760145.0000 - fn: 6093.0000 - accuracy: 0.9952 - precision: 0.8317 - recall: 0.6582 - auc: 0.8819 - val_loss: 0.3255 - val_tp: 11733.0000 - val_fp: 2422.5454 - val_tn: 1784420.5000 - val_fn: 6132.8184 - val_accuracy: 0.9953 - val_precision: 0.8289 - val_recall: 0.6567 - val_auc: 0.8816\n",
      "Epoch 43/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4298 - tp: 12071.0000 - fp: 2453.0000 - tn: 1803039.0000 - fn: 6173.0000 - accuracy: 0.9953 - precision: 0.8311 - recall: 0.6616 - auc: 0.8831 - val_loss: 0.3251 - val_tp: 12071.0000 - val_fp: 2508.9092 - val_tn: 1827306.1250 - val_fn: 6212.8184 - val_accuracy: 0.9953 - val_precision: 0.8279 - val_recall: 0.6602 - val_auc: 0.8827\n",
      "Epoch 44/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4270 - tp: 12410.0000 - fp: 2545.0000 - tn: 1845919.0000 - fn: 6252.0000 - accuracy: 0.9953 - precision: 0.8298 - recall: 0.6650 - auc: 0.8841 - val_loss: 0.3248 - val_tp: 12410.0000 - val_fp: 2610.9092 - val_tn: 1870176.3750 - val_fn: 6291.8184 - val_accuracy: 0.9953 - val_precision: 0.8262 - val_recall: 0.6636 - val_auc: 0.8837\n",
      "Epoch 45/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4238 - tp: 12750.0000 - fp: 2652.0000 - tn: 1888784.0000 - fn: 6330.0000 - accuracy: 0.9953 - precision: 0.8278 - recall: 0.6682 - auc: 0.8851 - val_loss: 0.3244 - val_tp: 12750.0000 - val_fp: 2724.7273 - val_tn: 1913034.5000 - val_fn: 6369.8184 - val_accuracy: 0.9953 - val_precision: 0.8239 - val_recall: 0.6668 - val_auc: 0.8847\n",
      "Epoch 46/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4237 - tp: 13089.0000 - fp: 2771.0000 - tn: 1931637.0000 - fn: 6409.0000 - accuracy: 0.9953 - precision: 0.8253 - recall: 0.6713 - auc: 0.8860 - val_loss: 0.3241 - val_tp: 13089.0000 - val_fp: 2853.9092 - val_tn: 1955877.2500 - val_fn: 6448.8184 - val_accuracy: 0.9953 - val_precision: 0.8210 - val_recall: 0.6699 - val_auc: 0.8856\n",
      "Epoch 47/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4205 - tp: 13429.0000 - fp: 2908.0000 - tn: 1974472.0000 - fn: 6487.0000 - accuracy: 0.9953 - precision: 0.8220 - recall: 0.6743 - auc: 0.8869 - val_loss: 0.3237 - val_tp: 13429.0000 - val_fp: 2997.4546 - val_tn: 1998705.8750 - val_fn: 6526.8184 - val_accuracy: 0.9953 - val_precision: 0.8175 - val_recall: 0.6729 - val_auc: 0.8864\n",
      "Epoch 48/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4176 - tp: 13769.0000 - fp: 3056.0000 - tn: 2017296.0000 - fn: 6565.0000 - accuracy: 0.9953 - precision: 0.8184 - recall: 0.6771 - auc: 0.8877 - val_loss: 0.3233 - val_tp: 13769.0000 - val_fp: 3151.8181 - val_tn: 2041523.2500 - val_fn: 6604.8184 - val_accuracy: 0.9953 - val_precision: 0.8137 - val_recall: 0.6758 - val_auc: 0.8873\n",
      "Epoch 49/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4165 - tp: 14109.0000 - fp: 3215.0000 - tn: 2060109.0000 - fn: 6643.0000 - accuracy: 0.9953 - precision: 0.8144 - recall: 0.6799 - auc: 0.8885 - val_loss: 0.3230 - val_tp: 14109.0000 - val_fp: 3319.8181 - val_tn: 2084327.2500 - val_fn: 6682.8184 - val_accuracy: 0.9953 - val_precision: 0.8095 - val_recall: 0.6786 - val_auc: 0.8880\n",
      "Epoch 50/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4143 - tp: 14449.0000 - fp: 3389.0000 - tn: 2102907.0000 - fn: 6721.0000 - accuracy: 0.9952 - precision: 0.8100 - recall: 0.6825 - auc: 0.8892 - val_loss: 0.3227 - val_tp: 14449.0000 - val_fp: 3508.4546 - val_tn: 2127110.5000 - val_fn: 6760.8184 - val_accuracy: 0.9952 - val_precision: 0.8046 - val_recall: 0.6812 - val_auc: 0.8887\n",
      "Epoch 51/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4126 - tp: 14789.0000 - fp: 3589.0000 - tn: 2145679.0000 - fn: 6799.0000 - accuracy: 0.9952 - precision: 0.8047 - recall: 0.6851 - auc: 0.8898 - val_loss: 0.3224 - val_tp: 14789.0000 - val_fp: 3719.2727 - val_tn: 2169872.0000 - val_fn: 6838.8184 - val_accuracy: 0.9952 - val_precision: 0.7991 - val_recall: 0.6838 - val_auc: 0.8894\n",
      "Epoch 52/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4113 - tp: 15130.0000 - fp: 3810.0000 - tn: 2188430.0000 - fn: 6876.0000 - accuracy: 0.9952 - precision: 0.7988 - recall: 0.6875 - auc: 0.8905 - val_loss: 0.3221 - val_tp: 15130.0000 - val_fp: 3951.6365 - val_tn: 2212611.7500 - val_fn: 6915.8184 - val_accuracy: 0.9951 - val_precision: 0.7929 - val_recall: 0.6863 - val_auc: 0.8900\n",
      "Epoch 53/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4103 - tp: 15471.0000 - fp: 4051.0000 - tn: 2231161.0000 - fn: 6953.0000 - accuracy: 0.9951 - precision: 0.7925 - recall: 0.6899 - auc: 0.8911 - val_loss: 0.3219 - val_tp: 15471.0000 - val_fp: 4205.0000 - val_tn: 2255330.0000 - val_fn: 6992.8184 - val_accuracy: 0.9951 - val_precision: 0.7863 - val_recall: 0.6887 - val_auc: 0.8906\n",
      "Epoch 54/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4074 - tp: 15812.0000 - fp: 4314.0000 - tn: 2273870.0000 - fn: 7030.0000 - accuracy: 0.9951 - precision: 0.7857 - recall: 0.6922 - auc: 0.8916 - val_loss: 0.3214 - val_tp: 15812.0000 - val_fp: 4474.0000 - val_tn: 2298033.0000 - val_fn: 7069.8184 - val_accuracy: 0.9950 - val_precision: 0.7795 - val_recall: 0.6910 - val_auc: 0.8912\n",
      "Epoch 55/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4049 - tp: 16154.0000 - fp: 4589.0000 - tn: 2316567.0000 - fn: 7106.0000 - accuracy: 0.9950 - precision: 0.7788 - recall: 0.6945 - auc: 0.8922 - val_loss: 0.3206 - val_tp: 16154.0000 - val_fp: 4741.3638 - val_tn: 2340737.7500 - val_fn: 7145.8184 - val_accuracy: 0.9950 - val_precision: 0.7731 - val_recall: 0.6933 - val_auc: 0.8917\n",
      "Epoch 56/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4034 - tp: 16495.0000 - fp: 4848.0000 - tn: 2359280.0000 - fn: 7183.0000 - accuracy: 0.9950 - precision: 0.7729 - recall: 0.6966 - auc: 0.8927 - val_loss: 0.3197 - val_tp: 16495.0000 - val_fp: 4994.6362 - val_tn: 2383456.2500 - val_fn: 7222.8184 - val_accuracy: 0.9949 - val_precision: 0.7676 - val_recall: 0.6955 - val_auc: 0.8923\n",
      "Epoch 57/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3988 - tp: 16837.0000 - fp: 5094.0000 - tn: 2402006.0000 - fn: 7259.0000 - accuracy: 0.9949 - precision: 0.7677 - recall: 0.6987 - auc: 0.8932 - val_loss: 0.3188 - val_tp: 16837.0000 - val_fp: 5233.8184 - val_tn: 2426189.5000 - val_fn: 7298.8184 - val_accuracy: 0.9949 - val_precision: 0.7629 - val_recall: 0.6976 - val_auc: 0.8928\n",
      "Epoch 58/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3968 - tp: 17179.0000 - fp: 5329.0000 - tn: 2444743.0000 - fn: 7335.0000 - accuracy: 0.9949 - precision: 0.7632 - recall: 0.7008 - auc: 0.8937 - val_loss: 0.3178 - val_tp: 17179.0000 - val_fp: 5458.8184 - val_tn: 2468936.2500 - val_fn: 7374.8184 - val_accuracy: 0.9949 - val_precision: 0.7589 - val_recall: 0.6996 - val_auc: 0.8933\n",
      "Epoch 59/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3951 - tp: 17521.0000 - fp: 5545.0000 - tn: 2487499.0000 - fn: 7411.0000 - accuracy: 0.9949 - precision: 0.7596 - recall: 0.7028 - auc: 0.8942 - val_loss: 0.3169 - val_tp: 17521.0000 - val_fp: 5669.3638 - val_tn: 2511697.7500 - val_fn: 7450.8184 - val_accuracy: 0.9948 - val_precision: 0.7555 - val_recall: 0.7016 - val_auc: 0.8937\n",
      "Epoch 60/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3920 - tp: 17863.0000 - fp: 5747.0000 - tn: 2530269.0000 - fn: 7487.0000 - accuracy: 0.9948 - precision: 0.7566 - recall: 0.7047 - auc: 0.8947 - val_loss: 0.3159 - val_tp: 17863.0000 - val_fp: 5864.8184 - val_tn: 2554474.2500 - val_fn: 7526.8184 - val_accuracy: 0.9948 - val_precision: 0.7528 - val_recall: 0.7036 - val_auc: 0.8942\n",
      "Epoch 61/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3902 - tp: 18205.0000 - fp: 5936.0000 - tn: 2573052.0000 - fn: 7563.0000 - accuracy: 0.9948 - precision: 0.7541 - recall: 0.7065 - auc: 0.8951 - val_loss: 0.3150 - val_tp: 18205.0000 - val_fp: 6047.9092 - val_tn: 2597263.2500 - val_fn: 7602.8184 - val_accuracy: 0.9948 - val_precision: 0.7506 - val_recall: 0.7054 - val_auc: 0.8946\n",
      "Epoch 62/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3890 - tp: 18546.0000 - fp: 6112.0000 - tn: 2615848.0000 - fn: 7640.0000 - accuracy: 0.9948 - precision: 0.7521 - recall: 0.7082 - auc: 0.8955 - val_loss: 0.3142 - val_tp: 18546.0000 - val_fp: 6221.3638 - val_tn: 2640061.7500 - val_fn: 7679.8184 - val_accuracy: 0.9948 - val_precision: 0.7488 - val_recall: 0.7072 - val_auc: 0.8951\n",
      "Epoch 63/100\n",
      "678/678 [==============================] - 2s 2ms/step - loss: 0.3865 - tp: 18888.0000 - fp: 6285.0000 - tn: 2658647.0000 - fn: 7716.0000 - accuracy: 0.9948 - precision: 0.7503 - recall: 0.7100 - auc: 0.8959 - val_loss: 0.3134 - val_tp: 18888.0000 - val_fp: 6392.7271 - val_tn: 2682862.2500 - val_fn: 7755.8184 - val_accuracy: 0.9948 - val_precision: 0.7471 - val_recall: 0.7089 - val_auc: 0.8955\n",
      "Epoch 64/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3847 - tp: 19230.0000 - fp: 6454.0000 - tn: 2701450.0000 - fn: 7792.0000 - accuracy: 0.9948 - precision: 0.7487 - recall: 0.7116 - auc: 0.8963 - val_loss: 0.3126 - val_tp: 19230.0000 - val_fp: 6561.1816 - val_tn: 2725665.7500 - val_fn: 7831.8184 - val_accuracy: 0.9948 - val_precision: 0.7456 - val_recall: 0.7106 - val_auc: 0.8959\n",
      "Epoch 65/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3841 - tp: 19571.0000 - fp: 6622.0000 - tn: 2744254.0000 - fn: 7869.0000 - accuracy: 0.9948 - precision: 0.7472 - recall: 0.7132 - auc: 0.8967 - val_loss: 0.3118 - val_tp: 19571.0000 - val_fp: 6726.7271 - val_tn: 2768472.2500 - val_fn: 7908.8184 - val_accuracy: 0.9948 - val_precision: 0.7442 - val_recall: 0.7122 - val_auc: 0.8963\n",
      "Epoch 66/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3813 - tp: 19913.0000 - fp: 6786.0000 - tn: 2787062.0000 - fn: 7945.0000 - accuracy: 0.9948 - precision: 0.7458 - recall: 0.7148 - auc: 0.8971 - val_loss: 0.3110 - val_tp: 19913.0000 - val_fp: 6889.7271 - val_tn: 2811281.2500 - val_fn: 7984.8184 - val_accuracy: 0.9948 - val_precision: 0.7429 - val_recall: 0.7138 - val_auc: 0.8966\n",
      "Epoch 67/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3796 - tp: 20255.0000 - fp: 6949.0000 - tn: 2829871.0000 - fn: 8021.0000 - accuracy: 0.9948 - precision: 0.7446 - recall: 0.7163 - auc: 0.8974 - val_loss: 0.3101 - val_tp: 20255.0000 - val_fp: 7052.4546 - val_tn: 2854090.5000 - val_fn: 8060.8184 - val_accuracy: 0.9948 - val_precision: 0.7417 - val_recall: 0.7153 - val_auc: 0.8970\n",
      "Epoch 68/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3767 - tp: 20598.0000 - fp: 7112.0000 - tn: 2872680.0000 - fn: 8096.0000 - accuracy: 0.9948 - precision: 0.7433 - recall: 0.7179 - auc: 0.8978 - val_loss: 0.3093 - val_tp: 20598.0000 - val_fp: 7214.0000 - val_tn: 2896900.7500 - val_fn: 8135.8184 - val_accuracy: 0.9948 - val_precision: 0.7406 - val_recall: 0.7169 - val_auc: 0.8973\n",
      "Epoch 69/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3751 - tp: 20941.0000 - fp: 7274.0000 - tn: 2915490.0000 - fn: 8171.0000 - accuracy: 0.9948 - precision: 0.7422 - recall: 0.7193 - auc: 0.8981 - val_loss: 0.3085 - val_tp: 20941.0000 - val_fp: 7374.0908 - val_tn: 2939712.7500 - val_fn: 8210.8184 - val_accuracy: 0.9948 - val_precision: 0.7396 - val_recall: 0.7183 - val_auc: 0.8977\n",
      "Epoch 70/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3744 - tp: 21283.0000 - fp: 7434.0000 - tn: 2958302.0000 - fn: 8247.0000 - accuracy: 0.9948 - precision: 0.7411 - recall: 0.7207 - auc: 0.8984 - val_loss: 0.3077 - val_tp: 21283.0000 - val_fp: 7528.7271 - val_tn: 2982530.2500 - val_fn: 8286.8184 - val_accuracy: 0.9948 - val_precision: 0.7387 - val_recall: 0.7198 - val_auc: 0.8980\n",
      "Epoch 71/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3715 - tp: 21625.0000 - fp: 7586.0000 - tn: 3001122.0000 - fn: 8323.0000 - accuracy: 0.9948 - precision: 0.7403 - recall: 0.7221 - auc: 0.8987 - val_loss: 0.3068 - val_tp: 21625.0000 - val_fp: 7677.6362 - val_tn: 3025353.5000 - val_fn: 8362.8184 - val_accuracy: 0.9948 - val_precision: 0.7380 - val_recall: 0.7211 - val_auc: 0.8983\n",
      "Epoch 72/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3704 - tp: 21967.0000 - fp: 7735.0000 - tn: 3043945.0000 - fn: 8399.0000 - accuracy: 0.9948 - precision: 0.7396 - recall: 0.7234 - auc: 0.8990 - val_loss: 0.3059 - val_tp: 21967.0000 - val_fp: 7825.0908 - val_tn: 3068177.7500 - val_fn: 8438.8184 - val_accuracy: 0.9948 - val_precision: 0.7373 - val_recall: 0.7225 - val_auc: 0.8986\n",
      "Epoch 73/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3675 - tp: 22310.0000 - fp: 7879.0000 - tn: 3086773.0000 - fn: 8474.0000 - accuracy: 0.9948 - precision: 0.7390 - recall: 0.7247 - auc: 0.8993 - val_loss: 0.3049 - val_tp: 22310.0000 - val_fp: 7962.0000 - val_tn: 3111013.0000 - val_fn: 8513.8184 - val_accuracy: 0.9948 - val_precision: 0.7370 - val_recall: 0.7238 - val_auc: 0.8989\n",
      "Epoch 74/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3654 - tp: 22653.0000 - fp: 8013.0000 - tn: 3129611.0000 - fn: 8549.0000 - accuracy: 0.9948 - precision: 0.7387 - recall: 0.7260 - auc: 0.8996 - val_loss: 0.3039 - val_tp: 22653.0000 - val_fp: 8092.7271 - val_tn: 3153854.5000 - val_fn: 8588.8184 - val_accuracy: 0.9948 - val_precision: 0.7368 - val_recall: 0.7251 - val_auc: 0.8992\n",
      "Epoch 75/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3661 - tp: 22995.0000 - fp: 8141.0000 - tn: 3172455.0000 - fn: 8625.0000 - accuracy: 0.9948 - precision: 0.7385 - recall: 0.7272 - auc: 0.8998 - val_loss: 0.3028 - val_tp: 22995.0000 - val_fp: 8211.5459 - val_tn: 3196707.2500 - val_fn: 8664.8184 - val_accuracy: 0.9948 - val_precision: 0.7369 - val_recall: 0.7263 - val_auc: 0.8994\n",
      "Epoch 76/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3629 - tp: 23338.0000 - fp: 8255.0000 - tn: 3215313.0000 - fn: 8700.0000 - accuracy: 0.9948 - precision: 0.7387 - recall: 0.7284 - auc: 0.9001 - val_loss: 0.3006 - val_tp: 23338.0000 - val_fp: 8286.5459 - val_tn: 3239604.2500 - val_fn: 8739.8184 - val_accuracy: 0.9948 - val_precision: 0.7380 - val_recall: 0.7275 - val_auc: 0.8997\n",
      "Epoch 77/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3603 - tp: 23681.0000 - fp: 8308.0000 - tn: 3258232.0000 - fn: 8775.0000 - accuracy: 0.9948 - precision: 0.7403 - recall: 0.7296 - auc: 0.9003 - val_loss: 0.2992 - val_tp: 23681.0000 - val_fp: 8321.0000 - val_tn: 3282541.7500 - val_fn: 8814.8184 - val_accuracy: 0.9948 - val_precision: 0.7400 - val_recall: 0.7287 - val_auc: 0.9000\n",
      "Epoch 78/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3588 - tp: 24023.0000 - fp: 8330.0000 - tn: 3301182.0000 - fn: 8851.0000 - accuracy: 0.9949 - precision: 0.7425 - recall: 0.7308 - auc: 0.9006 - val_loss: 0.2981 - val_tp: 24023.0000 - val_fp: 8337.2725 - val_tn: 3325497.7500 - val_fn: 8890.8184 - val_accuracy: 0.9949 - val_precision: 0.7424 - val_recall: 0.7299 - val_auc: 0.9002\n",
      "Epoch 79/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3736 - tp: 24354.0000 - fp: 8343.0000 - tn: 3344141.0000 - fn: 8938.0000 - accuracy: 0.9949 - precision: 0.7448 - recall: 0.7315 - auc: 0.9005 - val_loss: 0.2972 - val_tp: 24354.0000 - val_fp: 8347.3633 - val_tn: 3368459.7500 - val_fn: 8977.8184 - val_accuracy: 0.9949 - val_precision: 0.7447 - val_recall: 0.7307 - val_auc: 0.9002\n",
      "Epoch 80/100\n",
      "678/678 [==============================] - 2s 2ms/step - loss: 0.3829 - tp: 24677.0000 - fp: 8352.0000 - tn: 3387104.0000 - fn: 9033.0000 - accuracy: 0.9949 - precision: 0.7471 - recall: 0.7320 - auc: 0.9002 - val_loss: 0.2964 - val_tp: 24677.0000 - val_fp: 8354.5459 - val_tn: 3411424.2500 - val_fn: 9072.8184 - val_accuracy: 0.9950 - val_precision: 0.7471 - val_recall: 0.7312 - val_auc: 0.8999\n",
      "Epoch 81/100\n",
      "678/678 [==============================] - 2s 2ms/step - loss: 0.4115 - tp: 24979.0000 - fp: 8358.0000 - tn: 3430070.0000 - fn: 9149.0000 - accuracy: 0.9950 - precision: 0.7493 - recall: 0.7319 - auc: 0.8993 - val_loss: 0.2957 - val_tp: 24979.0000 - val_fp: 8359.8184 - val_tn: 3454391.2500 - val_fn: 9188.8184 - val_accuracy: 0.9950 - val_precision: 0.7492 - val_recall: 0.7311 - val_auc: 0.8990\n",
      "Epoch 82/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3785 - tp: 25303.0000 - fp: 8363.0000 - tn: 3473037.0000 - fn: 9243.0000 - accuracy: 0.9950 - precision: 0.7516 - recall: 0.7324 - auc: 0.8991 - val_loss: 0.2952 - val_tp: 25303.0000 - val_fp: 8366.6367 - val_tn: 3497356.2500 - val_fn: 9282.8184 - val_accuracy: 0.9950 - val_precision: 0.7515 - val_recall: 0.7316 - val_auc: 0.8987\n",
      "Epoch 83/100\n",
      "678/678 [==============================] - 2s 3ms/step - loss: 0.3521 - tp: 25645.0000 - fp: 8372.0000 - tn: 3516000.0000 - fn: 9319.0000 - accuracy: 0.9950 - precision: 0.7539 - recall: 0.7335 - auc: 0.8993 - val_loss: 0.2946 - val_tp: 25645.0000 - val_fp: 8377.1816 - val_tn: 3540317.7500 - val_fn: 9358.8184 - val_accuracy: 0.9951 - val_precision: 0.7538 - val_recall: 0.7326 - val_auc: 0.8990\n",
      "Epoch 84/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3491 - tp: 25988.0000 - fp: 8384.0000 - tn: 3558960.0000 - fn: 9394.0000 - accuracy: 0.9951 - precision: 0.7561 - recall: 0.7345 - auc: 0.8996 - val_loss: 0.2940 - val_tp: 25988.0000 - val_fp: 8391.0908 - val_tn: 3583276.2500 - val_fn: 9433.8184 - val_accuracy: 0.9951 - val_precision: 0.7559 - val_recall: 0.7337 - val_auc: 0.8992\n",
      "Epoch 85/100\n",
      "678/678 [==============================] - 2s 2ms/step - loss: 0.3487 - tp: 26330.0000 - fp: 8399.0000 - tn: 3601917.0000 - fn: 9470.0000 - accuracy: 0.9951 - precision: 0.7582 - recall: 0.7355 - auc: 0.8998 - val_loss: 0.2934 - val_tp: 26330.0000 - val_fp: 8408.4541 - val_tn: 3626230.5000 - val_fn: 9509.8184 - val_accuracy: 0.9951 - val_precision: 0.7579 - val_recall: 0.7347 - val_auc: 0.8995\n",
      "Epoch 86/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3460 - tp: 26673.0000 - fp: 8417.0000 - tn: 3644871.0000 - fn: 9545.0000 - accuracy: 0.9951 - precision: 0.7601 - recall: 0.7365 - auc: 0.9001 - val_loss: 0.2928 - val_tp: 26673.0000 - val_fp: 8432.1816 - val_tn: 3669178.5000 - val_fn: 9584.8184 - val_accuracy: 0.9951 - val_precision: 0.7598 - val_recall: 0.7356 - val_auc: 0.8997\n",
      "Epoch 87/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3444 - tp: 27016.0000 - fp: 8443.0000 - tn: 3687817.0000 - fn: 9620.0000 - accuracy: 0.9952 - precision: 0.7619 - recall: 0.7374 - auc: 0.9003 - val_loss: 0.2922 - val_tp: 27016.0000 - val_fp: 8461.9092 - val_tn: 3712121.0000 - val_fn: 9659.8184 - val_accuracy: 0.9952 - val_precision: 0.7615 - val_recall: 0.7366 - val_auc: 0.9000\n",
      "Epoch 88/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3444 - tp: 27358.0000 - fp: 8475.0000 - tn: 3730757.0000 - fn: 9696.0000 - accuracy: 0.9952 - precision: 0.7635 - recall: 0.7383 - auc: 0.9005 - val_loss: 0.2916 - val_tp: 27358.0000 - val_fp: 8501.0000 - val_tn: 3755053.7500 - val_fn: 9735.8184 - val_accuracy: 0.9952 - val_precision: 0.7629 - val_recall: 0.7375 - val_auc: 0.9002\n",
      "Epoch 89/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3428 - tp: 27700.0000 - fp: 8518.0000 - tn: 3773686.0000 - fn: 9772.0000 - accuracy: 0.9952 - precision: 0.7648 - recall: 0.7392 - auc: 0.9007 - val_loss: 0.2911 - val_tp: 27700.0000 - val_fp: 8552.8184 - val_tn: 3797974.2500 - val_fn: 9811.8184 - val_accuracy: 0.9952 - val_precision: 0.7641 - val_recall: 0.7384 - val_auc: 0.9004\n",
      "Epoch 90/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3400 - tp: 28043.0000 - fp: 8574.0000 - tn: 3816602.0000 - fn: 9847.0000 - accuracy: 0.9952 - precision: 0.7658 - recall: 0.7401 - auc: 0.9009 - val_loss: 0.2905 - val_tp: 28043.0000 - val_fp: 8615.0908 - val_tn: 3840883.7500 - val_fn: 9886.8184 - val_accuracy: 0.9952 - val_precision: 0.7650 - val_recall: 0.7393 - val_auc: 0.9006\n",
      "Epoch 91/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3385 - tp: 28386.0000 - fp: 8640.0000 - tn: 3859508.0000 - fn: 9922.0000 - accuracy: 0.9952 - precision: 0.7667 - recall: 0.7410 - auc: 0.9011 - val_loss: 0.2899 - val_tp: 28386.0000 - val_fp: 8687.8184 - val_tn: 3883783.2500 - val_fn: 9961.8184 - val_accuracy: 0.9953 - val_precision: 0.7657 - val_recall: 0.7402 - val_auc: 0.9008\n",
      "Epoch 92/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3371 - tp: 28729.0000 - fp: 8719.0000 - tn: 3902401.0000 - fn: 9997.0000 - accuracy: 0.9953 - precision: 0.7672 - recall: 0.7419 - auc: 0.9013 - val_loss: 0.2892 - val_tp: 28729.0000 - val_fp: 8769.8184 - val_tn: 3926673.5000 - val_fn: 10036.8184 - val_accuracy: 0.9953 - val_precision: 0.7661 - val_recall: 0.7411 - val_auc: 0.9010\n",
      "Epoch 93/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3355 - tp: 29072.0000 - fp: 8803.0000 - tn: 3945289.0000 - fn: 10072.0000 - accuracy: 0.9953 - precision: 0.7676 - recall: 0.7427 - auc: 0.9015 - val_loss: 0.2884 - val_tp: 29072.0000 - val_fp: 8857.2725 - val_tn: 3969557.7500 - val_fn: 10111.8184 - val_accuracy: 0.9953 - val_precision: 0.7665 - val_recall: 0.7419 - val_auc: 0.9012\n",
      "Epoch 94/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3346 - tp: 29415.0000 - fp: 8893.0000 - tn: 3988171.0000 - fn: 10147.0000 - accuracy: 0.9953 - precision: 0.7679 - recall: 0.7435 - auc: 0.9017 - val_loss: 0.2878 - val_tp: 29415.0000 - val_fp: 8955.4541 - val_tn: 4012431.7500 - val_fn: 10186.8184 - val_accuracy: 0.9953 - val_precision: 0.7666 - val_recall: 0.7428 - val_auc: 0.9014\n",
      "Epoch 95/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3331 - tp: 29757.0000 - fp: 8997.0000 - tn: 4031039.0000 - fn: 10223.0000 - accuracy: 0.9953 - precision: 0.7678 - recall: 0.7443 - auc: 0.9019 - val_loss: 0.2871 - val_tp: 29757.0000 - val_fp: 9064.8184 - val_tn: 4055293.7500 - val_fn: 10262.8184 - val_accuracy: 0.9953 - val_precision: 0.7665 - val_recall: 0.7436 - val_auc: 0.9015\n",
      "Epoch 96/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3312 - tp: 30100.0000 - fp: 9112.0000 - tn: 4073896.0000 - fn: 10298.0000 - accuracy: 0.9953 - precision: 0.7676 - recall: 0.7451 - auc: 0.9020 - val_loss: 0.2863 - val_tp: 30100.0000 - val_fp: 9182.7275 - val_tn: 4098148.2500 - val_fn: 10337.8184 - val_accuracy: 0.9953 - val_precision: 0.7662 - val_recall: 0.7444 - val_auc: 0.9017\n",
      "Epoch 97/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3297 - tp: 30443.0000 - fp: 9232.0000 - tn: 4116748.0000 - fn: 10373.0000 - accuracy: 0.9953 - precision: 0.7673 - recall: 0.7459 - auc: 0.9022 - val_loss: 0.2856 - val_tp: 30443.0000 - val_fp: 9304.8184 - val_tn: 4140998.2500 - val_fn: 10412.8184 - val_accuracy: 0.9953 - val_precision: 0.7659 - val_recall: 0.7451 - val_auc: 0.9019\n",
      "Epoch 98/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3283 - tp: 30786.0000 - fp: 9357.0000 - tn: 4159595.0000 - fn: 10448.0000 - accuracy: 0.9953 - precision: 0.7669 - recall: 0.7466 - auc: 0.9023 - val_loss: 0.2847 - val_tp: 30786.0000 - val_fp: 9431.2725 - val_tn: 4183843.7500 - val_fn: 10487.8184 - val_accuracy: 0.9953 - val_precision: 0.7655 - val_recall: 0.7459 - val_auc: 0.9020\n",
      "Epoch 99/100\n",
      "678/678 [==============================] - 2s 3ms/step - loss: 0.3269 - tp: 31129.0000 - fp: 9486.0000 - tn: 4202438.0000 - fn: 10523.0000 - accuracy: 0.9953 - precision: 0.7664 - recall: 0.7474 - auc: 0.9025 - val_loss: 0.2839 - val_tp: 31129.0000 - val_fp: 9564.3633 - val_tn: 4226683.0000 - val_fn: 10562.8184 - val_accuracy: 0.9953 - val_precision: 0.7650 - val_recall: 0.7466 - val_auc: 0.9022\n",
      "Epoch 100/100\n",
      "678/678 [==============================] - 2s 2ms/step - loss: 0.3256 - tp: 31472.0000 - fp: 9620.0000 - tn: 4245276.0000 - fn: 10598.0000 - accuracy: 0.9953 - precision: 0.7659 - recall: 0.7481 - auc: 0.9026 - val_loss: 0.2831 - val_tp: 31472.0000 - val_fp: 9702.8184 - val_tn: 4269516.0000 - val_fn: 10637.8184 - val_accuracy: 0.9953 - val_precision: 0.7644 - val_recall: 0.7474 - val_auc: 0.9023\n",
      "TRAIN | AUC Score: 0.998546511627907\n",
      "TEST | AUC Score: 0.49820578370898516\n",
      "error\n",
      "error\n",
      "Confusion Matrix:\n",
      "false positive pct: 0.3582215354358362\n",
      "tn  fp  fn  tp\n",
      "42484 153 74 0\n",
      "[[42484   153]\n",
      " [   74     0]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     42637\n",
      "           1       0.00      0.00      0.00        74\n",
      "\n",
      "    accuracy                           0.99     42711\n",
      "   macro avg       0.50      0.50      0.50     42711\n",
      "weighted avg       1.00      0.99      1.00     42711\n",
      "\n",
      "Specificity = 0.9964115674179703\n",
      "Sensitivity = 0.0\n",
      "lr 0.01 w1 1.0 w2 1.0 epochs 100 batch_size 4096\n"
     ]
    }
   ],
   "source": [
    "# run model\n",
    "METRICS = [\n",
    "      keras.metrics.TruePositives(name='tp'),\n",
    "      keras.metrics.FalsePositives(name='fp'),\n",
    "      keras.metrics.TrueNegatives(name='tn'),\n",
    "      keras.metrics.FalseNegatives(name='fn'), \n",
    "      keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      keras.metrics.Precision(name='precision'),\n",
    "      keras.metrics.Recall(name='recall'),\n",
    "      keras.metrics.AUC(name='auc'),\n",
    "]\n",
    "### Hyperparameters Tuning\n",
    "# First test optimal epochs holding everything else constant\n",
    "# Dropout: 0.1-0.6\n",
    "# GradientClipping: 0.1-10\n",
    "# BatchSize: 32,64,128,256,512 (power of 2)\n",
    "\n",
    "# setting variables\n",
    "lr = 0.025\n",
    "w1 = 4.0\n",
    "w2 = 1.0\n",
    "epochs = 100 # original was 200, also saw 100\n",
    "batch_size = 4096 # original was 10000, also saw 2048\n",
    "min_ = 0\n",
    "max_ = 2\n",
    "num_iter = 1\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_auc', \n",
    "    verbose=1,\n",
    "    patience=10,\n",
    "    mode='max',\n",
    "    restore_best_weights=True)\n",
    "\n",
    "for w1 in np.linspace(1, 4, num = 1):\n",
    "    for w2 in np.linspace(1, 4, num = 1):\n",
    "        # keep lr constant at 0.01\n",
    "        for lr in np.linspace(.01, .01, num=1):\n",
    "            # repeat same settings for num of interations\n",
    "            for n in np.linspace(0, 3, num=num_iter):\n",
    "                ### Train LSTM using Keras 2 API ###\n",
    "                model = Sequential()\n",
    "                #model.add(LSTM(20, return_sequences=True, input_shape=X_train.shape[1:], kernel_initializer='lecun_uniform', activation='relu', kernel_regularizer=regularizers.l1(0.1), recurrent_regularizer=regularizers.l1(0.01), bias_regularizer=None, activity_regularizer=None, dropout=0.2, recurrent_dropout=0.2))#, return_sequences=True))\n",
    "                #model.add(LSTM(20, return_sequences=True, input_shape=X_train.shape[1:], activation='relu', kernel_regularizer=regularizers.l1(0.1), recurrent_regularizer=regularizers.l1(0.01), bias_regularizer=None, activity_regularizer=None, dropout=0.2, recurrent_dropout=0.2))#, return_sequences=True))\n",
    "                # defaults seem to work best, otherwise get very low score, AUC of 0.5 and all 0 or all 1 predictions\n",
    "                model.add(LSTM(20, return_sequences=True, input_shape=X_train.shape[1:]))\n",
    "                model.add(Dropout(0.2))\n",
    "                model.add(LSTM(12, activation='relu', return_sequences=True))\n",
    "                model.add(Dropout(0.2))\n",
    "                model.add(LSTM(8, activation='relu', return_sequences=False)) # this is the last LSTM, so should return_sequences=False\n",
    "                #model.add(Dense(1, kernel_initializer='lecun_uniform', activation='sigmoid'))\n",
    "                model.add(Dense(units=1, kernel_initializer='uniform', activation='sigmoid'))\n",
    "                # from https://www.tensorflow.org/tutorials/structured_data/imbalanced_data\n",
    "                bias_initializer = ones/zeros # think this is just the ratio of minority/total, and so far the log() makes this worse\n",
    "                #bias_initializer = np.log([bias_initializer])\n",
    "                print('bias_initializer', bias_initializer)\n",
    "                bias_initializer = initializers.Constant(bias_initializer)\n",
    "                model.add(Dense(units=1, activation='sigmoid', bias_initializer=bias_initializer))\n",
    "                #model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "                #optimizer = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False) # ValueError: None values not supported.\n",
    "\n",
    "                optimizer = optimizers.Adam(lr=lr)\n",
    "\n",
    "                #model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "                #model.compile(loss='mean_squared_error', optimizer=adam)\n",
    "                #model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy']) #optimizer='rmsprop', optimizer='sgd', optimizer='adam'\n",
    "\n",
    "                a='''\n",
    "                lr_schedule = optimizers.schedules.ExponentialDecay(\n",
    "                    initial_learning_rate=1e-2,\n",
    "                    decay_steps=10000,\n",
    "                    decay_rate=0.9)\n",
    "                optimizer = optimizers.SGD(learning_rate=lr_schedule)'''\n",
    "\n",
    "                #optimizer = optimizers.SGD(learning_rate=lr)\n",
    "                #optimizer = optimizers.SGD(learning_rate=0.001, momentum=0.9,decay=0.01)\n",
    "\n",
    "                model.compile(loss='binary_crossentropy', optimizer=optimizer, \n",
    "                              metrics=METRICS) #optimizer='rmsprop', optimizer='sgd', optimizer='adam'\n",
    "                \n",
    "                results = model.evaluate(X_train, y_train, batch_size=batch_size, verbose=0)\n",
    "                print(\"Initial Loss: {:0.4f}\".format(results[0]))\n",
    "                print(model.metrics_names)\n",
    "                print(results)\n",
    "                #print(model.summary())\n",
    "                \n",
    "                lrs = LearningRateScheduler(my_learning_rate)\n",
    "                #model.fit(..., callbacks=[lrs])\n",
    "\n",
    "                #or\n",
    "\n",
    "                rlrop = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=100)\n",
    "                # TypeError: unsupported operand type(s) for +: 'ReduceLROnPlateau' and 'list'\n",
    "                #model.fit(..., callbacks=[rlrop])\n",
    "\n",
    "\n",
    "                #model.fit(X_train, y_train, epochs=200, batch_size=10000, class_weight={0 : 1., 1: float(int(1/np.mean(y_train)))}, validation_split=0.3)\n",
    "                #model.fit(X_train, y_train, epochs=4, batch_size=8)#, class_weight=np.where(y_train == 1,4.0,1.0).flatten() )\n",
    "                \n",
    "                #model_output = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, class_weight=np.where(y_train == 1,w1,w2).flatten() )\n",
    "                \n",
    "                model_output = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=1, callbacks=[rlrop],\n",
    "                                         class_weight=np.where(y_train == 1,w1,w2).flatten(), validation_data=(val_features, val_labels) )\n",
    "\n",
    "                train_predict = model.predict_classes(X_train)\n",
    "                test_predict = model.predict_classes(X_test)\n",
    "                \n",
    "                ### test AUC ###\n",
    "\n",
    "                fpr, tpr, thresholds = metrics.roc_curve(y_train, train_predict, pos_label=1)\n",
    "                print('TRAIN | AUC Score: ' + str((metrics.auc(fpr, tpr))))\n",
    "                fpr, tpr, thresholds = metrics.roc_curve(y_test, test_predict, pos_label=1)\n",
    "                print('TEST | AUC Score: ' + str((metrics.auc(fpr, tpr))))\n",
    "                tn, fp, fn, tp = display_metrics(model, X_train, X_test, y_train, y_test, test_predict)\n",
    "                cm_results.append([model.name, tn, fp, fn, tp, lr, w1, w2, epochs, batch_size])\n",
    "                print('lr',lr,'w1',w1,'w2',w2,'epochs',epochs,'batch_size',batch_size)\n",
    "                model.reset_states()\n",
    "                del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss(model_output, \"Plot Loss\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True 0.172749 0.172549 50.0 0.173217\n"
     ]
    }
   ],
   "source": [
    "final_results = pd.DataFrame(cm_results, columns=('algo','TN','FP','FN','TP', 'lr', 'w1', 'w2', 'epochs', 'batch_size')) \n",
    "#sp = round((tn1 + tn2)/(tn1 + tn2 +fp2), 3)\n",
    "#se = round(tp2/(tp2 + fn1 + fn2), 3)\n",
    "final_results['SP'] = round(final_results['TN']/(final_results['TN'] + final_results['FP']), 3)\n",
    "final_results['SE'] = round(final_results['TP']/(final_results['TP'] + final_results['FN']), 3)\n",
    "final_results['Avg'] = (final_results['SP'] + final_results['SE'])/2\n",
    "\n",
    "print(strat,OrigPct,TrainPct,TrainDownPct,TestPct)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           algo     TN   FP  FN  TP    lr   w1   w2  epochs  batch_size  \\\n",
      "0  sequential_1  42484  153  74   0  0.01  1.0  1.0     100        4096   \n",
      "\n",
      "      SP   SE    Avg  \n",
      "0  0.996  0.0  0.498  \n"
     ]
    }
   ],
   "source": [
    "sort = final_results.sort_values(final_results.columns[5], ascending = False)\n",
    "sort.to_csv('results_lstm.csv', sep=',', mode='a', encoding='utf-8', header=True)\n",
    "print(sort)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bias_initializer 0.0017284695005527083\n",
      "Train on 678 samples, validate on 42712 samples\n",
      "Epoch 1/200\n",
      "678/678 [==============================] - 3s 4ms/step - loss: 0.7376 - accuracy: 0.4926 - val_loss: 0.4545 - val_accuracy: 0.9983\n",
      "Epoch 2/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.7338 - accuracy: 0.4926 - val_loss: 0.4611 - val_accuracy: 0.9983\n",
      "Epoch 3/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.7283 - accuracy: 0.4926 - val_loss: 0.4676 - val_accuracy: 0.9983\n",
      "Epoch 4/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.7184 - accuracy: 0.4926 - val_loss: 0.4741 - val_accuracy: 0.9983\n",
      "Epoch 5/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.6955 - accuracy: 0.4926 - val_loss: 0.4802 - val_accuracy: 0.9983\n",
      "Epoch 6/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.6510 - accuracy: 0.4926 - val_loss: 0.4860 - val_accuracy: 0.9983\n",
      "Epoch 7/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.6027 - accuracy: 0.7699 - val_loss: 0.4905 - val_accuracy: 0.9983\n",
      "Epoch 8/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.5811 - accuracy: 0.9661 - val_loss: 0.4937 - val_accuracy: 0.9983\n",
      "Epoch 9/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.5776 - accuracy: 0.9882 - val_loss: 0.4959 - val_accuracy: 0.9978\n",
      "Epoch 10/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.5767 - accuracy: 0.9897 - val_loss: 0.4972 - val_accuracy: 0.9971\n",
      "Epoch 11/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.5749 - accuracy: 0.9912 - val_loss: 0.4975 - val_accuracy: 0.9965\n",
      "Epoch 12/200\n",
      "678/678 [==============================] - 2s 2ms/step - loss: 0.5734 - accuracy: 0.9897 - val_loss: 0.4969 - val_accuracy: 0.9959\n",
      "Epoch 13/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.5711 - accuracy: 0.9897 - val_loss: 0.4947 - val_accuracy: 0.9953\n",
      "Epoch 14/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.5681 - accuracy: 0.9897 - val_loss: 0.4899 - val_accuracy: 0.9948\n",
      "Epoch 15/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.5639 - accuracy: 0.9897 - val_loss: 0.4796 - val_accuracy: 0.9946\n",
      "Epoch 16/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.5570 - accuracy: 0.9882 - val_loss: 0.4570 - val_accuracy: 0.9946\n",
      "Epoch 17/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.5432 - accuracy: 0.9912 - val_loss: 0.4093 - val_accuracy: 0.9947\n",
      "Epoch 18/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.5196 - accuracy: 0.9912 - val_loss: 0.3428 - val_accuracy: 0.9954\n",
      "Epoch 19/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4869 - accuracy: 0.9926 - val_loss: 0.3169 - val_accuracy: 0.9960\n",
      "Epoch 20/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4717 - accuracy: 0.9926 - val_loss: 0.3154 - val_accuracy: 0.9963\n",
      "Epoch 21/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4691 - accuracy: 0.9912 - val_loss: 0.3148 - val_accuracy: 0.9966\n",
      "Epoch 22/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4675 - accuracy: 0.9912 - val_loss: 0.3143 - val_accuracy: 0.9967\n",
      "Epoch 23/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.4653 - accuracy: 0.9912 - val_loss: 0.3140 - val_accuracy: 0.9967\n",
      "Epoch 24/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.4631 - accuracy: 0.9912 - val_loss: 0.3136 - val_accuracy: 0.9967\n",
      "Epoch 25/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4609 - accuracy: 0.9912 - val_loss: 0.3132 - val_accuracy: 0.9967\n",
      "Epoch 26/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4587 - accuracy: 0.9912 - val_loss: 0.3129 - val_accuracy: 0.9966\n",
      "Epoch 27/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.4575 - accuracy: 0.9897 - val_loss: 0.3127 - val_accuracy: 0.9964\n",
      "Epoch 28/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.4541 - accuracy: 0.9912 - val_loss: 0.3124 - val_accuracy: 0.9963\n",
      "Epoch 29/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.4519 - accuracy: 0.9912 - val_loss: 0.3122 - val_accuracy: 0.9960\n",
      "Epoch 30/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.4517 - accuracy: 0.9867 - val_loss: 0.6474 - val_accuracy: 0.3764\n",
      "Epoch 31/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.5887 - accuracy: 0.7198 - val_loss: 0.3118 - val_accuracy: 0.9953\n",
      "Epoch 32/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4460 - accuracy: 0.9897 - val_loss: 0.3109 - val_accuracy: 0.9963\n",
      "Epoch 33/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4430 - accuracy: 0.9912 - val_loss: 0.3105 - val_accuracy: 0.9963\n",
      "Epoch 34/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4420 - accuracy: 0.9897 - val_loss: 0.3100 - val_accuracy: 0.9964\n",
      "Epoch 35/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.4399 - accuracy: 0.9897 - val_loss: 0.3096 - val_accuracy: 0.9964\n",
      "Epoch 36/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.4400 - accuracy: 0.9867 - val_loss: 0.3092 - val_accuracy: 0.9964\n",
      "Epoch 37/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.4368 - accuracy: 0.9882 - val_loss: 0.3088 - val_accuracy: 0.9965\n",
      "Epoch 38/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.4381 - accuracy: 0.9838 - val_loss: 0.3085 - val_accuracy: 0.9965\n",
      "Epoch 39/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.4384 - accuracy: 0.9808 - val_loss: 0.3082 - val_accuracy: 0.9963\n",
      "Epoch 40/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.4351 - accuracy: 0.9823 - val_loss: 0.3079 - val_accuracy: 0.9963\n",
      "Epoch 41/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.4342 - accuracy: 0.9808 - val_loss: 0.3076 - val_accuracy: 0.9962\n",
      "Epoch 42/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4250 - accuracy: 0.9897 - val_loss: 0.3072 - val_accuracy: 0.9962\n",
      "Epoch 43/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4371 - accuracy: 0.9720 - val_loss: 0.3067 - val_accuracy: 0.9966\n",
      "Epoch 44/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4219 - accuracy: 0.9882 - val_loss: 0.3063 - val_accuracy: 0.9967\n",
      "Epoch 45/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.4188 - accuracy: 0.9897 - val_loss: 0.3058 - val_accuracy: 0.9969\n",
      "Epoch 46/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.4180 - accuracy: 0.9882 - val_loss: 0.3054 - val_accuracy: 0.9970\n",
      "Epoch 47/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.4160 - accuracy: 0.9882 - val_loss: 0.3049 - val_accuracy: 0.9972\n",
      "Epoch 48/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4139 - accuracy: 0.9882 - val_loss: 0.3045 - val_accuracy: 0.9972\n",
      "Epoch 49/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.4107 - accuracy: 0.9897 - val_loss: 0.3040 - val_accuracy: 0.9972\n",
      "Epoch 50/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.4078 - accuracy: 0.9912 - val_loss: 0.3035 - val_accuracy: 0.9974\n",
      "Epoch 51/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.4070 - accuracy: 0.9897 - val_loss: 0.3031 - val_accuracy: 0.9974\n",
      "Epoch 52/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4051 - accuracy: 0.9897 - val_loss: 0.3026 - val_accuracy: 0.9974\n",
      "Epoch 53/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4031 - accuracy: 0.9897 - val_loss: 0.3022 - val_accuracy: 0.9974\n",
      "Epoch 54/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.4009 - accuracy: 0.9897 - val_loss: 0.3017 - val_accuracy: 0.9974\n",
      "Epoch 55/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.3980 - accuracy: 0.9912 - val_loss: 0.3011 - val_accuracy: 0.9975\n",
      "Epoch 56/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.3961 - accuracy: 0.9912 - val_loss: 0.3007 - val_accuracy: 0.9975\n",
      "Epoch 57/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.3955 - accuracy: 0.9897 - val_loss: 0.3001 - val_accuracy: 0.9975\n",
      "Epoch 58/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.3923 - accuracy: 0.9912 - val_loss: 0.2996 - val_accuracy: 0.9975\n",
      "Epoch 59/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3904 - accuracy: 0.9912 - val_loss: 0.2990 - val_accuracy: 0.9975\n",
      "Epoch 60/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.3886 - accuracy: 0.9912 - val_loss: 0.2985 - val_accuracy: 0.9975\n",
      "Epoch 61/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.3867 - accuracy: 0.9912 - val_loss: 0.2979 - val_accuracy: 0.9975\n",
      "Epoch 62/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3849 - accuracy: 0.9912 - val_loss: 0.2973 - val_accuracy: 0.9976\n",
      "Epoch 63/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3845 - accuracy: 0.9897 - val_loss: 0.2967 - val_accuracy: 0.9976\n",
      "Epoch 64/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3813 - accuracy: 0.9912 - val_loss: 0.2961 - val_accuracy: 0.9976\n",
      "Epoch 65/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.3795 - accuracy: 0.9912 - val_loss: 0.2955 - val_accuracy: 0.9976\n",
      "Epoch 66/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.3778 - accuracy: 0.9912 - val_loss: 0.2949 - val_accuracy: 0.9976\n",
      "Epoch 67/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3760 - accuracy: 0.9912 - val_loss: 0.2943 - val_accuracy: 0.9976\n",
      "Epoch 68/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3743 - accuracy: 0.9912 - val_loss: 0.2936 - val_accuracy: 0.9976\n",
      "Epoch 69/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3726 - accuracy: 0.9912 - val_loss: 0.2930 - val_accuracy: 0.9976\n",
      "Epoch 70/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.3695 - accuracy: 0.9926 - val_loss: 0.2923 - val_accuracy: 0.9976\n",
      "Epoch 71/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3692 - accuracy: 0.9912 - val_loss: 0.2916 - val_accuracy: 0.9976\n",
      "Epoch 72/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3675 - accuracy: 0.9912 - val_loss: 0.2909 - val_accuracy: 0.9976\n",
      "Epoch 73/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3658 - accuracy: 0.9912 - val_loss: 0.2903 - val_accuracy: 0.9976\n",
      "Epoch 74/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.3642 - accuracy: 0.9912 - val_loss: 0.2896 - val_accuracy: 0.9976\n",
      "Epoch 75/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.3625 - accuracy: 0.9912 - val_loss: 0.2888 - val_accuracy: 0.9977\n",
      "Epoch 76/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.3609 - accuracy: 0.9912 - val_loss: 0.2881 - val_accuracy: 0.9977\n",
      "Epoch 77/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.3578 - accuracy: 0.9926 - val_loss: 0.2874 - val_accuracy: 0.9977\n",
      "Epoch 78/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.3577 - accuracy: 0.9912 - val_loss: 0.2867 - val_accuracy: 0.9977\n",
      "Epoch 79/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.3561 - accuracy: 0.9912 - val_loss: 0.2859 - val_accuracy: 0.9977\n",
      "Epoch 80/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3545 - accuracy: 0.9912 - val_loss: 0.2852 - val_accuracy: 0.9977\n",
      "Epoch 81/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3529 - accuracy: 0.9912 - val_loss: 0.2845 - val_accuracy: 0.9977\n",
      "Epoch 82/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.3514 - accuracy: 0.9912 - val_loss: 0.2837 - val_accuracy: 0.9977\n",
      "Epoch 83/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.3498 - accuracy: 0.9912 - val_loss: 0.2829 - val_accuracy: 0.9977\n",
      "Epoch 84/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.3483 - accuracy: 0.9912 - val_loss: 0.2822 - val_accuracy: 0.9977\n",
      "Epoch 85/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.3454 - accuracy: 0.9926 - val_loss: 0.2814 - val_accuracy: 0.9977\n",
      "Epoch 86/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.3453 - accuracy: 0.9912 - val_loss: 0.2806 - val_accuracy: 0.9977\n",
      "Epoch 87/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.3438 - accuracy: 0.9912 - val_loss: 0.2799 - val_accuracy: 0.9976\n",
      "Epoch 88/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.3423 - accuracy: 0.9912 - val_loss: 0.2792 - val_accuracy: 0.9976\n",
      "Epoch 89/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.3408 - accuracy: 0.9912 - val_loss: 0.2784 - val_accuracy: 0.9976\n",
      "Epoch 90/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.3393 - accuracy: 0.9912 - val_loss: 0.2776 - val_accuracy: 0.9975\n",
      "Epoch 91/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.3364 - accuracy: 0.9926 - val_loss: 0.2769 - val_accuracy: 0.9975\n",
      "Epoch 92/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3349 - accuracy: 0.9926 - val_loss: 0.2761 - val_accuracy: 0.9974\n",
      "Epoch 93/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3335 - accuracy: 0.9926 - val_loss: 0.2754 - val_accuracy: 0.9974\n",
      "Epoch 94/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3321 - accuracy: 0.9926 - val_loss: 0.2746 - val_accuracy: 0.9973\n",
      "Epoch 95/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.3307 - accuracy: 0.9926 - val_loss: 0.2738 - val_accuracy: 0.9973\n",
      "Epoch 96/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.3306 - accuracy: 0.9912 - val_loss: 0.2730 - val_accuracy: 0.9973\n",
      "Epoch 97/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.3264 - accuracy: 0.9941 - val_loss: 0.2722 - val_accuracy: 0.9973\n",
      "Epoch 98/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.3278 - accuracy: 0.9912 - val_loss: 0.2714 - val_accuracy: 0.9973\n",
      "Epoch 99/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3251 - accuracy: 0.9926 - val_loss: 0.2705 - val_accuracy: 0.9973\n",
      "Epoch 100/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.3237 - accuracy: 0.9926 - val_loss: 0.2697 - val_accuracy: 0.9972\n",
      "Epoch 101/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.3208 - accuracy: 0.9941 - val_loss: 0.2689 - val_accuracy: 0.9972\n",
      "Epoch 102/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3224 - accuracy: 0.9912 - val_loss: 0.2681 - val_accuracy: 0.9972\n",
      "Epoch 103/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3197 - accuracy: 0.9926 - val_loss: 0.2673 - val_accuracy: 0.9971\n",
      "Epoch 104/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3181 - accuracy: 0.9926 - val_loss: 0.2665 - val_accuracy: 0.9971\n",
      "Epoch 105/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3168 - accuracy: 0.9926 - val_loss: 0.2656 - val_accuracy: 0.9971\n",
      "Epoch 106/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3140 - accuracy: 0.9941 - val_loss: 0.2648 - val_accuracy: 0.9970\n",
      "Epoch 107/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3141 - accuracy: 0.9926 - val_loss: 0.2640 - val_accuracy: 0.9970\n",
      "Epoch 108/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3128 - accuracy: 0.9926 - val_loss: 0.2631 - val_accuracy: 0.9970\n",
      "Epoch 109/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3101 - accuracy: 0.9941 - val_loss: 0.2623 - val_accuracy: 0.9970\n",
      "Epoch 110/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3103 - accuracy: 0.9926 - val_loss: 0.2615 - val_accuracy: 0.9969\n",
      "Epoch 111/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3090 - accuracy: 0.9926 - val_loss: 0.2606 - val_accuracy: 0.9969\n",
      "Epoch 112/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.3077 - accuracy: 0.9926 - val_loss: 0.2597 - val_accuracy: 0.9969\n",
      "Epoch 113/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.3050 - accuracy: 0.9941 - val_loss: 0.2588 - val_accuracy: 0.9969\n",
      "Epoch 114/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.3052 - accuracy: 0.9926 - val_loss: 0.2580 - val_accuracy: 0.9969\n",
      "Epoch 115/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.3040 - accuracy: 0.9926 - val_loss: 0.2571 - val_accuracy: 0.9969\n",
      "Epoch 116/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.3027 - accuracy: 0.9926 - val_loss: 0.2562 - val_accuracy: 0.9969\n",
      "Epoch 117/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3015 - accuracy: 0.9926 - val_loss: 0.2554 - val_accuracy: 0.9969\n",
      "Epoch 118/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.3003 - accuracy: 0.9926 - val_loss: 0.2545 - val_accuracy: 0.9969\n",
      "Epoch 119/200\n",
      "678/678 [==============================] - 2s 2ms/step - loss: 0.2991 - accuracy: 0.9926 - val_loss: 0.2536 - val_accuracy: 0.9969\n",
      "Epoch 120/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.2979 - accuracy: 0.9926 - val_loss: 0.2528 - val_accuracy: 0.9969\n",
      "Epoch 121/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.2967 - accuracy: 0.9926 - val_loss: 0.2519 - val_accuracy: 0.9969\n",
      "Epoch 122/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.2955 - accuracy: 0.9926 - val_loss: 0.2511 - val_accuracy: 0.9969\n",
      "Epoch 123/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.2911 - accuracy: 0.9956 - val_loss: 0.2500 - val_accuracy: 0.9971\n",
      "Epoch 124/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.2932 - accuracy: 0.9926 - val_loss: 0.2489 - val_accuracy: 0.9973\n",
      "Epoch 125/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.2905 - accuracy: 0.9941 - val_loss: 0.2479 - val_accuracy: 0.9974\n",
      "Epoch 126/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.2893 - accuracy: 0.9941 - val_loss: 0.2470 - val_accuracy: 0.9975\n",
      "Epoch 127/200\n",
      "678/678 [==============================] - 2s 2ms/step - loss: 0.2881 - accuracy: 0.9941 - val_loss: 0.2460 - val_accuracy: 0.9976\n",
      "Epoch 128/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.2886 - accuracy: 0.9926 - val_loss: 0.2450 - val_accuracy: 0.9977\n",
      "Epoch 129/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.2858 - accuracy: 0.9941 - val_loss: 0.2442 - val_accuracy: 0.9978\n",
      "Epoch 130/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.2847 - accuracy: 0.9941 - val_loss: 0.2433 - val_accuracy: 0.9978\n",
      "Epoch 131/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.2836 - accuracy: 0.9941 - val_loss: 0.2424 - val_accuracy: 0.9978\n",
      "Epoch 132/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.2825 - accuracy: 0.9941 - val_loss: 0.2415 - val_accuracy: 0.9979\n",
      "Epoch 133/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.2814 - accuracy: 0.9941 - val_loss: 0.2407 - val_accuracy: 0.9979\n",
      "Epoch 134/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.2802 - accuracy: 0.9941 - val_loss: 0.2398 - val_accuracy: 0.9980\n",
      "Epoch 135/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.2791 - accuracy: 0.9941 - val_loss: 0.2389 - val_accuracy: 0.9980\n",
      "Epoch 136/200\n",
      "678/678 [==============================] - 2s 3ms/step - loss: 0.2781 - accuracy: 0.9941 - val_loss: 0.2381 - val_accuracy: 0.9980\n",
      "Epoch 137/200\n",
      "678/678 [==============================] - 2s 3ms/step - loss: 0.2765 - accuracy: 0.9941 - val_loss: 0.2374 - val_accuracy: 0.9980\n",
      "Epoch 138/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.2746 - accuracy: 0.9956 - val_loss: 0.2366 - val_accuracy: 0.9979\n",
      "Epoch 139/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.2735 - accuracy: 0.9956 - val_loss: 0.2360 - val_accuracy: 0.9977\n",
      "Epoch 140/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.2703 - accuracy: 0.9971 - val_loss: 0.2358 - val_accuracy: 0.9971\n",
      "Epoch 141/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.2732 - accuracy: 0.9941 - val_loss: 0.2353 - val_accuracy: 0.9970\n",
      "Epoch 142/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.2691 - accuracy: 0.9956 - val_loss: 0.2344 - val_accuracy: 0.9970\n",
      "Epoch 143/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.2668 - accuracy: 0.9971 - val_loss: 0.2336 - val_accuracy: 0.9969\n",
      "Epoch 144/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.2661 - accuracy: 0.9971 - val_loss: 0.2328 - val_accuracy: 0.9969\n",
      "Epoch 145/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.2648 - accuracy: 0.9971 - val_loss: 0.2321 - val_accuracy: 0.9969\n",
      "Epoch 146/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.2636 - accuracy: 0.9971 - val_loss: 0.2314 - val_accuracy: 0.9967\n",
      "Epoch 147/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.2626 - accuracy: 0.9971 - val_loss: 0.2308 - val_accuracy: 0.9965\n",
      "Epoch 148/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.2608 - accuracy: 0.9971 - val_loss: 0.2303 - val_accuracy: 0.9963\n",
      "Epoch 149/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.2606 - accuracy: 0.9971 - val_loss: 0.2297 - val_accuracy: 0.9961\n",
      "Epoch 150/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.2595 - accuracy: 0.9971 - val_loss: 0.2292 - val_accuracy: 0.9960\n",
      "Epoch 151/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.2590 - accuracy: 0.9971 - val_loss: 0.2288 - val_accuracy: 0.9957\n",
      "Epoch 152/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.2572 - accuracy: 0.9971 - val_loss: 0.2285 - val_accuracy: 0.9954\n",
      "Epoch 153/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.2560 - accuracy: 0.9971 - val_loss: 0.2283 - val_accuracy: 0.9951\n",
      "Epoch 154/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.2547 - accuracy: 0.9971 - val_loss: 0.2281 - val_accuracy: 0.9948\n",
      "Epoch 155/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.2536 - accuracy: 0.9985 - val_loss: 0.2279 - val_accuracy: 0.9943\n",
      "Epoch 156/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.2542 - accuracy: 0.9971 - val_loss: 0.2276 - val_accuracy: 0.9940\n",
      "Epoch 157/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.2514 - accuracy: 0.9985 - val_loss: 0.2270 - val_accuracy: 0.9938\n",
      "Epoch 158/200\n",
      "678/678 [==============================] - 2s 2ms/step - loss: 0.2504 - accuracy: 0.9985 - val_loss: 0.2260 - val_accuracy: 0.9938\n",
      "Epoch 159/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.2491 - accuracy: 0.9985 - val_loss: 0.2249 - val_accuracy: 0.9938\n",
      "Epoch 160/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.2500 - accuracy: 0.9971 - val_loss: 0.2239 - val_accuracy: 0.9938\n",
      "Epoch 161/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.2508 - accuracy: 0.9956 - val_loss: 0.2229 - val_accuracy: 0.9939\n",
      "Epoch 162/200\n",
      "678/678 [==============================] - 2s 2ms/step - loss: 0.2476 - accuracy: 0.9971 - val_loss: 0.2219 - val_accuracy: 0.9939\n",
      "Epoch 163/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.2448 - accuracy: 0.9985 - val_loss: 0.2210 - val_accuracy: 0.9940\n",
      "Epoch 164/200\n",
      "678/678 [==============================] - 2s 2ms/step - loss: 0.2438 - accuracy: 0.9985 - val_loss: 0.2200 - val_accuracy: 0.9941\n",
      "Epoch 165/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.2429 - accuracy: 0.9985 - val_loss: 0.2190 - val_accuracy: 0.9942\n",
      "Epoch 166/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.2419 - accuracy: 0.9985 - val_loss: 0.2181 - val_accuracy: 0.9943\n",
      "Epoch 167/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.2430 - accuracy: 0.9971 - val_loss: 0.2173 - val_accuracy: 0.9944\n",
      "Epoch 168/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.2400 - accuracy: 0.9985 - val_loss: 0.2164 - val_accuracy: 0.9945\n",
      "Epoch 169/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.2412 - accuracy: 0.9971 - val_loss: 0.2155 - val_accuracy: 0.9946\n",
      "Epoch 170/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.2403 - accuracy: 0.9971 - val_loss: 0.2147 - val_accuracy: 0.9946\n",
      "Epoch 171/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.2373 - accuracy: 0.9985 - val_loss: 0.2139 - val_accuracy: 0.9946\n",
      "Epoch 172/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.2364 - accuracy: 0.9985 - val_loss: 0.2131 - val_accuracy: 0.9947\n",
      "Epoch 173/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.2354 - accuracy: 0.9985 - val_loss: 0.2123 - val_accuracy: 0.9947\n",
      "Epoch 174/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.2345 - accuracy: 0.9985 - val_loss: 0.2115 - val_accuracy: 0.9948\n",
      "Epoch 175/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.2337 - accuracy: 0.9985 - val_loss: 0.2107 - val_accuracy: 0.9948\n",
      "Epoch 176/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.2328 - accuracy: 0.9985 - val_loss: 0.2100 - val_accuracy: 0.9948\n",
      "Epoch 177/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.2319 - accuracy: 0.9985 - val_loss: 0.2093 - val_accuracy: 0.9948\n",
      "Epoch 178/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.2313 - accuracy: 0.9985 - val_loss: 0.2089 - val_accuracy: 0.9946\n",
      "Epoch 179/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.2323 - accuracy: 0.9971 - val_loss: 0.2085 - val_accuracy: 0.9943\n",
      "Epoch 180/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.2293 - accuracy: 0.9985 - val_loss: 0.2080 - val_accuracy: 0.9941\n",
      "Epoch 181/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.2305 - accuracy: 0.9971 - val_loss: 0.2075 - val_accuracy: 0.9940\n",
      "Epoch 182/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.2297 - accuracy: 0.9971 - val_loss: 0.2070 - val_accuracy: 0.9939\n",
      "Epoch 183/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.2286 - accuracy: 0.9971 - val_loss: 0.2067 - val_accuracy: 0.9935\n",
      "Epoch 184/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.2258 - accuracy: 0.9985 - val_loss: 0.2065 - val_accuracy: 0.9931\n",
      "Epoch 185/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.2250 - accuracy: 0.9985 - val_loss: 0.2063 - val_accuracy: 0.9927\n",
      "Epoch 186/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.2279 - accuracy: 0.9956 - val_loss: 0.2084 - val_accuracy: 0.9908\n",
      "Epoch 187/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.2257 - accuracy: 0.9971 - val_loss: 0.2051 - val_accuracy: 0.9927\n",
      "Epoch 188/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.2226 - accuracy: 0.9985 - val_loss: 0.2030 - val_accuracy: 0.9937\n",
      "Epoch 189/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.2222 - accuracy: 0.9985 - val_loss: 0.2019 - val_accuracy: 0.9941\n",
      "Epoch 190/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.2208 - accuracy: 0.9985 - val_loss: 0.2008 - val_accuracy: 0.9944\n",
      "Epoch 191/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.2200 - accuracy: 0.9985 - val_loss: 0.1998 - val_accuracy: 0.9946\n",
      "Epoch 192/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.2192 - accuracy: 0.9985 - val_loss: 0.1989 - val_accuracy: 0.9948\n",
      "Epoch 193/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.2184 - accuracy: 0.9985 - val_loss: 0.1979 - val_accuracy: 0.9951\n",
      "Epoch 194/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.2184 - accuracy: 0.9971 - val_loss: 0.1972 - val_accuracy: 0.9951\n",
      "Epoch 195/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.2190 - accuracy: 0.9971 - val_loss: 0.1966 - val_accuracy: 0.9951\n",
      "Epoch 196/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.2160 - accuracy: 0.9985 - val_loss: 0.1960 - val_accuracy: 0.9951\n",
      "Epoch 197/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.2153 - accuracy: 0.9985 - val_loss: 0.1954 - val_accuracy: 0.9951\n",
      "Epoch 198/200\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 0.2158 - accuracy: 0.9971 - val_loss: 0.1950 - val_accuracy: 0.9948\n",
      "Epoch 199/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.2141 - accuracy: 0.9985 - val_loss: 0.1944 - val_accuracy: 0.9948\n",
      "Epoch 200/200\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 0.2130 - accuracy: 0.9985 - val_loss: 0.1939 - val_accuracy: 0.9948\n"
     ]
    }
   ],
   "source": [
    "# Build the best model based on the parameter tuning\n",
    "w1=1\n",
    "w2=1\n",
    "batch_size=2048\n",
    "epochs=200\n",
    "cm_results = []\n",
    "final_model = Sequential()\n",
    "final_model.add(LSTM(20, return_sequences=True, input_shape=X_train.shape[1:]))\n",
    "final_model.add(Dropout(0.2))\n",
    "final_model.add(LSTM(12, activation='relu', return_sequences=True))\n",
    "final_model.add(Dropout(0.2))\n",
    "final_model.add(LSTM(8, activation='relu', return_sequences=False)) # this is the last LSTM, so should return_sequences=False\n",
    "final_model.add(Dense(units=1, kernel_initializer='uniform', activation='sigmoid'))\n",
    "bias_initializer = ones/zeros \n",
    "#bias_initializer = np.log([bias_initializer])\n",
    "print('bias_initializer', bias_initializer)\n",
    "bias_initializer = initializers.Constant(bias_initializer)\n",
    "final_model.add(Dense(units=1, activation='sigmoid', bias_initializer=bias_initializer))\n",
    "optimizer = optimizers.Adam(lr=lr)\n",
    "final_model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy']) #optimizer='rmsprop', optimizer='sgd', optimizer='adam'\n",
    "\n",
    "model_output = final_model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=1,\n",
    "                         class_weight=np.where(y_train == 1,w1,w2).flatten(), validation_data=(val_features, val_labels) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       ...,\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#final_model.predict(X_val)\n",
    "#final_model.predict_proba(X_val)\n",
    "final_model.predict_classes(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_4 (LSTM)                (None, 10, 20)            3920      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 10, 20)            0         \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 10, 12)            1584      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 10, 12)            0         \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 8)                 672       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 9         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 2         \n",
      "=================================================================\n",
      "Total params: 6,187\n",
      "Trainable params: 6,187\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "final_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN | AUC Score: 1.0\n",
      "TEST | AUC Score: 0.5043501711927209\n",
      "VAL | AUC Score: 0.49924539943058316\n",
      "error\n",
      "error\n",
      "Confusion Matrix:\n",
      "false positive pct: 1.4141224948492226\n",
      "tn  fp  fn  tp\n",
      "42029 604 78 1\n",
      "[[42029   604]\n",
      " [   78     1]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99     42633\n",
      "           1       0.00      0.01      0.00        79\n",
      "\n",
      "    accuracy                           0.98     42712\n",
      "   macro avg       0.50      0.50      0.50     42712\n",
      "weighted avg       1.00      0.98      0.99     42712\n",
      "\n",
      "Specificity = 0.985832571013065\n",
      "Sensitivity = 0.012658227848101266\n",
      "lr 0.01 w1 1 w2 1 epochs 200 batch_size 2048\n",
      "           algo     TN   FP  FN  TP    lr  w1  w2  epochs  batch_size     SP  \\\n",
      "0  sequential_2  42029  604  78   1  0.01   1   1     200        2048  0.986   \n",
      "\n",
      "      SE     Avg  \n",
      "0  0.013  0.4995  \n"
     ]
    }
   ],
   "source": [
    "val_predict = final_model.predict_classes(X_val)\n",
    "\n",
    "### test AUC ###\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_train, train_predict, pos_label=1)\n",
    "print('TRAIN | AUC Score: ' + str((metrics.auc(fpr, tpr))))\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, test_predict, pos_label=1)\n",
    "print('TEST | AUC Score: ' + str((metrics.auc(fpr, tpr))))\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_val, val_predict, pos_label=1)\n",
    "print('VAL | AUC Score: ' + str((metrics.auc(fpr, tpr))))\n",
    "tn, fp, fn, tp = display_metrics(final_model, X_train, X_val, y_train, y_val, val_predict)\n",
    "cm_results.append([final_model.name, tn, fp, fn, tp, lr, w1, w2, epochs, batch_size])\n",
    "print('lr',lr,'w1',w1,'w2',w2,'epochs',epochs,'batch_size',batch_size)\n",
    "final_results = pd.DataFrame(cm_results, columns=('algo','TN','FP','FN','TP', 'lr', 'w1', 'w2', 'epochs', 'batch_size')) \n",
    "final_results['SP'] = round(final_results['TN']/(final_results['TN'] + final_results['FP']), 3)\n",
    "final_results['SE'] = round(final_results['TP']/(final_results['TP'] + final_results['FN']), 3)\n",
    "final_results['Avg'] = (final_results['SP'] + final_results['SE'])/2\n",
    "\n",
    "print(final_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "\n",
    "So far cannot get avg of SP and SE above 50%\n",
    "\n",
    "1) Adam is more consistent for optimization\n",
    "2) Stratify doesn't make much difference, include it for consistency\n",
    "3) lr needs to be less than 0.05, keep at 0.01\n",
    "\n",
    "4) Try wts later\n",
    "5) epochs\n",
    "6) batch size\n",
    "\n",
    "Details:\n",
    "\n",
    "SGD vs Adam and Stratify vs No Stratify:\n",
    "\n",
    "    SGD with Stratify produces inconsistent results, lots of 0's in the confusion matrix, esp TP\n",
    "    Adam with stratify is much more consistent, almost always has some values in all 4 categories, and FP has less variation, even if TP count is still low (~1-3)\n",
    "\n",
    "    SGD with Stratify:\n",
    "\n",
    "                 algo     TN     FP   FN   TP    lr   w1   w2  epochs  batch_size  \\\n",
    "    0    sequential_1  79719   5566  142    6  0.01  4.0  1.0      10           8   \n",
    "    13  sequential_14  85202     83  148    0  0.01  4.0  1.0      10           8   \n",
    "    23  sequential_24  85160    125  148    0  0.01  4.0  1.0      10           8   \n",
    "    22  sequential_23  40018  45267   69   79  0.01  4.0  1.0      10           8   \n",
    "    21  sequential_22  85279      6  148    0  0.01  4.0  1.0      10           8   \n",
    "    20  sequential_21  84649    636  148    0  0.01  4.0  1.0      10           8   \n",
    "    19  sequential_20  85221     64  148    0  0.01  4.0  1.0      10           8   \n",
    "    18  sequential_19  85252     33  148    0  0.01  4.0  1.0      10           8 \n",
    "\n",
    "\n",
    "    SGD without Stratify:\n",
    "\n",
    "                 algo     TN     FP   FN   TP    lr   w1   w2  epochs  batch_size  \\\n",
    "    0    sequential_1  85216     77  140    0  0.01  4.0  1.0      10           8   \n",
    "    13  sequential_14  85035    258  138    2  0.01  4.0  1.0      10           8   \n",
    "    23  sequential_24  85255     38  139    1  0.01  4.0  1.0      10           8   \n",
    "    22  sequential_23  83812   1481  138    2  0.01  4.0  1.0      10           8   \n",
    "    21  sequential_22  84769    524  140    0  0.01  4.0  1.0      10           8   \n",
    "    20  sequential_21  76723   8570  129   11  0.01  4.0  1.0      10           8   \n",
    "    19  sequential_20  85146    147  139    1  0.01  4.0  1.0      10           8\n",
    "\n",
    "    ADAM with Stratify:\n",
    "\n",
    "                 algo     TN    FP   FN  TP    lr   w1   w2  epochs  batch_size  \\\n",
    "    0    sequential_1  84840   445  147   1  0.01  4.0  1.0      10           8   \n",
    "    13  sequential_14  84984   301  148   0  0.01  4.0  1.0      10           8   \n",
    "    23  sequential_24  84613   672  148   0  0.01  4.0  1.0      10           8   \n",
    "    22  sequential_23  84447   838  147   1  0.01  4.0  1.0      10           8   \n",
    "    21  sequential_22  84386   899  148   0  0.01  4.0  1.0      10           8   \n",
    "    20  sequential_21  85037   248  148   0  0.01  4.0  1.0      10           8   \n",
    "    19  sequential_20  84207  1078  145   3  0.01  4.0  1.0      10           8   \n",
    "    18  sequential_19  85001   284  148   0  0.01  4.0  1.0      10           8   \n",
    "    17  sequential_18  84990   295  148   0  0.01  4.0  1.0      10           8   \n",
    "    16  sequential_17  84863   422  148   0  0.01  4.0  1.0      10           8   \n",
    "    15  sequential_16  84585   700  147   1  0.01  4.0  1.0      10           8   \n",
    "    14  sequential_15  84728   557  148   0  0.01  4.0  1.0      10           8   \n",
    "    12  sequential_13  84396   889  146   2  0.01  4.0  1.0      10           8   \n",
    "    ADAM without Stratify:\n",
    "\n",
    "                 algo     TN    FP   FN  TP    lr   w1   w2  epochs  batch_size  \\\n",
    "    0    sequential_1  84218  1055  159   1  0.01  4.0  1.0      10           8   \n",
    "    13  sequential_14  85036   237  160   0  0.01  4.0  1.0      10           8   \n",
    "    23  sequential_24  84721   552  159   1  0.01  4.0  1.0      10           8   \n",
    "    22  sequential_23  84609   664  160   0  0.01  4.0  1.0      10           8   \n",
    "    21  sequential_22  84730   543  160   0  0.01  4.0  1.0      10           8   \n",
    "    20  sequential_21  85046   227  160   0  0.01  4.0  1.0      10           8   \n",
    "    19  sequential_20  84771   502  158   2  0.01  4.0  1.0      10           8   \n",
    "    18  sequential_19  84612   661  160   0  0.01  4.0  1.0      10           8   \n",
    "    17  sequential_18  82808  2465  154   6  0.01  4.0  1.0      10           8   \n",
    "    16  sequential_17  84382   891  159   1  0.01  4.0  1.0      10           8   \n",
    "    15  sequential_16  84715   558  160   0  0.01  4.0  1.0      10           8   \n",
    "    14  sequential_15  84879   394  160   0  0.01  4.0  1.0      10           8   \n",
    "    12  sequential_13  85240    33  160   0  0.01  4.0  1.0      10           8   \n",
    "    1    sequential_2  84906   367  160   0  0.01  4.0  1.0      10           8   \n",
    "\n",
    "lr values (ADAM and Stratify): use value of 0.025\n",
    "\n",
    "need value less than 0.05 to show consistency in results\n",
    "\n",
    "             algo     TN     FP   FN   TP        lr   w1   w2  epochs  \\\n",
    "24  sequential_25  85285      0  148    0  0.100000  4.0  1.0      10   \n",
    "23  sequential_24  85285      0  148    0  0.100000  4.0  1.0      10   \n",
    "22  sequential_23  85285      0  148    0  0.100000  4.0  1.0      10   \n",
    "21  sequential_22      0  85285    0  148  0.100000  4.0  1.0      10   \n",
    "20  sequential_21      1  85284    0  148  0.100000  4.0  1.0      10   \n",
    "19  sequential_20  84877    408  144    4  0.075025  4.0  1.0      10   \n",
    "18  sequential_19  84508    777  144    4  0.075025  4.0  1.0      10   \n",
    "17  sequential_18      0  85285    0  148  0.075025  4.0  1.0      10   \n",
    "16  sequential_17  81305   3980  141    7  0.075025  4.0  1.0      10   \n",
    "15  sequential_16  84733    552  145    3  0.075025  4.0  1.0      10  \n",
    "**** cutoff here\n",
    "13  sequential_14  83804   1481  143    5  0.050050  4.0  1.0      10   \n",
    "14  sequential_15  83316   1969  143    5  0.050050  4.0  1.0      10   \n",
    "12  sequential_13  85011    274  147    1  0.050050  4.0  1.0      10   \n",
    "11  sequential_12  84644    641  144    4  0.050050  4.0  1.0      10   \n",
    "10  sequential_11  85131    154  147    1  0.050050  4.0  1.0      10   \n",
    "9   sequential_10  84520    765  145    3  0.025075  4.0  1.0      10   \n",
    "8    sequential_9  84118   1167  142    6  0.025075  4.0  1.0      10   \n",
    "7    sequential_8  84874    411  147    1  0.025075  4.0  1.0      10   \n",
    "6    sequential_7  84557    728  144    4  0.025075  4.0  1.0      10   \n",
    "5    sequential_6  84555    730  144    4  0.025075  4.0  1.0      10   \n",
    "1    sequential_2  85038    247  147    1  0.000100  4.0  1.0      10   \n",
    "4    sequential_5  85123    162  146    2  0.000100  4.0  1.0      10   \n",
    "3    sequential_4  85026    259  147    1  0.000100  4.0  1.0      10   \n",
    "2    sequential_3  84763    522  146    2  0.000100  4.0  1.0      10   \n",
    "0    sequential_1  84537    748  146    2  0.000100  4.0  1.0      10\n",
    "\n",
    "\n",
    "             algo     TN    FP   FN  TP        lr   w1   w2  epochs  \\\n",
    "24  sequential_25  84318   967  146   2  0.050000  4.0  1.0      10   \n",
    "23  sequential_24  85045   240  147   1  0.050000  4.0  1.0      10   \n",
    "22  sequential_23  84672   613  146   2  0.050000  4.0  1.0      10   \n",
    "21  sequential_22  82670  2615  142   6  0.050000  4.0  1.0      10   \n",
    "20  sequential_21  83087  2198  146   2  0.050000  4.0  1.0      10   \n",
    "19  sequential_20  84231  1054  148   0  0.037525  4.0  1.0      10   \n",
    "18  sequential_19  85006   279  148   0  0.037525  4.0  1.0      10   \n",
    "17  sequential_18  84610   675  148   0  0.037525  4.0  1.0      10   \n",
    "16  sequential_17  84759   526  148   0  0.037525  4.0  1.0      10   \n",
    "15  sequential_16  84522   763  148   0  0.037525  4.0  1.0      10   \n",
    "13  sequential_14  83834  1451  146   2  0.025050  4.0  1.0      10   \n",
    "14  sequential_15  83814  1471  146   2  0.025050  4.0  1.0      10   \n",
    "12  sequential_13  84764   521  148   0  0.025050  4.0  1.0      10   \n",
    "11  sequential_12  84809   476  147   1  0.025050  4.0  1.0      10   \n",
    "10  sequential_11  84723   562  147   1  0.025050  4.0  1.0      10   \n",
    "9   sequential_10  84879   406  148   0  0.012575  4.0  1.0      10   \n",
    "8    sequential_9  83464  1821  147   1  0.012575  4.0  1.0      10   \n",
    "7    sequential_8  84803   482  148   0  0.012575  4.0  1.0      10   \n",
    "6    sequential_7  82689  2596  144   4  0.012575  4.0  1.0      10   \n",
    "5    sequential_6  83254  2031  147   1  0.012575  4.0  1.0      10   \n",
    "1    sequential_2  84693   592  148   0  0.000100  4.0  1.0      10   \n",
    "4    sequential_5  85028   257  148   0  0.000100  4.0  1.0      10   \n",
    "3    sequential_4  84807   478  148   0  0.000100  4.0  1.0      10   \n",
    "2    sequential_3  84371   914  148   0  0.000100  4.0  1.0      10   \n",
    "0    sequential_1  84495   790  148   0  0.000100  4.0  1.0      10  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    }
   ],
   "source": [
    "print('finish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
